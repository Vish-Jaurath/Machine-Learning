{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be15ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b57e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= ['They were foxes trying to eat Ji-ah but # % she was saved by Lee Yeon.', \n",
    "       ' Even though he compelled her to forget !! him, the magic did not work on her.', \n",
    "       'Later, she found herself at the accident \\n site but her parents bodies were never discovered.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25b3a5a",
   "metadata": {},
   "source": [
    "## Step 1 : Convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bfc884f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they were foxes trying to eat ji-ah but # % she was saved by lee yeon.',\n",
       " ' even though he compelled her to forget !! him, the magic did not work on her.',\n",
       " 'later, she found herself at the accident \\n site but her parents bodies were never discovered.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs= [ word.lower() for word in docs]\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d5427e",
   "metadata": {},
   "source": [
    "## Step 2 : Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50661b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb62c7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['they', 'were', 'foxes', 'trying', 'to', 'eat', 'ji-ah', 'but', '#', '%', 'she', 'was', 'saved', 'by', 'lee', 'yeon', '.'], ['even', 'though', 'he', 'compelled', 'her', 'to', 'forget', '!', '!', 'him', ',', 'the', 'magic', 'did', 'not', 'work', 'on', 'her', '.'], ['later', ',', 'she', 'found', 'herself', 'at', 'the', 'accident', 'site', 'but', 'her', 'parents', 'bodies', 'were', 'never', 'discovered', '.']]\n"
     ]
    }
   ],
   "source": [
    "docs_tokenized =  [word_tokenize(word) for word in docs]\n",
    "print(docs_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d955b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['they were foxes trying to eat ji-ah but # % she was saved by lee yeon.'], [' even though he compelled her to forget !!', 'him, the magic did not work on her.'], ['later, she found herself at the accident \\n site but her parents bodies were never discovered.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenized =  [sent_tokenize(word) for word in docs]\n",
    "print(sent_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b7d0f3",
   "metadata": {},
   "source": [
    "## Step 3 : Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d6a8c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'[!\"\\#\\$%\\&\\'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~]',\n",
       "re.UNICODE)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, string\n",
    "reg= re.compile('[%s]' %  re.escape(string.punctuation))\n",
    "reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f83f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tokenized_no_punctuation= []\n",
    "\n",
    "for stm in docs_tokenized:\n",
    "    words= []    \n",
    "    for word in stm:\n",
    "        token= reg.sub(u'',word)\n",
    "        if not token == u'':\n",
    "            words.append(token)        \n",
    "    docs_tokenized_no_punctuation.append(words)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03f4bb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['they', 'were', 'foxes', 'trying', 'to', 'eat', 'jiah', 'but', 'she', 'was', 'saved', 'by', 'lee', 'yeon'], ['even', 'though', 'he', 'compelled', 'her', 'to', 'forget', 'him', 'the', 'magic', 'did', 'not', 'work', 'on', 'her'], ['later', 'she', 'found', 'herself', 'at', 'the', 'accident', 'site', 'but', 'her', 'parents', 'bodies', 'were', 'never', 'discovered']]\n"
     ]
    }
   ],
   "source": [
    "print(docs_tokenized_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1764fb",
   "metadata": {},
   "source": [
    "## Step 4 : Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bea29bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7fb2e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['foxes', 'trying', 'eat', 'jiah', 'saved', 'lee', 'yeon'], ['even', 'though', 'compelled', 'forget', 'magic', 'work'], ['later', 'found', 'accident', 'site', 'parents', 'bodies', 'never', 'discovered']]\n"
     ]
    }
   ],
   "source": [
    "docs_tokenized_no_stopwords= []\n",
    "\n",
    "for stm in docs_tokenized_no_punctuation:\n",
    "    words= []    \n",
    "    for word in stm:\n",
    "        if not word in stopwords.words('english'):     \n",
    "            words.append(word)        \n",
    "    docs_tokenized_no_stopwords.append(words) \n",
    "    \n",
    "print(docs_tokenized_no_stopwords)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f5ced2",
   "metadata": {},
   "source": [
    "## Step 5 : Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e34e9481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fox', 'trying', 'eat', 'jiah', 'saved', 'lee', 'yeon'], ['even', 'though', 'compelled', 'forget', 'magic', 'work'], ['later', 'found', 'accident', 'site', 'parent', 'body', 'never', 'discovered']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "porter= PorterStemmer()\n",
    "lemma= WordNetLemmatizer()\n",
    "\n",
    "preprocessed_docs= []\n",
    "\n",
    "for stm in docs_tokenized_no_stopwords:\n",
    "    words= []    \n",
    "    for word in stm:            \n",
    "        #words.append(porter.stem(word))  \n",
    "        words.append(lemma.lemmatize(word)) \n",
    "    preprocessed_docs.append(words) \n",
    "    \n",
    "print(preprocessed_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60523e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watch Out: fox trying eat jiah saved lee yeon\n"
     ]
    }
   ],
   "source": [
    "nn= ['fox', 'trying', 'eat', 'jiah', 'saved', 'lee', 'yeon']\n",
    "print(f\"Watch Out: {' '.join(nn)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
