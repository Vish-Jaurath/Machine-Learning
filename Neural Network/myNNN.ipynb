{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Implement Neural Network</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I have used 'Insurance dataset' that will help us evaluate how likely an individual would buy an insurance based on Age and Affordability.\n",
    "Also built Gradient Descent and Neural network from scratch by using plain python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>affordibility</th>\n",
       "      <th>bought_insurance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  affordibility  bought_insurance\n",
       "0   22              1                 0\n",
       "1   25              0                 0\n",
       "2   47              1                 1\n",
       "3   52              0                 0\n",
       "4   46              1                 1\n",
       "5   56              1                 1\n",
       "6   55              0                 0\n",
       "7   60              0                 1\n",
       "8   62              1                 1\n",
       "9   61              1                 1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(r'C:\\Users\\Administrator\\machine learning\\insurance_dataset.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled age column as need to have both affordibility and age columns on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>affordibility</th>\n",
       "      <th>bought_insurance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.47</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.46</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  affordibility  bought_insurance\n",
       "0  0.22              1                 0\n",
       "1  0.25              0                 0\n",
       "2  0.47              1                 1\n",
       "3  0.52              0                 0\n",
       "4  0.46              1                 1\n",
       "5  0.56              1                 1\n",
       "6  0.55              0                 0\n",
       "7  0.60              0                 1\n",
       "8  0.62              1                 1\n",
       "9  0.61              1                 1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scale= df.copy()\n",
    "df_scale.age= df_scale.age/100 # from 1 to 100\n",
    "df_scale.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset.\n",
    "\n",
    "x_train,x_test,y_train,y_test= train_test_split(df_scale[['age','affordibility']], df_scale.bought_insurance, train_size=0.8, random_state=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemented Neural network using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "model= Sequential([\n",
    "    Dense(1, input_shape=(2,), activation= 'sigmoid', kernel_initializer= 'ones', bias_initializer= 'zeros')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= 'binary_crossentropy',\n",
    "             optimizer= 'adam',\n",
    "             metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7068 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x23af7e87d88>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train) # one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, bias= model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9992559],\n",
       "       [0.9992559]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight    # starting weights of age, affordibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00074412], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias   # starting bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3500\n",
      "22/22 [==============================] - 0s 1000us/step - loss: 0.7065 - accuracy: 0.5000\n",
      "Epoch 2/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.7062 - accuracy: 0.5000\n",
      "Epoch 3/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.7059 - accuracy: 0.5000\n",
      "Epoch 4/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.7055 - accuracy: 0.5000\n",
      "Epoch 5/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.7052 - accuracy: 0.5000\n",
      "Epoch 6/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.7048 - accuracy: 0.5000\n",
      "Epoch 7/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.7044 - accuracy: 0.5000\n",
      "Epoch 8/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.7041 - accuracy: 0.5000\n",
      "Epoch 9/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.7037 - accuracy: 0.5000\n",
      "Epoch 10/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.7033 - accuracy: 0.5000\n",
      "Epoch 11/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.7030 - accuracy: 0.5000\n",
      "Epoch 12/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.7026 - accuracy: 0.5000\n",
      "Epoch 13/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.7022 - accuracy: 0.5000\n",
      "Epoch 14/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.7019 - accuracy: 0.5000\n",
      "Epoch 15/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.7015 - accuracy: 0.5000\n",
      "Epoch 16/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.7012 - accuracy: 0.5000\n",
      "Epoch 17/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.7008 - accuracy: 0.5000\n",
      "Epoch 18/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.7004 - accuracy: 0.5000\n",
      "Epoch 19/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.7001 - accuracy: 0.5000\n",
      "Epoch 20/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6997 - accuracy: 0.5000\n",
      "Epoch 21/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6994 - accuracy: 0.5000\n",
      "Epoch 22/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6990 - accuracy: 0.5000\n",
      "Epoch 23/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6986 - accuracy: 0.5000\n",
      "Epoch 24/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6983 - accuracy: 0.5000\n",
      "Epoch 25/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6979 - accuracy: 0.5000\n",
      "Epoch 26/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6976 - accuracy: 0.5000\n",
      "Epoch 27/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6972 - accuracy: 0.5000\n",
      "Epoch 28/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6969 - accuracy: 0.5000\n",
      "Epoch 29/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6965 - accuracy: 0.5000\n",
      "Epoch 30/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6962 - accuracy: 0.5000\n",
      "Epoch 31/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6958 - accuracy: 0.5000\n",
      "Epoch 32/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6955 - accuracy: 0.5000\n",
      "Epoch 33/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6951 - accuracy: 0.5000\n",
      "Epoch 34/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6948 - accuracy: 0.5000\n",
      "Epoch 35/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6944 - accuracy: 0.5000\n",
      "Epoch 36/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6941 - accuracy: 0.5000\n",
      "Epoch 37/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6937 - accuracy: 0.5000\n",
      "Epoch 38/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 39/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6931 - accuracy: 0.5000\n",
      "Epoch 40/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6927 - accuracy: 0.5000\n",
      "Epoch 41/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6924 - accuracy: 0.5000\n",
      "Epoch 42/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6920 - accuracy: 0.5000\n",
      "Epoch 43/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6917 - accuracy: 0.5000\n",
      "Epoch 44/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6914 - accuracy: 0.5000\n",
      "Epoch 45/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6910 - accuracy: 0.5000\n",
      "Epoch 46/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6907 - accuracy: 0.5000\n",
      "Epoch 47/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6904 - accuracy: 0.5000\n",
      "Epoch 48/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6900 - accuracy: 0.5000\n",
      "Epoch 49/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6897 - accuracy: 0.5000\n",
      "Epoch 50/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6894 - accuracy: 0.5000\n",
      "Epoch 51/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6891 - accuracy: 0.5000\n",
      "Epoch 52/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6887 - accuracy: 0.5000\n",
      "Epoch 53/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6884 - accuracy: 0.5000\n",
      "Epoch 54/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6881 - accuracy: 0.5000\n",
      "Epoch 55/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6878 - accuracy: 0.5000\n",
      "Epoch 56/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6874 - accuracy: 0.5000\n",
      "Epoch 57/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6871 - accuracy: 0.5000\n",
      "Epoch 58/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6868 - accuracy: 0.5000\n",
      "Epoch 59/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.6865 - accuracy: 0.5000\n",
      "Epoch 60/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6862 - accuracy: 0.5000\n",
      "Epoch 61/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6859 - accuracy: 0.5000\n",
      "Epoch 62/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6855 - accuracy: 0.5000\n",
      "Epoch 63/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6852 - accuracy: 0.5000\n",
      "Epoch 64/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6849 - accuracy: 0.5000\n",
      "Epoch 65/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6846 - accuracy: 0.5000\n",
      "Epoch 66/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6843 - accuracy: 0.5000\n",
      "Epoch 67/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6840 - accuracy: 0.5000\n",
      "Epoch 68/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6837 - accuracy: 0.5000\n",
      "Epoch 69/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6834 - accuracy: 0.5000\n",
      "Epoch 70/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6831 - accuracy: 0.5000\n",
      "Epoch 71/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6828 - accuracy: 0.5000\n",
      "Epoch 72/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6825 - accuracy: 0.5000\n",
      "Epoch 73/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6822 - accuracy: 0.5000\n",
      "Epoch 74/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6819 - accuracy: 0.5000\n",
      "Epoch 75/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6816 - accuracy: 0.5000\n",
      "Epoch 76/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6813 - accuracy: 0.5000\n",
      "Epoch 77/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6810 - accuracy: 0.5000\n",
      "Epoch 78/3500\n",
      "22/22 [==============================] - 0s 682us/step - loss: 0.6807 - accuracy: 0.5000\n",
      "Epoch 79/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6804 - accuracy: 0.5000\n",
      "Epoch 80/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6801 - accuracy: 0.5000\n",
      "Epoch 81/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6798 - accuracy: 0.5000\n",
      "Epoch 82/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6795 - accuracy: 0.5000\n",
      "Epoch 83/3500\n",
      "22/22 [==============================] - 0s 500us/step - loss: 0.6792 - accuracy: 0.5000\n",
      "Epoch 84/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6789 - accuracy: 0.5000\n",
      "Epoch 85/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6786 - accuracy: 0.5000\n",
      "Epoch 86/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6783 - accuracy: 0.5000\n",
      "Epoch 87/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6780 - accuracy: 0.5000\n",
      "Epoch 88/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6778 - accuracy: 0.5000\n",
      "Epoch 89/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6775 - accuracy: 0.5000\n",
      "Epoch 90/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6772 - accuracy: 0.5000\n",
      "Epoch 91/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6769 - accuracy: 0.5000\n",
      "Epoch 92/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6766 - accuracy: 0.5000\n",
      "Epoch 93/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6764 - accuracy: 0.5000\n",
      "Epoch 94/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6761 - accuracy: 0.5000\n",
      "Epoch 95/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6758 - accuracy: 0.5000\n",
      "Epoch 96/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6755 - accuracy: 0.5000\n",
      "Epoch 97/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6752 - accuracy: 0.5000\n",
      "Epoch 98/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6750 - accuracy: 0.5000\n",
      "Epoch 99/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6747 - accuracy: 0.5000\n",
      "Epoch 100/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6744 - accuracy: 0.5000\n",
      "Epoch 101/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6742 - accuracy: 0.5000\n",
      "Epoch 102/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6739 - accuracy: 0.5000\n",
      "Epoch 103/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6736 - accuracy: 0.5000\n",
      "Epoch 104/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6733 - accuracy: 0.5000\n",
      "Epoch 105/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6731 - accuracy: 0.5000\n",
      "Epoch 106/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6728 - accuracy: 0.5000\n",
      "Epoch 107/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6726 - accuracy: 0.5000\n",
      "Epoch 108/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6723 - accuracy: 0.5000\n",
      "Epoch 109/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6720 - accuracy: 0.5000\n",
      "Epoch 110/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6718 - accuracy: 0.5000\n",
      "Epoch 111/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6715 - accuracy: 0.5000\n",
      "Epoch 112/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6712 - accuracy: 0.5000\n",
      "Epoch 113/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6710 - accuracy: 0.5000\n",
      "Epoch 114/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6707 - accuracy: 0.5000\n",
      "Epoch 115/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6705 - accuracy: 0.5000\n",
      "Epoch 116/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6702 - accuracy: 0.5000\n",
      "Epoch 117/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6700 - accuracy: 0.5000\n",
      "Epoch 118/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6697 - accuracy: 0.5000\n",
      "Epoch 119/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6695 - accuracy: 0.5000\n",
      "Epoch 120/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6692 - accuracy: 0.5000\n",
      "Epoch 121/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6689 - accuracy: 0.5000\n",
      "Epoch 122/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6687 - accuracy: 0.5000\n",
      "Epoch 123/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6685 - accuracy: 0.5000\n",
      "Epoch 124/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6682 - accuracy: 0.5000\n",
      "Epoch 125/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6680 - accuracy: 0.5000\n",
      "Epoch 126/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6677 - accuracy: 0.5000\n",
      "Epoch 127/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6675 - accuracy: 0.5000\n",
      "Epoch 128/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6672 - accuracy: 0.5000\n",
      "Epoch 129/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6670 - accuracy: 0.5000\n",
      "Epoch 130/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6667 - accuracy: 0.5000\n",
      "Epoch 131/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6665 - accuracy: 0.5000\n",
      "Epoch 132/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6663 - accuracy: 0.5000\n",
      "Epoch 133/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6660 - accuracy: 0.5000\n",
      "Epoch 134/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6658 - accuracy: 0.5000\n",
      "Epoch 135/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6655 - accuracy: 0.5000\n",
      "Epoch 136/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6653 - accuracy: 0.5000\n",
      "Epoch 137/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6651 - accuracy: 0.5000\n",
      "Epoch 138/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6648 - accuracy: 0.5000\n",
      "Epoch 139/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6646 - accuracy: 0.5000\n",
      "Epoch 140/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6644 - accuracy: 0.5000\n",
      "Epoch 141/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6641 - accuracy: 0.5000\n",
      "Epoch 142/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6639 - accuracy: 0.5000\n",
      "Epoch 143/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6637 - accuracy: 0.5000\n",
      "Epoch 144/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6635 - accuracy: 0.5000\n",
      "Epoch 145/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6632 - accuracy: 0.5000\n",
      "Epoch 146/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6630 - accuracy: 0.5000\n",
      "Epoch 147/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6628 - accuracy: 0.5000\n",
      "Epoch 148/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6625 - accuracy: 0.5000\n",
      "Epoch 149/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6623 - accuracy: 0.5000\n",
      "Epoch 150/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6621 - accuracy: 0.5000\n",
      "Epoch 151/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6619 - accuracy: 0.5000\n",
      "Epoch 152/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6617 - accuracy: 0.5000\n",
      "Epoch 153/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6614 - accuracy: 0.5000\n",
      "Epoch 154/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6612 - accuracy: 0.5000\n",
      "Epoch 155/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6610 - accuracy: 0.5000\n",
      "Epoch 156/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6608 - accuracy: 0.5000\n",
      "Epoch 157/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6606 - accuracy: 0.5000\n",
      "Epoch 158/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6604 - accuracy: 0.5000\n",
      "Epoch 159/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6601 - accuracy: 0.5000\n",
      "Epoch 160/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6599 - accuracy: 0.5000\n",
      "Epoch 161/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6597 - accuracy: 0.5000\n",
      "Epoch 162/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6595 - accuracy: 0.5000\n",
      "Epoch 163/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6593 - accuracy: 0.5000\n",
      "Epoch 164/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6591 - accuracy: 0.5000\n",
      "Epoch 165/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6589 - accuracy: 0.5000\n",
      "Epoch 166/3500\n",
      "22/22 [==============================] - 0s 545us/step - loss: 0.6587 - accuracy: 0.5000\n",
      "Epoch 167/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6585 - accuracy: 0.5000\n",
      "Epoch 168/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6583 - accuracy: 0.5000\n",
      "Epoch 169/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6580 - accuracy: 0.5000\n",
      "Epoch 170/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6578 - accuracy: 0.5000\n",
      "Epoch 171/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6576 - accuracy: 0.5000\n",
      "Epoch 172/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6574 - accuracy: 0.5000\n",
      "Epoch 173/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6572 - accuracy: 0.5455\n",
      "Epoch 174/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6570 - accuracy: 0.5455\n",
      "Epoch 175/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6568 - accuracy: 0.5455\n",
      "Epoch 176/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6566 - accuracy: 0.5455\n",
      "Epoch 177/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6564 - accuracy: 0.5455\n",
      "Epoch 178/3500\n",
      "22/22 [==============================] - 0s 228us/step - loss: 0.6562 - accuracy: 0.5455\n",
      "Epoch 179/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6560 - accuracy: 0.5455\n",
      "Epoch 180/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6558 - accuracy: 0.5455\n",
      "Epoch 181/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6557 - accuracy: 0.5455\n",
      "Epoch 182/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6555 - accuracy: 0.5455\n",
      "Epoch 183/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6553 - accuracy: 0.5455\n",
      "Epoch 184/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.6551 - accuracy: 0.5455\n",
      "Epoch 185/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.6549 - accuracy: 0.5455\n",
      "Epoch 186/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6547 - accuracy: 0.5455\n",
      "Epoch 187/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6545 - accuracy: 0.5455\n",
      "Epoch 188/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6543 - accuracy: 0.5455\n",
      "Epoch 189/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6541 - accuracy: 0.5455\n",
      "Epoch 190/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6539 - accuracy: 0.5455\n",
      "Epoch 191/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6537 - accuracy: 0.5455\n",
      "Epoch 192/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6536 - accuracy: 0.5455\n",
      "Epoch 193/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6534 - accuracy: 0.5455\n",
      "Epoch 194/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6532 - accuracy: 0.5455\n",
      "Epoch 195/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6530 - accuracy: 0.5455\n",
      "Epoch 196/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6528 - accuracy: 0.5455\n",
      "Epoch 197/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6526 - accuracy: 0.5455\n",
      "Epoch 198/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6525 - accuracy: 0.5455\n",
      "Epoch 199/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6523 - accuracy: 0.5455\n",
      "Epoch 200/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6521 - accuracy: 0.5455\n",
      "Epoch 201/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6519 - accuracy: 0.5455\n",
      "Epoch 202/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6518 - accuracy: 0.5455\n",
      "Epoch 203/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6516 - accuracy: 0.5455\n",
      "Epoch 204/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6514 - accuracy: 0.5455\n",
      "Epoch 205/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6512 - accuracy: 0.5455\n",
      "Epoch 206/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6510 - accuracy: 0.5455\n",
      "Epoch 207/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6509 - accuracy: 0.5455\n",
      "Epoch 208/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6507 - accuracy: 0.5455\n",
      "Epoch 209/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6505 - accuracy: 0.5455\n",
      "Epoch 210/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6504 - accuracy: 0.5455\n",
      "Epoch 211/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6502 - accuracy: 0.5455\n",
      "Epoch 212/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6500 - accuracy: 0.5455\n",
      "Epoch 213/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6498 - accuracy: 0.5455\n",
      "Epoch 214/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6497 - accuracy: 0.5455\n",
      "Epoch 215/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6495 - accuracy: 0.5455\n",
      "Epoch 216/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6493 - accuracy: 0.5455\n",
      "Epoch 217/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6492 - accuracy: 0.5455\n",
      "Epoch 218/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6490 - accuracy: 0.5455\n",
      "Epoch 219/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6489 - accuracy: 0.5455\n",
      "Epoch 220/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6487 - accuracy: 0.5455\n",
      "Epoch 221/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6485 - accuracy: 0.5455\n",
      "Epoch 222/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6484 - accuracy: 0.5455\n",
      "Epoch 223/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6482 - accuracy: 0.5455\n",
      "Epoch 224/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.6480 - accuracy: 0.5455\n",
      "Epoch 225/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6479 - accuracy: 0.5455\n",
      "Epoch 226/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6477 - accuracy: 0.5455\n",
      "Epoch 227/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6476 - accuracy: 0.5455\n",
      "Epoch 228/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6474 - accuracy: 0.5455\n",
      "Epoch 229/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6472 - accuracy: 0.5455\n",
      "Epoch 230/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6471 - accuracy: 0.5455\n",
      "Epoch 231/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6469 - accuracy: 0.5455\n",
      "Epoch 232/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6468 - accuracy: 0.5455\n",
      "Epoch 233/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6466 - accuracy: 0.5455\n",
      "Epoch 234/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6465 - accuracy: 0.5909\n",
      "Epoch 235/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6463 - accuracy: 0.5909\n",
      "Epoch 236/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6462 - accuracy: 0.5909\n",
      "Epoch 237/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6460 - accuracy: 0.5909\n",
      "Epoch 238/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6459 - accuracy: 0.5909\n",
      "Epoch 239/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6457 - accuracy: 0.5909\n",
      "Epoch 240/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6456 - accuracy: 0.5909\n",
      "Epoch 241/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6454 - accuracy: 0.5909\n",
      "Epoch 242/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6453 - accuracy: 0.5909\n",
      "Epoch 243/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6451 - accuracy: 0.6364\n",
      "Epoch 244/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6450 - accuracy: 0.6364\n",
      "Epoch 245/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6448 - accuracy: 0.6364\n",
      "Epoch 246/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6447 - accuracy: 0.6364\n",
      "Epoch 247/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6445 - accuracy: 0.6364\n",
      "Epoch 248/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6444 - accuracy: 0.6364\n",
      "Epoch 249/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6443 - accuracy: 0.6364\n",
      "Epoch 250/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6441 - accuracy: 0.6364\n",
      "Epoch 251/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6440 - accuracy: 0.6364\n",
      "Epoch 252/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6438 - accuracy: 0.6364\n",
      "Epoch 253/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6437 - accuracy: 0.6364\n",
      "Epoch 254/3500\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.6435 - accuracy: 0.6364\n",
      "Epoch 255/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6434 - accuracy: 0.6364\n",
      "Epoch 256/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6433 - accuracy: 0.6364\n",
      "Epoch 257/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6431 - accuracy: 0.6364\n",
      "Epoch 258/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.6430 - accuracy: 0.6364\n",
      "Epoch 259/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6429 - accuracy: 0.6364\n",
      "Epoch 260/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6427 - accuracy: 0.6818\n",
      "Epoch 261/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6426 - accuracy: 0.6818\n",
      "Epoch 262/3500\n",
      "22/22 [==============================] - 0s 636us/step - loss: 0.6424 - accuracy: 0.6818\n",
      "Epoch 263/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6423 - accuracy: 0.6818\n",
      "Epoch 264/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6422 - accuracy: 0.6818\n",
      "Epoch 265/3500\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.6420 - accuracy: 0.6818\n",
      "Epoch 266/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6419 - accuracy: 0.6818\n",
      "Epoch 267/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6418 - accuracy: 0.6818\n",
      "Epoch 268/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6417 - accuracy: 0.6818\n",
      "Epoch 269/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6415 - accuracy: 0.6818\n",
      "Epoch 270/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6414 - accuracy: 0.6818\n",
      "Epoch 271/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6413 - accuracy: 0.6818\n",
      "Epoch 272/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6411 - accuracy: 0.6818\n",
      "Epoch 273/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6410 - accuracy: 0.6818\n",
      "Epoch 274/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6409 - accuracy: 0.6818\n",
      "Epoch 275/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6407 - accuracy: 0.6818\n",
      "Epoch 276/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6406 - accuracy: 0.6818\n",
      "Epoch 277/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6405 - accuracy: 0.6818\n",
      "Epoch 278/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6404 - accuracy: 0.6818\n",
      "Epoch 279/3500\n",
      "22/22 [==============================] - 0s 46us/step - loss: 0.6402 - accuracy: 0.6818\n",
      "Epoch 280/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6401 - accuracy: 0.6818\n",
      "Epoch 281/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6400 - accuracy: 0.6818\n",
      "Epoch 282/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6399 - accuracy: 0.6818\n",
      "Epoch 283/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6398 - accuracy: 0.6818\n",
      "Epoch 284/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6396 - accuracy: 0.6818\n",
      "Epoch 285/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6395 - accuracy: 0.6818\n",
      "Epoch 286/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6394 - accuracy: 0.6818\n",
      "Epoch 287/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6393 - accuracy: 0.6818\n",
      "Epoch 288/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6391 - accuracy: 0.6818\n",
      "Epoch 289/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6390 - accuracy: 0.6818\n",
      "Epoch 290/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6389 - accuracy: 0.6818\n",
      "Epoch 291/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6388 - accuracy: 0.6818\n",
      "Epoch 292/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6387 - accuracy: 0.6818\n",
      "Epoch 293/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6386 - accuracy: 0.6818\n",
      "Epoch 294/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6384 - accuracy: 0.6818\n",
      "Epoch 295/3500\n",
      "22/22 [==============================] - 0s 682us/step - loss: 0.6383 - accuracy: 0.6818\n",
      "Epoch 296/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6382 - accuracy: 0.6818\n",
      "Epoch 297/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6381 - accuracy: 0.6818\n",
      "Epoch 298/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6380 - accuracy: 0.6818\n",
      "Epoch 299/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6379 - accuracy: 0.6818\n",
      "Epoch 300/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6378 - accuracy: 0.6818\n",
      "Epoch 301/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6376 - accuracy: 0.6818\n",
      "Epoch 302/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6375 - accuracy: 0.6818\n",
      "Epoch 303/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6374 - accuracy: 0.6818\n",
      "Epoch 304/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6373 - accuracy: 0.6818\n",
      "Epoch 305/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6372 - accuracy: 0.6818\n",
      "Epoch 306/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6371 - accuracy: 0.6818\n",
      "Epoch 307/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6370 - accuracy: 0.6818\n",
      "Epoch 308/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6369 - accuracy: 0.6818\n",
      "Epoch 309/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6368 - accuracy: 0.6818\n",
      "Epoch 310/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6366 - accuracy: 0.6818\n",
      "Epoch 311/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6365 - accuracy: 0.6818\n",
      "Epoch 312/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6364 - accuracy: 0.6818\n",
      "Epoch 313/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6363 - accuracy: 0.6818\n",
      "Epoch 314/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6362 - accuracy: 0.6818\n",
      "Epoch 315/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6361 - accuracy: 0.6818\n",
      "Epoch 316/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6360 - accuracy: 0.6818\n",
      "Epoch 317/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6359 - accuracy: 0.6818\n",
      "Epoch 318/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6358 - accuracy: 0.6818\n",
      "Epoch 319/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6357 - accuracy: 0.6818\n",
      "Epoch 320/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6356 - accuracy: 0.6818\n",
      "Epoch 321/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6355 - accuracy: 0.6818\n",
      "Epoch 322/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6354 - accuracy: 0.6818\n",
      "Epoch 323/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6353 - accuracy: 0.6818\n",
      "Epoch 324/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6352 - accuracy: 0.6818\n",
      "Epoch 325/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6351 - accuracy: 0.6818\n",
      "Epoch 326/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6350 - accuracy: 0.6818\n",
      "Epoch 327/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6349 - accuracy: 0.6818\n",
      "Epoch 328/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.6348 - accuracy: 0.6818\n",
      "Epoch 329/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6347 - accuracy: 0.6818\n",
      "Epoch 330/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6346 - accuracy: 0.6818\n",
      "Epoch 331/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6345 - accuracy: 0.6818\n",
      "Epoch 332/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6344 - accuracy: 0.6818\n",
      "Epoch 333/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6343 - accuracy: 0.6818\n",
      "Epoch 334/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6342 - accuracy: 0.6818\n",
      "Epoch 335/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6341 - accuracy: 0.6818\n",
      "Epoch 336/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6340 - accuracy: 0.6818\n",
      "Epoch 337/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6339 - accuracy: 0.6818\n",
      "Epoch 338/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6338 - accuracy: 0.6818\n",
      "Epoch 339/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6337 - accuracy: 0.6818\n",
      "Epoch 340/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6336 - accuracy: 0.6818\n",
      "Epoch 341/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6335 - accuracy: 0.6818\n",
      "Epoch 342/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6334 - accuracy: 0.6818\n",
      "Epoch 343/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6333 - accuracy: 0.6818\n",
      "Epoch 344/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6332 - accuracy: 0.6818\n",
      "Epoch 345/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6331 - accuracy: 0.6818\n",
      "Epoch 346/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6330 - accuracy: 0.6818\n",
      "Epoch 347/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6329 - accuracy: 0.6818\n",
      "Epoch 348/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6329 - accuracy: 0.6818\n",
      "Epoch 349/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6328 - accuracy: 0.6818\n",
      "Epoch 350/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6327 - accuracy: 0.6818\n",
      "Epoch 351/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6326 - accuracy: 0.6818\n",
      "Epoch 352/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6325 - accuracy: 0.6818\n",
      "Epoch 353/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6324 - accuracy: 0.6818\n",
      "Epoch 354/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6323 - accuracy: 0.6818\n",
      "Epoch 355/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6322 - accuracy: 0.6818\n",
      "Epoch 356/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6321 - accuracy: 0.6818\n",
      "Epoch 357/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6320 - accuracy: 0.6818\n",
      "Epoch 358/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6320 - accuracy: 0.6818\n",
      "Epoch 359/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6319 - accuracy: 0.6818\n",
      "Epoch 360/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6318 - accuracy: 0.6818\n",
      "Epoch 361/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6317 - accuracy: 0.6818\n",
      "Epoch 362/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6316 - accuracy: 0.6818\n",
      "Epoch 363/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6315 - accuracy: 0.6818\n",
      "Epoch 364/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6314 - accuracy: 0.6818\n",
      "Epoch 365/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6313 - accuracy: 0.6818\n",
      "Epoch 366/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6313 - accuracy: 0.6818\n",
      "Epoch 367/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6312 - accuracy: 0.6818\n",
      "Epoch 368/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6311 - accuracy: 0.6818\n",
      "Epoch 369/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6310 - accuracy: 0.6818\n",
      "Epoch 370/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6309 - accuracy: 0.6818\n",
      "Epoch 371/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6308 - accuracy: 0.6818\n",
      "Epoch 372/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6307 - accuracy: 0.6818\n",
      "Epoch 373/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6307 - accuracy: 0.6818\n",
      "Epoch 374/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6306 - accuracy: 0.6818\n",
      "Epoch 375/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6305 - accuracy: 0.6818\n",
      "Epoch 376/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6304 - accuracy: 0.6818\n",
      "Epoch 377/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6303 - accuracy: 0.6818\n",
      "Epoch 378/3500\n",
      "22/22 [==============================] - 0s 500us/step - loss: 0.6303 - accuracy: 0.6818\n",
      "Epoch 379/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6302 - accuracy: 0.6818\n",
      "Epoch 380/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6301 - accuracy: 0.6818\n",
      "Epoch 381/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6300 - accuracy: 0.6818\n",
      "Epoch 382/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6299 - accuracy: 0.6818\n",
      "Epoch 383/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6298 - accuracy: 0.6818\n",
      "Epoch 384/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6298 - accuracy: 0.6818\n",
      "Epoch 385/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6297 - accuracy: 0.6818\n",
      "Epoch 386/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6296 - accuracy: 0.6818\n",
      "Epoch 387/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6295 - accuracy: 0.6818\n",
      "Epoch 388/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6294 - accuracy: 0.6818\n",
      "Epoch 389/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6294 - accuracy: 0.6818\n",
      "Epoch 390/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6293 - accuracy: 0.6818\n",
      "Epoch 391/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6292 - accuracy: 0.6818\n",
      "Epoch 392/3500\n",
      "22/22 [==============================] - 0s 46us/step - loss: 0.6291 - accuracy: 0.6818\n",
      "Epoch 393/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6291 - accuracy: 0.6818\n",
      "Epoch 394/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6290 - accuracy: 0.6818\n",
      "Epoch 395/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6289 - accuracy: 0.6818\n",
      "Epoch 396/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6288 - accuracy: 0.6818\n",
      "Epoch 397/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6287 - accuracy: 0.6818\n",
      "Epoch 398/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6287 - accuracy: 0.6818\n",
      "Epoch 399/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6286 - accuracy: 0.6818\n",
      "Epoch 400/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6285 - accuracy: 0.6818\n",
      "Epoch 401/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6284 - accuracy: 0.6818\n",
      "Epoch 402/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6284 - accuracy: 0.6818\n",
      "Epoch 403/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6283 - accuracy: 0.6818\n",
      "Epoch 404/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6282 - accuracy: 0.6818\n",
      "Epoch 405/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6281 - accuracy: 0.6818\n",
      "Epoch 406/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6281 - accuracy: 0.6818\n",
      "Epoch 407/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6280 - accuracy: 0.6818\n",
      "Epoch 408/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6279 - accuracy: 0.6818\n",
      "Epoch 409/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6278 - accuracy: 0.6818\n",
      "Epoch 410/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6278 - accuracy: 0.6818\n",
      "Epoch 411/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6277 - accuracy: 0.6818\n",
      "Epoch 412/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6276 - accuracy: 0.6818\n",
      "Epoch 413/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6275 - accuracy: 0.6818\n",
      "Epoch 414/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6275 - accuracy: 0.6818\n",
      "Epoch 415/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6274 - accuracy: 0.6818\n",
      "Epoch 416/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6273 - accuracy: 0.6818\n",
      "Epoch 417/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6273 - accuracy: 0.6818\n",
      "Epoch 418/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6272 - accuracy: 0.6818\n",
      "Epoch 419/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6271 - accuracy: 0.6818\n",
      "Epoch 420/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6270 - accuracy: 0.6818\n",
      "Epoch 421/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6270 - accuracy: 0.6818\n",
      "Epoch 422/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6269 - accuracy: 0.6818\n",
      "Epoch 423/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6268 - accuracy: 0.6818\n",
      "Epoch 424/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6268 - accuracy: 0.6818\n",
      "Epoch 425/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6267 - accuracy: 0.6818\n",
      "Epoch 426/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6266 - accuracy: 0.6818\n",
      "Epoch 427/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6265 - accuracy: 0.6818\n",
      "Epoch 428/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6265 - accuracy: 0.6818\n",
      "Epoch 429/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6264 - accuracy: 0.6818\n",
      "Epoch 430/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6263 - accuracy: 0.6818\n",
      "Epoch 431/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6263 - accuracy: 0.6818\n",
      "Epoch 432/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6262 - accuracy: 0.6818\n",
      "Epoch 433/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6261 - accuracy: 0.6818\n",
      "Epoch 434/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6261 - accuracy: 0.6818\n",
      "Epoch 435/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6260 - accuracy: 0.6818\n",
      "Epoch 436/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.6259 - accuracy: 0.6818\n",
      "Epoch 437/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6258 - accuracy: 0.6818\n",
      "Epoch 438/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6258 - accuracy: 0.6818\n",
      "Epoch 439/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6257 - accuracy: 0.6818\n",
      "Epoch 440/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6256 - accuracy: 0.6818\n",
      "Epoch 441/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6256 - accuracy: 0.6818\n",
      "Epoch 442/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6255 - accuracy: 0.6818\n",
      "Epoch 443/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6254 - accuracy: 0.6818\n",
      "Epoch 444/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6254 - accuracy: 0.6818\n",
      "Epoch 445/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6253 - accuracy: 0.6818\n",
      "Epoch 446/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6252 - accuracy: 0.6818\n",
      "Epoch 447/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6252 - accuracy: 0.6818\n",
      "Epoch 448/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6251 - accuracy: 0.6818\n",
      "Epoch 449/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6250 - accuracy: 0.6818\n",
      "Epoch 450/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6250 - accuracy: 0.6818\n",
      "Epoch 451/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6249 - accuracy: 0.6818\n",
      "Epoch 452/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6248 - accuracy: 0.6818\n",
      "Epoch 453/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6248 - accuracy: 0.6818\n",
      "Epoch 454/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6247 - accuracy: 0.6818\n",
      "Epoch 455/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6246 - accuracy: 0.6818\n",
      "Epoch 456/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6246 - accuracy: 0.6818\n",
      "Epoch 457/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6245 - accuracy: 0.6818\n",
      "Epoch 458/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6244 - accuracy: 0.6818\n",
      "Epoch 459/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6244 - accuracy: 0.6818\n",
      "Epoch 460/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6243 - accuracy: 0.6818\n",
      "Epoch 461/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6243 - accuracy: 0.6818\n",
      "Epoch 462/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6242 - accuracy: 0.6818\n",
      "Epoch 463/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6241 - accuracy: 0.6818\n",
      "Epoch 464/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6241 - accuracy: 0.6818\n",
      "Epoch 465/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6240 - accuracy: 0.6818\n",
      "Epoch 466/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6239 - accuracy: 0.6818\n",
      "Epoch 467/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6239 - accuracy: 0.6818\n",
      "Epoch 468/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6238 - accuracy: 0.6818\n",
      "Epoch 469/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6237 - accuracy: 0.6818\n",
      "Epoch 470/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6237 - accuracy: 0.6818\n",
      "Epoch 471/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6236 - accuracy: 0.6818\n",
      "Epoch 472/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6235 - accuracy: 0.6818\n",
      "Epoch 473/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6235 - accuracy: 0.6818\n",
      "Epoch 474/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6234 - accuracy: 0.6818\n",
      "Epoch 475/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6234 - accuracy: 0.6818\n",
      "Epoch 476/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6233 - accuracy: 0.6818\n",
      "Epoch 477/3500\n",
      "22/22 [==============================] - 0s 727us/step - loss: 0.6232 - accuracy: 0.6818\n",
      "Epoch 478/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6232 - accuracy: 0.6818\n",
      "Epoch 479/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6231 - accuracy: 0.6818\n",
      "Epoch 480/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6230 - accuracy: 0.6818\n",
      "Epoch 481/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6230 - accuracy: 0.6818\n",
      "Epoch 482/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6229 - accuracy: 0.6818\n",
      "Epoch 483/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6229 - accuracy: 0.6818\n",
      "Epoch 484/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6228 - accuracy: 0.6818\n",
      "Epoch 485/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6227 - accuracy: 0.6818\n",
      "Epoch 486/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6227 - accuracy: 0.6818\n",
      "Epoch 487/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6226 - accuracy: 0.6818\n",
      "Epoch 488/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6225 - accuracy: 0.6818\n",
      "Epoch 489/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6225 - accuracy: 0.6818\n",
      "Epoch 490/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6224 - accuracy: 0.6818\n",
      "Epoch 491/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6224 - accuracy: 0.6818\n",
      "Epoch 492/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6223 - accuracy: 0.6818\n",
      "Epoch 493/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6222 - accuracy: 0.6818\n",
      "Epoch 494/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6222 - accuracy: 0.6818\n",
      "Epoch 495/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6221 - accuracy: 0.6818\n",
      "Epoch 496/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6221 - accuracy: 0.6818\n",
      "Epoch 497/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6220 - accuracy: 0.6818\n",
      "Epoch 498/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6219 - accuracy: 0.6818\n",
      "Epoch 499/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6219 - accuracy: 0.6818\n",
      "Epoch 500/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6218 - accuracy: 0.6818\n",
      "Epoch 501/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6218 - accuracy: 0.6818\n",
      "Epoch 502/3500\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.6217 - accuracy: 0.6818\n",
      "Epoch 503/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6216 - accuracy: 0.6818\n",
      "Epoch 504/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6216 - accuracy: 0.6818\n",
      "Epoch 505/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6215 - accuracy: 0.6818\n",
      "Epoch 506/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6215 - accuracy: 0.6818\n",
      "Epoch 507/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6214 - accuracy: 0.6818\n",
      "Epoch 508/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6213 - accuracy: 0.6818\n",
      "Epoch 509/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6213 - accuracy: 0.6818\n",
      "Epoch 510/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6212 - accuracy: 0.6818\n",
      "Epoch 511/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6212 - accuracy: 0.6818\n",
      "Epoch 512/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6211 - accuracy: 0.6818\n",
      "Epoch 513/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6210 - accuracy: 0.6818\n",
      "Epoch 514/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6210 - accuracy: 0.6818\n",
      "Epoch 515/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6209 - accuracy: 0.6818\n",
      "Epoch 516/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6209 - accuracy: 0.6818\n",
      "Epoch 517/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6208 - accuracy: 0.6818\n",
      "Epoch 518/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6207 - accuracy: 0.6818\n",
      "Epoch 519/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6207 - accuracy: 0.6818\n",
      "Epoch 520/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6206 - accuracy: 0.6818\n",
      "Epoch 521/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6206 - accuracy: 0.6818\n",
      "Epoch 522/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6205 - accuracy: 0.6818\n",
      "Epoch 523/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6204 - accuracy: 0.6818\n",
      "Epoch 524/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6204 - accuracy: 0.6818\n",
      "Epoch 525/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6203 - accuracy: 0.6818\n",
      "Epoch 526/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6203 - accuracy: 0.6818\n",
      "Epoch 527/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6202 - accuracy: 0.6818\n",
      "Epoch 528/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6201 - accuracy: 0.6818\n",
      "Epoch 529/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6201 - accuracy: 0.6818\n",
      "Epoch 530/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6200 - accuracy: 0.6818\n",
      "Epoch 531/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6200 - accuracy: 0.6818\n",
      "Epoch 532/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6199 - accuracy: 0.6818\n",
      "Epoch 533/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6199 - accuracy: 0.6818\n",
      "Epoch 534/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6198 - accuracy: 0.6818\n",
      "Epoch 535/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6197 - accuracy: 0.6818\n",
      "Epoch 536/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6197 - accuracy: 0.6818\n",
      "Epoch 537/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6196 - accuracy: 0.6818\n",
      "Epoch 538/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6196 - accuracy: 0.6818\n",
      "Epoch 539/3500\n",
      "22/22 [==============================] - 0s 500us/step - loss: 0.6195 - accuracy: 0.6818\n",
      "Epoch 540/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6194 - accuracy: 0.6818\n",
      "Epoch 541/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6194 - accuracy: 0.6818\n",
      "Epoch 542/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6193 - accuracy: 0.6818\n",
      "Epoch 543/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6193 - accuracy: 0.6818\n",
      "Epoch 544/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6192 - accuracy: 0.6818\n",
      "Epoch 545/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6192 - accuracy: 0.6818\n",
      "Epoch 546/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6191 - accuracy: 0.6818\n",
      "Epoch 547/3500\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.6190 - accuracy: 0.6818\n",
      "Epoch 548/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6190 - accuracy: 0.6818\n",
      "Epoch 549/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6189 - accuracy: 0.6818\n",
      "Epoch 550/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6189 - accuracy: 0.6818\n",
      "Epoch 551/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6188 - accuracy: 0.6818\n",
      "Epoch 552/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6188 - accuracy: 0.6818\n",
      "Epoch 553/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.6187 - accuracy: 0.6818\n",
      "Epoch 554/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6186 - accuracy: 0.6818\n",
      "Epoch 555/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6186 - accuracy: 0.6818\n",
      "Epoch 556/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6185 - accuracy: 0.6818\n",
      "Epoch 557/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6185 - accuracy: 0.6818\n",
      "Epoch 558/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6184 - accuracy: 0.6818\n",
      "Epoch 559/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6184 - accuracy: 0.6818\n",
      "Epoch 560/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6183 - accuracy: 0.6818\n",
      "Epoch 561/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6182 - accuracy: 0.6818\n",
      "Epoch 562/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6182 - accuracy: 0.6818\n",
      "Epoch 563/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6181 - accuracy: 0.6818\n",
      "Epoch 564/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6181 - accuracy: 0.6818\n",
      "Epoch 565/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6180 - accuracy: 0.6818\n",
      "Epoch 566/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6180 - accuracy: 0.6818\n",
      "Epoch 567/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6179 - accuracy: 0.6818\n",
      "Epoch 568/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6178 - accuracy: 0.6818\n",
      "Epoch 569/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6178 - accuracy: 0.6818\n",
      "Epoch 570/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6177 - accuracy: 0.6818\n",
      "Epoch 571/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6177 - accuracy: 0.6818\n",
      "Epoch 572/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6176 - accuracy: 0.6818\n",
      "Epoch 573/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6176 - accuracy: 0.6818\n",
      "Epoch 574/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6175 - accuracy: 0.6818\n",
      "Epoch 575/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6174 - accuracy: 0.6818\n",
      "Epoch 576/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6174 - accuracy: 0.6818\n",
      "Epoch 577/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6173 - accuracy: 0.6818\n",
      "Epoch 578/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6173 - accuracy: 0.6818\n",
      "Epoch 579/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6172 - accuracy: 0.6818\n",
      "Epoch 580/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6172 - accuracy: 0.6818\n",
      "Epoch 581/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6171 - accuracy: 0.6818\n",
      "Epoch 582/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6171 - accuracy: 0.6818\n",
      "Epoch 583/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6170 - accuracy: 0.6818\n",
      "Epoch 584/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6169 - accuracy: 0.6818\n",
      "Epoch 585/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6169 - accuracy: 0.6818\n",
      "Epoch 586/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.6168 - accuracy: 0.6818\n",
      "Epoch 587/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6168 - accuracy: 0.6818\n",
      "Epoch 588/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6167 - accuracy: 0.6818\n",
      "Epoch 589/3500\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.6167 - accuracy: 0.6818\n",
      "Epoch 590/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.6166 - accuracy: 0.6818\n",
      "Epoch 591/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6165 - accuracy: 0.6818\n",
      "Epoch 592/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6165 - accuracy: 0.6818\n",
      "Epoch 593/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6164 - accuracy: 0.6818\n",
      "Epoch 594/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6164 - accuracy: 0.6818\n",
      "Epoch 595/3500\n",
      "22/22 [==============================] - 0s 1000us/step - loss: 0.6163 - accuracy: 0.6818\n",
      "Epoch 596/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6163 - accuracy: 0.6818\n",
      "Epoch 597/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6162 - accuracy: 0.6818\n",
      "Epoch 598/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6162 - accuracy: 0.6818\n",
      "Epoch 599/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6161 - accuracy: 0.6818\n",
      "Epoch 600/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6160 - accuracy: 0.6818\n",
      "Epoch 601/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6160 - accuracy: 0.6818\n",
      "Epoch 602/3500\n",
      "22/22 [==============================] - 0s 727us/step - loss: 0.6159 - accuracy: 0.6818\n",
      "Epoch 603/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6159 - accuracy: 0.6818\n",
      "Epoch 604/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6158 - accuracy: 0.6818\n",
      "Epoch 605/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6158 - accuracy: 0.6818\n",
      "Epoch 606/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6157 - accuracy: 0.6818\n",
      "Epoch 607/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6156 - accuracy: 0.6818\n",
      "Epoch 608/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6156 - accuracy: 0.6818\n",
      "Epoch 609/3500\n",
      "22/22 [==============================] - 0s 909us/step - loss: 0.6155 - accuracy: 0.6818\n",
      "Epoch 610/3500\n",
      "22/22 [==============================] - 0s 500us/step - loss: 0.6155 - accuracy: 0.6818\n",
      "Epoch 611/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6154 - accuracy: 0.6818\n",
      "Epoch 612/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6154 - accuracy: 0.6818\n",
      "Epoch 613/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6153 - accuracy: 0.6818\n",
      "Epoch 614/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6153 - accuracy: 0.6818\n",
      "Epoch 615/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6152 - accuracy: 0.6818\n",
      "Epoch 616/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6151 - accuracy: 0.6818\n",
      "Epoch 617/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6151 - accuracy: 0.6818\n",
      "Epoch 618/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6150 - accuracy: 0.6818\n",
      "Epoch 619/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6150 - accuracy: 0.6818\n",
      "Epoch 620/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6149 - accuracy: 0.6818\n",
      "Epoch 621/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6149 - accuracy: 0.6818\n",
      "Epoch 622/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6148 - accuracy: 0.6818\n",
      "Epoch 623/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6148 - accuracy: 0.6818\n",
      "Epoch 624/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6147 - accuracy: 0.6818\n",
      "Epoch 625/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6146 - accuracy: 0.6818\n",
      "Epoch 626/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6146 - accuracy: 0.6818\n",
      "Epoch 627/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6145 - accuracy: 0.6818\n",
      "Epoch 628/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6145 - accuracy: 0.6818\n",
      "Epoch 629/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6144 - accuracy: 0.6818\n",
      "Epoch 630/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.6144 - accuracy: 0.6818\n",
      "Epoch 631/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6143 - accuracy: 0.6818\n",
      "Epoch 632/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6143 - accuracy: 0.6818\n",
      "Epoch 633/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6142 - accuracy: 0.6818\n",
      "Epoch 634/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6141 - accuracy: 0.6818\n",
      "Epoch 635/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6141 - accuracy: 0.6818\n",
      "Epoch 636/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6140 - accuracy: 0.6818\n",
      "Epoch 637/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6140 - accuracy: 0.6818\n",
      "Epoch 638/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6139 - accuracy: 0.6818\n",
      "Epoch 639/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6139 - accuracy: 0.6818\n",
      "Epoch 640/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6138 - accuracy: 0.6818\n",
      "Epoch 641/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6138 - accuracy: 0.6818\n",
      "Epoch 642/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6137 - accuracy: 0.6818\n",
      "Epoch 643/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6136 - accuracy: 0.6818\n",
      "Epoch 644/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.6136 - accuracy: 0.6818\n",
      "Epoch 645/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6135 - accuracy: 0.6818\n",
      "Epoch 646/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6135 - accuracy: 0.6818\n",
      "Epoch 647/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6134 - accuracy: 0.6818\n",
      "Epoch 648/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6134 - accuracy: 0.6818\n",
      "Epoch 649/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6133 - accuracy: 0.6818\n",
      "Epoch 650/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6133 - accuracy: 0.6818\n",
      "Epoch 651/3500\n",
      "22/22 [==============================] - 0s 500us/step - loss: 0.6132 - accuracy: 0.6818\n",
      "Epoch 652/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6131 - accuracy: 0.6818\n",
      "Epoch 653/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6131 - accuracy: 0.6818\n",
      "Epoch 654/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6130 - accuracy: 0.6818\n",
      "Epoch 655/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6130 - accuracy: 0.6818\n",
      "Epoch 656/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6129 - accuracy: 0.6818\n",
      "Epoch 657/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6129 - accuracy: 0.6818\n",
      "Epoch 658/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6128 - accuracy: 0.6818\n",
      "Epoch 659/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6128 - accuracy: 0.6818\n",
      "Epoch 660/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6127 - accuracy: 0.6818\n",
      "Epoch 661/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6126 - accuracy: 0.6818\n",
      "Epoch 662/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6126 - accuracy: 0.6818\n",
      "Epoch 663/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6125 - accuracy: 0.6818\n",
      "Epoch 664/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6125 - accuracy: 0.6818\n",
      "Epoch 665/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6124 - accuracy: 0.6818\n",
      "Epoch 666/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6124 - accuracy: 0.6818\n",
      "Epoch 667/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6123 - accuracy: 0.6818\n",
      "Epoch 668/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6123 - accuracy: 0.6818\n",
      "Epoch 669/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6122 - accuracy: 0.6818\n",
      "Epoch 670/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6121 - accuracy: 0.6818\n",
      "Epoch 671/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6121 - accuracy: 0.6818\n",
      "Epoch 672/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6120 - accuracy: 0.6818\n",
      "Epoch 673/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6120 - accuracy: 0.6818\n",
      "Epoch 674/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6119 - accuracy: 0.6818\n",
      "Epoch 675/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6119 - accuracy: 0.6818\n",
      "Epoch 676/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6118 - accuracy: 0.6818\n",
      "Epoch 677/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6118 - accuracy: 0.6818\n",
      "Epoch 678/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6117 - accuracy: 0.6818\n",
      "Epoch 679/3500\n",
      "22/22 [==============================] - 0s 636us/step - loss: 0.6116 - accuracy: 0.6818\n",
      "Epoch 680/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6116 - accuracy: 0.6818\n",
      "Epoch 681/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6115 - accuracy: 0.6818\n",
      "Epoch 682/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6115 - accuracy: 0.6818\n",
      "Epoch 683/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6114 - accuracy: 0.6818\n",
      "Epoch 684/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6114 - accuracy: 0.6818\n",
      "Epoch 685/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6113 - accuracy: 0.6818\n",
      "Epoch 686/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.6113 - accuracy: 0.6818\n",
      "Epoch 687/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6112 - accuracy: 0.6818\n",
      "Epoch 688/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6111 - accuracy: 0.6818\n",
      "Epoch 689/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6111 - accuracy: 0.6818\n",
      "Epoch 690/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6110 - accuracy: 0.6818\n",
      "Epoch 691/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6110 - accuracy: 0.6818\n",
      "Epoch 692/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6109 - accuracy: 0.6818\n",
      "Epoch 693/3500\n",
      "22/22 [==============================] - 0s 682us/step - loss: 0.6109 - accuracy: 0.6818\n",
      "Epoch 694/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6108 - accuracy: 0.6818\n",
      "Epoch 695/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6108 - accuracy: 0.6818\n",
      "Epoch 696/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6107 - accuracy: 0.6818\n",
      "Epoch 697/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6107 - accuracy: 0.6818\n",
      "Epoch 698/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6106 - accuracy: 0.6818\n",
      "Epoch 699/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6105 - accuracy: 0.6818\n",
      "Epoch 700/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6105 - accuracy: 0.6818\n",
      "Epoch 701/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6104 - accuracy: 0.6818\n",
      "Epoch 702/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6104 - accuracy: 0.6818\n",
      "Epoch 703/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6103 - accuracy: 0.6818\n",
      "Epoch 704/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6103 - accuracy: 0.6818\n",
      "Epoch 705/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6102 - accuracy: 0.6818\n",
      "Epoch 706/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6102 - accuracy: 0.6818\n",
      "Epoch 707/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6101 - accuracy: 0.6818\n",
      "Epoch 708/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6100 - accuracy: 0.6818\n",
      "Epoch 709/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6100 - accuracy: 0.6818\n",
      "Epoch 710/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6099 - accuracy: 0.6818\n",
      "Epoch 711/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6099 - accuracy: 0.6818\n",
      "Epoch 712/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6098 - accuracy: 0.6818\n",
      "Epoch 713/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6098 - accuracy: 0.6818\n",
      "Epoch 714/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6097 - accuracy: 0.6818\n",
      "Epoch 715/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6097 - accuracy: 0.6818\n",
      "Epoch 716/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6096 - accuracy: 0.6818\n",
      "Epoch 717/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6095 - accuracy: 0.6818\n",
      "Epoch 718/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6095 - accuracy: 0.6818\n",
      "Epoch 719/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6094 - accuracy: 0.6818\n",
      "Epoch 720/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6094 - accuracy: 0.6818\n",
      "Epoch 721/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6093 - accuracy: 0.6818\n",
      "Epoch 722/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6093 - accuracy: 0.6818\n",
      "Epoch 723/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6092 - accuracy: 0.6818\n",
      "Epoch 724/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6092 - accuracy: 0.6818\n",
      "Epoch 725/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6091 - accuracy: 0.6818\n",
      "Epoch 726/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6090 - accuracy: 0.6818\n",
      "Epoch 727/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6090 - accuracy: 0.6818\n",
      "Epoch 728/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6089 - accuracy: 0.6818\n",
      "Epoch 729/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6089 - accuracy: 0.6818\n",
      "Epoch 730/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6088 - accuracy: 0.6818\n",
      "Epoch 731/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6088 - accuracy: 0.6818\n",
      "Epoch 732/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6087 - accuracy: 0.6818\n",
      "Epoch 733/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6087 - accuracy: 0.6818\n",
      "Epoch 734/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6086 - accuracy: 0.6818\n",
      "Epoch 735/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6085 - accuracy: 0.6818\n",
      "Epoch 736/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6085 - accuracy: 0.6818\n",
      "Epoch 737/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6084 - accuracy: 0.6818\n",
      "Epoch 738/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6084 - accuracy: 0.6818\n",
      "Epoch 739/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6083 - accuracy: 0.6818\n",
      "Epoch 740/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6083 - accuracy: 0.6818\n",
      "Epoch 741/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6082 - accuracy: 0.6818\n",
      "Epoch 742/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.6082 - accuracy: 0.6818\n",
      "Epoch 743/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6081 - accuracy: 0.6818\n",
      "Epoch 744/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6080 - accuracy: 0.6818\n",
      "Epoch 745/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6080 - accuracy: 0.6818\n",
      "Epoch 746/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6079 - accuracy: 0.6818\n",
      "Epoch 747/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6079 - accuracy: 0.6818\n",
      "Epoch 748/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6078 - accuracy: 0.6818\n",
      "Epoch 749/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6078 - accuracy: 0.6818\n",
      "Epoch 750/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.6077 - accuracy: 0.6818\n",
      "Epoch 751/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6077 - accuracy: 0.6818\n",
      "Epoch 752/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6076 - accuracy: 0.6818\n",
      "Epoch 753/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6075 - accuracy: 0.6818\n",
      "Epoch 754/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6075 - accuracy: 0.6818\n",
      "Epoch 755/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6074 - accuracy: 0.6818\n",
      "Epoch 756/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6074 - accuracy: 0.6818\n",
      "Epoch 757/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6073 - accuracy: 0.6818\n",
      "Epoch 758/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6073 - accuracy: 0.6818\n",
      "Epoch 759/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6072 - accuracy: 0.6818\n",
      "Epoch 760/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6072 - accuracy: 0.6818\n",
      "Epoch 761/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6071 - accuracy: 0.6818\n",
      "Epoch 762/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6070 - accuracy: 0.6818\n",
      "Epoch 763/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6070 - accuracy: 0.6818\n",
      "Epoch 764/3500\n",
      "22/22 [==============================] - 0s 500us/step - loss: 0.6069 - accuracy: 0.6818\n",
      "Epoch 765/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6069 - accuracy: 0.6818\n",
      "Epoch 766/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6068 - accuracy: 0.6818\n",
      "Epoch 767/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6068 - accuracy: 0.6818\n",
      "Epoch 768/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6067 - accuracy: 0.6818\n",
      "Epoch 769/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6067 - accuracy: 0.6818\n",
      "Epoch 770/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6066 - accuracy: 0.6818\n",
      "Epoch 771/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.6066 - accuracy: 0.6818\n",
      "Epoch 772/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6065 - accuracy: 0.6818\n",
      "Epoch 773/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6064 - accuracy: 0.6818\n",
      "Epoch 774/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.6064 - accuracy: 0.6818\n",
      "Epoch 775/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6063 - accuracy: 0.6818\n",
      "Epoch 776/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6063 - accuracy: 0.6818\n",
      "Epoch 777/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.6062 - accuracy: 0.6818\n",
      "Epoch 778/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6062 - accuracy: 0.6818\n",
      "Epoch 779/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6061 - accuracy: 0.6818\n",
      "Epoch 780/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6061 - accuracy: 0.6818\n",
      "Epoch 781/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6060 - accuracy: 0.6818\n",
      "Epoch 782/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6059 - accuracy: 0.6818\n",
      "Epoch 783/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6059 - accuracy: 0.6818\n",
      "Epoch 784/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6058 - accuracy: 0.6818\n",
      "Epoch 785/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6058 - accuracy: 0.6818\n",
      "Epoch 786/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6057 - accuracy: 0.6818\n",
      "Epoch 787/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6057 - accuracy: 0.6818\n",
      "Epoch 788/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6056 - accuracy: 0.6818\n",
      "Epoch 789/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6056 - accuracy: 0.6818\n",
      "Epoch 790/3500\n",
      "22/22 [==============================] - 0s 500us/step - loss: 0.6055 - accuracy: 0.6818\n",
      "Epoch 791/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6054 - accuracy: 0.6818\n",
      "Epoch 792/3500\n",
      "22/22 [==============================] - 0s 682us/step - loss: 0.6054 - accuracy: 0.6818\n",
      "Epoch 793/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6053 - accuracy: 0.6818\n",
      "Epoch 794/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6053 - accuracy: 0.6818\n",
      "Epoch 795/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6052 - accuracy: 0.6818\n",
      "Epoch 796/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6052 - accuracy: 0.6818\n",
      "Epoch 797/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6051 - accuracy: 0.6818\n",
      "Epoch 798/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6051 - accuracy: 0.6818\n",
      "Epoch 799/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6050 - accuracy: 0.6818\n",
      "Epoch 800/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6049 - accuracy: 0.6818\n",
      "Epoch 801/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6049 - accuracy: 0.6818\n",
      "Epoch 802/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6048 - accuracy: 0.6818\n",
      "Epoch 803/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6048 - accuracy: 0.6818\n",
      "Epoch 804/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6047 - accuracy: 0.6818\n",
      "Epoch 805/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6047 - accuracy: 0.6818\n",
      "Epoch 806/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6046 - accuracy: 0.6818\n",
      "Epoch 807/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6046 - accuracy: 0.6818\n",
      "Epoch 808/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6045 - accuracy: 0.6818\n",
      "Epoch 809/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6044 - accuracy: 0.6818\n",
      "Epoch 810/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6044 - accuracy: 0.6818\n",
      "Epoch 811/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6043 - accuracy: 0.6818\n",
      "Epoch 812/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6043 - accuracy: 0.6818\n",
      "Epoch 813/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6042 - accuracy: 0.6818\n",
      "Epoch 814/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6042 - accuracy: 0.6818\n",
      "Epoch 815/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6041 - accuracy: 0.6818\n",
      "Epoch 816/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.6041 - accuracy: 0.6818\n",
      "Epoch 817/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6040 - accuracy: 0.6818\n",
      "Epoch 818/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.6039 - accuracy: 0.6818\n",
      "Epoch 819/3500\n",
      "22/22 [==============================] - 0s 955us/step - loss: 0.6039 - accuracy: 0.6818\n",
      "Epoch 820/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6038 - accuracy: 0.6818\n",
      "Epoch 821/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6038 - accuracy: 0.6818\n",
      "Epoch 822/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6037 - accuracy: 0.6818\n",
      "Epoch 823/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6037 - accuracy: 0.6818\n",
      "Epoch 824/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6036 - accuracy: 0.6818\n",
      "Epoch 825/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6036 - accuracy: 0.6818\n",
      "Epoch 826/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6035 - accuracy: 0.6818\n",
      "Epoch 827/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6034 - accuracy: 0.6818\n",
      "Epoch 828/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6034 - accuracy: 0.6818\n",
      "Epoch 829/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6033 - accuracy: 0.6818\n",
      "Epoch 830/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6033 - accuracy: 0.6818\n",
      "Epoch 831/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6032 - accuracy: 0.6818\n",
      "Epoch 832/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6032 - accuracy: 0.6818\n",
      "Epoch 833/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.6031 - accuracy: 0.6818\n",
      "Epoch 834/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6031 - accuracy: 0.6818\n",
      "Epoch 835/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6030 - accuracy: 0.6818\n",
      "Epoch 836/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6029 - accuracy: 0.6818\n",
      "Epoch 837/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6029 - accuracy: 0.6818\n",
      "Epoch 838/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6028 - accuracy: 0.6818\n",
      "Epoch 839/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6028 - accuracy: 0.6818\n",
      "Epoch 840/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6027 - accuracy: 0.6818\n",
      "Epoch 841/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6027 - accuracy: 0.6818\n",
      "Epoch 842/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6026 - accuracy: 0.6818\n",
      "Epoch 843/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6026 - accuracy: 0.6818\n",
      "Epoch 844/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6025 - accuracy: 0.6818\n",
      "Epoch 845/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6024 - accuracy: 0.6818\n",
      "Epoch 846/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6024 - accuracy: 0.6818\n",
      "Epoch 847/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6023 - accuracy: 0.6818\n",
      "Epoch 848/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6023 - accuracy: 0.6818\n",
      "Epoch 849/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6022 - accuracy: 0.6818\n",
      "Epoch 850/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.6022 - accuracy: 0.6818\n",
      "Epoch 851/3500\n",
      "22/22 [==============================] - 0s 500us/step - loss: 0.6021 - accuracy: 0.6818\n",
      "Epoch 852/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6021 - accuracy: 0.6818\n",
      "Epoch 853/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6020 - accuracy: 0.6818\n",
      "Epoch 854/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6019 - accuracy: 0.6818\n",
      "Epoch 855/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6019 - accuracy: 0.6818\n",
      "Epoch 856/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6018 - accuracy: 0.6818\n",
      "Epoch 857/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6018 - accuracy: 0.6818\n",
      "Epoch 858/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6017 - accuracy: 0.6818\n",
      "Epoch 859/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6017 - accuracy: 0.6818\n",
      "Epoch 860/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6016 - accuracy: 0.6818\n",
      "Epoch 861/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6015 - accuracy: 0.6818\n",
      "Epoch 862/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6015 - accuracy: 0.6818\n",
      "Epoch 863/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6014 - accuracy: 0.6818\n",
      "Epoch 864/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6014 - accuracy: 0.6818\n",
      "Epoch 865/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6013 - accuracy: 0.6818\n",
      "Epoch 866/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6013 - accuracy: 0.6818\n",
      "Epoch 867/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6012 - accuracy: 0.6818\n",
      "Epoch 868/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6012 - accuracy: 0.6818\n",
      "Epoch 869/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6011 - accuracy: 0.6818\n",
      "Epoch 870/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6010 - accuracy: 0.6818\n",
      "Epoch 871/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.6010 - accuracy: 0.6818\n",
      "Epoch 872/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6009 - accuracy: 0.6818\n",
      "Epoch 873/3500\n",
      "22/22 [==============================] - 0s 272us/step - loss: 0.6009 - accuracy: 0.6818\n",
      "Epoch 874/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6008 - accuracy: 0.6818\n",
      "Epoch 875/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.6008 - accuracy: 0.6818\n",
      "Epoch 876/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6007 - accuracy: 0.6818\n",
      "Epoch 877/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6007 - accuracy: 0.6818\n",
      "Epoch 878/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.6006 - accuracy: 0.6818\n",
      "Epoch 879/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6005 - accuracy: 0.6818\n",
      "Epoch 880/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6005 - accuracy: 0.6818\n",
      "Epoch 881/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6004 - accuracy: 0.6818\n",
      "Epoch 882/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6004 - accuracy: 0.6818\n",
      "Epoch 883/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.6003 - accuracy: 0.6818\n",
      "Epoch 884/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6003 - accuracy: 0.6818\n",
      "Epoch 885/3500\n",
      "22/22 [==============================] - 0s 591us/step - loss: 0.6002 - accuracy: 0.6818\n",
      "Epoch 886/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6002 - accuracy: 0.6818\n",
      "Epoch 887/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.6001 - accuracy: 0.6818\n",
      "Epoch 888/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6000 - accuracy: 0.6818\n",
      "Epoch 889/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.6000 - accuracy: 0.6818\n",
      "Epoch 890/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5999 - accuracy: 0.6818\n",
      "Epoch 891/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5999 - accuracy: 0.6818\n",
      "Epoch 892/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5998 - accuracy: 0.6818\n",
      "Epoch 893/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5998 - accuracy: 0.6818\n",
      "Epoch 894/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5997 - accuracy: 0.6818\n",
      "Epoch 895/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5997 - accuracy: 0.6818\n",
      "Epoch 896/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5996 - accuracy: 0.6818\n",
      "Epoch 897/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5995 - accuracy: 0.6818\n",
      "Epoch 898/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5995 - accuracy: 0.6818\n",
      "Epoch 899/3500\n",
      "22/22 [==============================] - 0s 228us/step - loss: 0.5994 - accuracy: 0.6818\n",
      "Epoch 900/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5994 - accuracy: 0.6818\n",
      "Epoch 901/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5993 - accuracy: 0.6818\n",
      "Epoch 902/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5993 - accuracy: 0.6818\n",
      "Epoch 903/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5992 - accuracy: 0.6818\n",
      "Epoch 904/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5992 - accuracy: 0.6818\n",
      "Epoch 905/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5991 - accuracy: 0.6818\n",
      "Epoch 906/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5990 - accuracy: 0.6818\n",
      "Epoch 907/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5990 - accuracy: 0.6818\n",
      "Epoch 908/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5989 - accuracy: 0.6818\n",
      "Epoch 909/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5989 - accuracy: 0.6818\n",
      "Epoch 910/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5988 - accuracy: 0.6818\n",
      "Epoch 911/3500\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.5988 - accuracy: 0.6818\n",
      "Epoch 912/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5987 - accuracy: 0.6818\n",
      "Epoch 913/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5987 - accuracy: 0.6818\n",
      "Epoch 914/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5986 - accuracy: 0.6818\n",
      "Epoch 915/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5985 - accuracy: 0.6818\n",
      "Epoch 916/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5985 - accuracy: 0.6818\n",
      "Epoch 917/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5984 - accuracy: 0.6818\n",
      "Epoch 918/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5984 - accuracy: 0.6818\n",
      "Epoch 919/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5983 - accuracy: 0.6818\n",
      "Epoch 920/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5983 - accuracy: 0.6818\n",
      "Epoch 921/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5982 - accuracy: 0.6818\n",
      "Epoch 922/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5982 - accuracy: 0.6818\n",
      "Epoch 923/3500\n",
      "22/22 [==============================] - 0s 500us/step - loss: 0.5981 - accuracy: 0.6818\n",
      "Epoch 924/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5980 - accuracy: 0.6818\n",
      "Epoch 925/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5980 - accuracy: 0.6818\n",
      "Epoch 926/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5979 - accuracy: 0.6818\n",
      "Epoch 927/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5979 - accuracy: 0.6818\n",
      "Epoch 928/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5978 - accuracy: 0.6818\n",
      "Epoch 929/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5978 - accuracy: 0.6818\n",
      "Epoch 930/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5977 - accuracy: 0.6818\n",
      "Epoch 931/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5977 - accuracy: 0.6818\n",
      "Epoch 932/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5976 - accuracy: 0.6818\n",
      "Epoch 933/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5975 - accuracy: 0.6818\n",
      "Epoch 934/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5975 - accuracy: 0.6818\n",
      "Epoch 935/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5974 - accuracy: 0.6818\n",
      "Epoch 936/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5974 - accuracy: 0.6818\n",
      "Epoch 937/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5973 - accuracy: 0.6818\n",
      "Epoch 938/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5973 - accuracy: 0.6818\n",
      "Epoch 939/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5972 - accuracy: 0.6818\n",
      "Epoch 940/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5972 - accuracy: 0.6818\n",
      "Epoch 941/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5971 - accuracy: 0.6818\n",
      "Epoch 942/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5970 - accuracy: 0.6818\n",
      "Epoch 943/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5970 - accuracy: 0.6818\n",
      "Epoch 944/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5969 - accuracy: 0.6818\n",
      "Epoch 945/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5969 - accuracy: 0.6818\n",
      "Epoch 946/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5968 - accuracy: 0.6818\n",
      "Epoch 947/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5968 - accuracy: 0.6818\n",
      "Epoch 948/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5967 - accuracy: 0.6818\n",
      "Epoch 949/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5967 - accuracy: 0.6818\n",
      "Epoch 950/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5966 - accuracy: 0.6818\n",
      "Epoch 951/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5965 - accuracy: 0.6818\n",
      "Epoch 952/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5965 - accuracy: 0.6818\n",
      "Epoch 953/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5964 - accuracy: 0.6818\n",
      "Epoch 954/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5964 - accuracy: 0.6818\n",
      "Epoch 955/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5963 - accuracy: 0.6818\n",
      "Epoch 956/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5963 - accuracy: 0.6818\n",
      "Epoch 957/3500\n",
      "22/22 [==============================] - 0s 454us/step - loss: 0.5962 - accuracy: 0.6818\n",
      "Epoch 958/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5962 - accuracy: 0.6818\n",
      "Epoch 959/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5961 - accuracy: 0.6818\n",
      "Epoch 960/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5960 - accuracy: 0.6818\n",
      "Epoch 961/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5960 - accuracy: 0.6818\n",
      "Epoch 962/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5959 - accuracy: 0.6818\n",
      "Epoch 963/3500\n",
      "22/22 [==============================] - 0s 864us/step - loss: 0.5959 - accuracy: 0.6818\n",
      "Epoch 964/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5958 - accuracy: 0.6818\n",
      "Epoch 965/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5958 - accuracy: 0.6818\n",
      "Epoch 966/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5957 - accuracy: 0.6818\n",
      "Epoch 967/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5957 - accuracy: 0.6818\n",
      "Epoch 968/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5956 - accuracy: 0.6818\n",
      "Epoch 969/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5955 - accuracy: 0.6818\n",
      "Epoch 970/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5955 - accuracy: 0.6818\n",
      "Epoch 971/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5954 - accuracy: 0.6818\n",
      "Epoch 972/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5954 - accuracy: 0.6818\n",
      "Epoch 973/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5953 - accuracy: 0.6818\n",
      "Epoch 974/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5953 - accuracy: 0.6818\n",
      "Epoch 975/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5952 - accuracy: 0.6818\n",
      "Epoch 976/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5952 - accuracy: 0.6818\n",
      "Epoch 977/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5951 - accuracy: 0.6818\n",
      "Epoch 978/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5950 - accuracy: 0.6818\n",
      "Epoch 979/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5950 - accuracy: 0.6818\n",
      "Epoch 980/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5949 - accuracy: 0.6818\n",
      "Epoch 981/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5949 - accuracy: 0.6818\n",
      "Epoch 982/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5948 - accuracy: 0.6818\n",
      "Epoch 983/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5948 - accuracy: 0.6818\n",
      "Epoch 984/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.5947 - accuracy: 0.6818\n",
      "Epoch 985/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5947 - accuracy: 0.6818\n",
      "Epoch 986/3500\n",
      "22/22 [==============================] - 0s 363us/step - loss: 0.5946 - accuracy: 0.6818\n",
      "Epoch 987/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5945 - accuracy: 0.6818\n",
      "Epoch 988/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5945 - accuracy: 0.6818\n",
      "Epoch 989/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5944 - accuracy: 0.6818\n",
      "Epoch 990/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5944 - accuracy: 0.6818\n",
      "Epoch 991/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.5943 - accuracy: 0.6818\n",
      "Epoch 992/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5943 - accuracy: 0.6818\n",
      "Epoch 993/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5942 - accuracy: 0.6818\n",
      "Epoch 994/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5942 - accuracy: 0.6818\n",
      "Epoch 995/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5941 - accuracy: 0.6818\n",
      "Epoch 996/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5940 - accuracy: 0.6818\n",
      "Epoch 997/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5940 - accuracy: 0.6818\n",
      "Epoch 998/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5939 - accuracy: 0.6818\n",
      "Epoch 999/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5939 - accuracy: 0.6818\n",
      "Epoch 1000/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5938 - accuracy: 0.6818\n",
      "Epoch 1001/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5938 - accuracy: 0.6818\n",
      "Epoch 1002/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5937 - accuracy: 0.6818\n",
      "Epoch 1003/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5937 - accuracy: 0.6818\n",
      "Epoch 1004/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5936 - accuracy: 0.6818\n",
      "Epoch 1005/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5935 - accuracy: 0.6818\n",
      "Epoch 1006/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5935 - accuracy: 0.6818\n",
      "Epoch 1007/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5934 - accuracy: 0.6818\n",
      "Epoch 1008/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5934 - accuracy: 0.6818\n",
      "Epoch 1009/3500\n",
      "22/22 [==============================] - 0s 636us/step - loss: 0.5933 - accuracy: 0.6818\n",
      "Epoch 1010/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5933 - accuracy: 0.6818\n",
      "Epoch 1011/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5932 - accuracy: 0.6818\n",
      "Epoch 1012/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5932 - accuracy: 0.6818\n",
      "Epoch 1013/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5931 - accuracy: 0.6818\n",
      "Epoch 1014/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5930 - accuracy: 0.6818\n",
      "Epoch 1015/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5930 - accuracy: 0.6818\n",
      "Epoch 1016/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5929 - accuracy: 0.6818\n",
      "Epoch 1017/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5929 - accuracy: 0.6818\n",
      "Epoch 1018/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5928 - accuracy: 0.6818\n",
      "Epoch 1019/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5928 - accuracy: 0.6818\n",
      "Epoch 1020/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5927 - accuracy: 0.6818\n",
      "Epoch 1021/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5927 - accuracy: 0.6818\n",
      "Epoch 1022/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5926 - accuracy: 0.6818\n",
      "Epoch 1023/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5925 - accuracy: 0.6818\n",
      "Epoch 1024/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5925 - accuracy: 0.6818\n",
      "Epoch 1025/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5924 - accuracy: 0.6818\n",
      "Epoch 1026/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5924 - accuracy: 0.6818\n",
      "Epoch 1027/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5923 - accuracy: 0.6818\n",
      "Epoch 1028/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5923 - accuracy: 0.6818\n",
      "Epoch 1029/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5922 - accuracy: 0.6818\n",
      "Epoch 1030/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5922 - accuracy: 0.6818\n",
      "Epoch 1031/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5921 - accuracy: 0.6818\n",
      "Epoch 1032/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5920 - accuracy: 0.6818\n",
      "Epoch 1033/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5920 - accuracy: 0.6818\n",
      "Epoch 1034/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5919 - accuracy: 0.6818\n",
      "Epoch 1035/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5919 - accuracy: 0.6818\n",
      "Epoch 1036/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5918 - accuracy: 0.6818\n",
      "Epoch 1037/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5918 - accuracy: 0.6818\n",
      "Epoch 1038/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5917 - accuracy: 0.6818\n",
      "Epoch 1039/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5917 - accuracy: 0.6818\n",
      "Epoch 1040/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5916 - accuracy: 0.6818\n",
      "Epoch 1041/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5915 - accuracy: 0.6818\n",
      "Epoch 1042/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5915 - accuracy: 0.6818\n",
      "Epoch 1043/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5914 - accuracy: 0.6818\n",
      "Epoch 1044/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5914 - accuracy: 0.6818\n",
      "Epoch 1045/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5913 - accuracy: 0.6818\n",
      "Epoch 1046/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5913 - accuracy: 0.6818\n",
      "Epoch 1047/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5912 - accuracy: 0.6818\n",
      "Epoch 1048/3500\n",
      "22/22 [==============================] - 0s 637us/step - loss: 0.5912 - accuracy: 0.6818\n",
      "Epoch 1049/3500\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.5911 - accuracy: 0.6818\n",
      "Epoch 1050/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5910 - accuracy: 0.6818\n",
      "Epoch 1051/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5910 - accuracy: 0.6818\n",
      "Epoch 1052/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5909 - accuracy: 0.6818\n",
      "Epoch 1053/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5909 - accuracy: 0.6818\n",
      "Epoch 1054/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5908 - accuracy: 0.6818\n",
      "Epoch 1055/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5908 - accuracy: 0.6818\n",
      "Epoch 1056/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5907 - accuracy: 0.6818\n",
      "Epoch 1057/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5907 - accuracy: 0.6818\n",
      "Epoch 1058/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5906 - accuracy: 0.6818\n",
      "Epoch 1059/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5905 - accuracy: 0.6818\n",
      "Epoch 1060/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5905 - accuracy: 0.6818\n",
      "Epoch 1061/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5904 - accuracy: 0.6818\n",
      "Epoch 1062/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5904 - accuracy: 0.6818\n",
      "Epoch 1063/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5903 - accuracy: 0.6818\n",
      "Epoch 1064/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5903 - accuracy: 0.6818\n",
      "Epoch 1065/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5902 - accuracy: 0.6818\n",
      "Epoch 1066/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5902 - accuracy: 0.6818\n",
      "Epoch 1067/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5901 - accuracy: 0.6818\n",
      "Epoch 1068/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5901 - accuracy: 0.6818\n",
      "Epoch 1069/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5900 - accuracy: 0.6818\n",
      "Epoch 1070/3500\n",
      "22/22 [==============================] - 0s 591us/step - loss: 0.5899 - accuracy: 0.6818\n",
      "Epoch 1071/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5899 - accuracy: 0.6818\n",
      "Epoch 1072/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5898 - accuracy: 0.6818\n",
      "Epoch 1073/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5898 - accuracy: 0.6818\n",
      "Epoch 1074/3500\n",
      "22/22 [==============================] - 0s 682us/step - loss: 0.5897 - accuracy: 0.6818\n",
      "Epoch 1075/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5897 - accuracy: 0.6818\n",
      "Epoch 1076/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5896 - accuracy: 0.6818\n",
      "Epoch 1077/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5896 - accuracy: 0.6818\n",
      "Epoch 1078/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5895 - accuracy: 0.6818\n",
      "Epoch 1079/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5894 - accuracy: 0.6818\n",
      "Epoch 1080/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5894 - accuracy: 0.6818\n",
      "Epoch 1081/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5893 - accuracy: 0.6818\n",
      "Epoch 1082/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5893 - accuracy: 0.6818\n",
      "Epoch 1083/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5892 - accuracy: 0.6818\n",
      "Epoch 1084/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5892 - accuracy: 0.6818\n",
      "Epoch 1085/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5891 - accuracy: 0.6818\n",
      "Epoch 1086/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5891 - accuracy: 0.6818\n",
      "Epoch 1087/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5890 - accuracy: 0.6818\n",
      "Epoch 1088/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5889 - accuracy: 0.6818\n",
      "Epoch 1089/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5889 - accuracy: 0.6818\n",
      "Epoch 1090/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5888 - accuracy: 0.6818\n",
      "Epoch 1091/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5888 - accuracy: 0.6818\n",
      "Epoch 1092/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5887 - accuracy: 0.6818\n",
      "Epoch 1093/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5887 - accuracy: 0.6818\n",
      "Epoch 1094/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5886 - accuracy: 0.6818\n",
      "Epoch 1095/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5886 - accuracy: 0.6818\n",
      "Epoch 1096/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5885 - accuracy: 0.6818\n",
      "Epoch 1097/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5884 - accuracy: 0.6818\n",
      "Epoch 1098/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5884 - accuracy: 0.6818\n",
      "Epoch 1099/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5883 - accuracy: 0.6818\n",
      "Epoch 1100/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5883 - accuracy: 0.6818\n",
      "Epoch 1101/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5882 - accuracy: 0.6818\n",
      "Epoch 1102/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5882 - accuracy: 0.6818\n",
      "Epoch 1103/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5881 - accuracy: 0.6818\n",
      "Epoch 1104/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5881 - accuracy: 0.6818\n",
      "Epoch 1105/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5880 - accuracy: 0.6818\n",
      "Epoch 1106/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5880 - accuracy: 0.6818\n",
      "Epoch 1107/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5879 - accuracy: 0.6818\n",
      "Epoch 1108/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5878 - accuracy: 0.6818\n",
      "Epoch 1109/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5878 - accuracy: 0.6818\n",
      "Epoch 1110/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5877 - accuracy: 0.6818\n",
      "Epoch 1111/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5877 - accuracy: 0.6818\n",
      "Epoch 1112/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5876 - accuracy: 0.6818\n",
      "Epoch 1113/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5876 - accuracy: 0.6818\n",
      "Epoch 1114/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5875 - accuracy: 0.6818\n",
      "Epoch 1115/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5875 - accuracy: 0.6818\n",
      "Epoch 1116/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5874 - accuracy: 0.6818\n",
      "Epoch 1117/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5873 - accuracy: 0.6818\n",
      "Epoch 1118/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5873 - accuracy: 0.6818\n",
      "Epoch 1119/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5872 - accuracy: 0.6818\n",
      "Epoch 1120/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5872 - accuracy: 0.6818\n",
      "Epoch 1121/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5871 - accuracy: 0.6818\n",
      "Epoch 1122/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5871 - accuracy: 0.6818\n",
      "Epoch 1123/3500\n",
      "22/22 [==============================] - 0s 228us/step - loss: 0.5870 - accuracy: 0.6818\n",
      "Epoch 1124/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5870 - accuracy: 0.6818\n",
      "Epoch 1125/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5869 - accuracy: 0.7273\n",
      "Epoch 1126/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5869 - accuracy: 0.7273\n",
      "Epoch 1127/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5868 - accuracy: 0.7273\n",
      "Epoch 1128/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5867 - accuracy: 0.7273\n",
      "Epoch 1129/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5867 - accuracy: 0.7273\n",
      "Epoch 1130/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5866 - accuracy: 0.7273\n",
      "Epoch 1131/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5866 - accuracy: 0.7273\n",
      "Epoch 1132/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5865 - accuracy: 0.7273\n",
      "Epoch 1133/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5865 - accuracy: 0.7273\n",
      "Epoch 1134/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5864 - accuracy: 0.7273\n",
      "Epoch 1135/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5864 - accuracy: 0.7273\n",
      "Epoch 1136/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5863 - accuracy: 0.7273\n",
      "Epoch 1137/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5862 - accuracy: 0.7273\n",
      "Epoch 1138/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5862 - accuracy: 0.7273\n",
      "Epoch 1139/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5861 - accuracy: 0.7273\n",
      "Epoch 1140/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5861 - accuracy: 0.7273\n",
      "Epoch 1141/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5860 - accuracy: 0.7273\n",
      "Epoch 1142/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5860 - accuracy: 0.7273\n",
      "Epoch 1143/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5859 - accuracy: 0.7273\n",
      "Epoch 1144/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5859 - accuracy: 0.7273\n",
      "Epoch 1145/3500\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.5858 - accuracy: 0.7273\n",
      "Epoch 1146/3500\n",
      "22/22 [==============================] - 0s 636us/step - loss: 0.5858 - accuracy: 0.7273\n",
      "Epoch 1147/3500\n",
      "22/22 [==============================] - 0s 500us/step - loss: 0.5857 - accuracy: 0.7273\n",
      "Epoch 1148/3500\n",
      "22/22 [==============================] - 0s 909us/step - loss: 0.5856 - accuracy: 0.7273\n",
      "Epoch 1149/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5856 - accuracy: 0.7273\n",
      "Epoch 1150/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5855 - accuracy: 0.7273\n",
      "Epoch 1151/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5855 - accuracy: 0.7273\n",
      "Epoch 1152/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5854 - accuracy: 0.7273\n",
      "Epoch 1153/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5854 - accuracy: 0.7273\n",
      "Epoch 1154/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5853 - accuracy: 0.7273\n",
      "Epoch 1155/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5853 - accuracy: 0.7273\n",
      "Epoch 1156/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5852 - accuracy: 0.7273\n",
      "Epoch 1157/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5851 - accuracy: 0.7273\n",
      "Epoch 1158/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5851 - accuracy: 0.7273\n",
      "Epoch 1159/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5850 - accuracy: 0.7273\n",
      "Epoch 1160/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5850 - accuracy: 0.7273\n",
      "Epoch 1161/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5849 - accuracy: 0.7273\n",
      "Epoch 1162/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5849 - accuracy: 0.7273\n",
      "Epoch 1163/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5848 - accuracy: 0.7273\n",
      "Epoch 1164/3500\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.5848 - accuracy: 0.7273\n",
      "Epoch 1165/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5847 - accuracy: 0.7273\n",
      "Epoch 1166/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5847 - accuracy: 0.7273\n",
      "Epoch 1167/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5846 - accuracy: 0.7273\n",
      "Epoch 1168/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5845 - accuracy: 0.7273\n",
      "Epoch 1169/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5845 - accuracy: 0.7273\n",
      "Epoch 1170/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5844 - accuracy: 0.7273\n",
      "Epoch 1171/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5844 - accuracy: 0.7273\n",
      "Epoch 1172/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5843 - accuracy: 0.7273\n",
      "Epoch 1173/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5843 - accuracy: 0.7273\n",
      "Epoch 1174/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5842 - accuracy: 0.7273\n",
      "Epoch 1175/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5842 - accuracy: 0.7273\n",
      "Epoch 1176/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5841 - accuracy: 0.7273\n",
      "Epoch 1177/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5841 - accuracy: 0.7273\n",
      "Epoch 1178/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5840 - accuracy: 0.7273\n",
      "Epoch 1179/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5839 - accuracy: 0.7273\n",
      "Epoch 1180/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5839 - accuracy: 0.7273\n",
      "Epoch 1181/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5838 - accuracy: 0.7273\n",
      "Epoch 1182/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5838 - accuracy: 0.7273\n",
      "Epoch 1183/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5837 - accuracy: 0.7273\n",
      "Epoch 1184/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5837 - accuracy: 0.7273\n",
      "Epoch 1185/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5836 - accuracy: 0.7273\n",
      "Epoch 1186/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5836 - accuracy: 0.7273\n",
      "Epoch 1187/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5835 - accuracy: 0.7273\n",
      "Epoch 1188/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5835 - accuracy: 0.7273\n",
      "Epoch 1189/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5834 - accuracy: 0.7273\n",
      "Epoch 1190/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5833 - accuracy: 0.7273\n",
      "Epoch 1191/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5833 - accuracy: 0.7273\n",
      "Epoch 1192/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5832 - accuracy: 0.7273\n",
      "Epoch 1193/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5832 - accuracy: 0.7273\n",
      "Epoch 1194/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5831 - accuracy: 0.7273\n",
      "Epoch 1195/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5831 - accuracy: 0.7273\n",
      "Epoch 1196/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5830 - accuracy: 0.7273\n",
      "Epoch 1197/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5830 - accuracy: 0.7273\n",
      "Epoch 1198/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5829 - accuracy: 0.7273\n",
      "Epoch 1199/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5829 - accuracy: 0.7273\n",
      "Epoch 1200/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5828 - accuracy: 0.7273\n",
      "Epoch 1201/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5827 - accuracy: 0.7273\n",
      "Epoch 1202/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5827 - accuracy: 0.7273\n",
      "Epoch 1203/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5826 - accuracy: 0.7273\n",
      "Epoch 1204/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5826 - accuracy: 0.7273\n",
      "Epoch 1205/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5825 - accuracy: 0.7273\n",
      "Epoch 1206/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5825 - accuracy: 0.7273\n",
      "Epoch 1207/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5824 - accuracy: 0.7273\n",
      "Epoch 1208/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5824 - accuracy: 0.7273\n",
      "Epoch 1209/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5823 - accuracy: 0.7273\n",
      "Epoch 1210/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5823 - accuracy: 0.7273\n",
      "Epoch 1211/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5822 - accuracy: 0.7273\n",
      "Epoch 1212/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5821 - accuracy: 0.7273\n",
      "Epoch 1213/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5821 - accuracy: 0.7273\n",
      "Epoch 1214/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5820 - accuracy: 0.7273\n",
      "Epoch 1215/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5820 - accuracy: 0.7273\n",
      "Epoch 1216/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5819 - accuracy: 0.7273\n",
      "Epoch 1217/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5819 - accuracy: 0.7273\n",
      "Epoch 1218/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5818 - accuracy: 0.7273\n",
      "Epoch 1219/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5818 - accuracy: 0.7273\n",
      "Epoch 1220/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5817 - accuracy: 0.7273\n",
      "Epoch 1221/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5817 - accuracy: 0.7273\n",
      "Epoch 1222/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5816 - accuracy: 0.7273\n",
      "Epoch 1223/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5815 - accuracy: 0.7273\n",
      "Epoch 1224/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5815 - accuracy: 0.7273\n",
      "Epoch 1225/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5814 - accuracy: 0.7273\n",
      "Epoch 1226/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5814 - accuracy: 0.7273\n",
      "Epoch 1227/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5813 - accuracy: 0.7273\n",
      "Epoch 1228/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5813 - accuracy: 0.7273\n",
      "Epoch 1229/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5812 - accuracy: 0.7273\n",
      "Epoch 1230/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5812 - accuracy: 0.7273\n",
      "Epoch 1231/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5811 - accuracy: 0.7273\n",
      "Epoch 1232/3500\n",
      "22/22 [==============================] - 0s 454us/step - loss: 0.5811 - accuracy: 0.7273\n",
      "Epoch 1233/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5810 - accuracy: 0.7273\n",
      "Epoch 1234/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5809 - accuracy: 0.7273\n",
      "Epoch 1235/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5809 - accuracy: 0.7273\n",
      "Epoch 1236/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5808 - accuracy: 0.7273\n",
      "Epoch 1237/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5808 - accuracy: 0.7273\n",
      "Epoch 1238/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5807 - accuracy: 0.7273\n",
      "Epoch 1239/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5807 - accuracy: 0.7273\n",
      "Epoch 1240/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5806 - accuracy: 0.7273\n",
      "Epoch 1241/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5806 - accuracy: 0.7273\n",
      "Epoch 1242/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5805 - accuracy: 0.7273\n",
      "Epoch 1243/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5805 - accuracy: 0.7273\n",
      "Epoch 1244/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5804 - accuracy: 0.7273\n",
      "Epoch 1245/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5803 - accuracy: 0.7273\n",
      "Epoch 1246/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5803 - accuracy: 0.7273\n",
      "Epoch 1247/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5802 - accuracy: 0.7273\n",
      "Epoch 1248/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5802 - accuracy: 0.7273\n",
      "Epoch 1249/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5801 - accuracy: 0.7273\n",
      "Epoch 1250/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5801 - accuracy: 0.7273\n",
      "Epoch 1251/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5800 - accuracy: 0.7273\n",
      "Epoch 1252/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5800 - accuracy: 0.7273\n",
      "Epoch 1253/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5799 - accuracy: 0.7273\n",
      "Epoch 1254/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5799 - accuracy: 0.7273\n",
      "Epoch 1255/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5798 - accuracy: 0.7273\n",
      "Epoch 1256/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5798 - accuracy: 0.7273\n",
      "Epoch 1257/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5797 - accuracy: 0.7273\n",
      "Epoch 1258/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5796 - accuracy: 0.7273\n",
      "Epoch 1259/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5796 - accuracy: 0.7273\n",
      "Epoch 1260/3500\n",
      "22/22 [==============================] - 0s 954us/step - loss: 0.5795 - accuracy: 0.7273\n",
      "Epoch 1261/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5795 - accuracy: 0.7273\n",
      "Epoch 1262/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5794 - accuracy: 0.7273\n",
      "Epoch 1263/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5794 - accuracy: 0.7273\n",
      "Epoch 1264/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5793 - accuracy: 0.7273\n",
      "Epoch 1265/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5793 - accuracy: 0.7273\n",
      "Epoch 1266/3500\n",
      "22/22 [==============================] - 0s 864us/step - loss: 0.5792 - accuracy: 0.7273\n",
      "Epoch 1267/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5792 - accuracy: 0.7273\n",
      "Epoch 1268/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5791 - accuracy: 0.7273\n",
      "Epoch 1269/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5790 - accuracy: 0.7273\n",
      "Epoch 1270/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5790 - accuracy: 0.7273\n",
      "Epoch 1271/3500\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.5789 - accuracy: 0.7273\n",
      "Epoch 1272/3500\n",
      "22/22 [==============================] - 0s 909us/step - loss: 0.5789 - accuracy: 0.7273\n",
      "Epoch 1273/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5788 - accuracy: 0.7273\n",
      "Epoch 1274/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5788 - accuracy: 0.7273\n",
      "Epoch 1275/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5787 - accuracy: 0.7273\n",
      "Epoch 1276/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5787 - accuracy: 0.7273\n",
      "Epoch 1277/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5786 - accuracy: 0.7273\n",
      "Epoch 1278/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5786 - accuracy: 0.7273\n",
      "Epoch 1279/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5785 - accuracy: 0.7273\n",
      "Epoch 1280/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5785 - accuracy: 0.7273\n",
      "Epoch 1281/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5784 - accuracy: 0.7273\n",
      "Epoch 1282/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5783 - accuracy: 0.7273\n",
      "Epoch 1283/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5783 - accuracy: 0.7273\n",
      "Epoch 1284/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5782 - accuracy: 0.7273\n",
      "Epoch 1285/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5782 - accuracy: 0.7273\n",
      "Epoch 1286/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5781 - accuracy: 0.7273\n",
      "Epoch 1287/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5781 - accuracy: 0.7273\n",
      "Epoch 1288/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5780 - accuracy: 0.7273\n",
      "Epoch 1289/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5780 - accuracy: 0.7273\n",
      "Epoch 1290/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5779 - accuracy: 0.7273\n",
      "Epoch 1291/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5779 - accuracy: 0.7273\n",
      "Epoch 1292/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5778 - accuracy: 0.7273\n",
      "Epoch 1293/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5778 - accuracy: 0.7273\n",
      "Epoch 1294/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5777 - accuracy: 0.7273\n",
      "Epoch 1295/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5776 - accuracy: 0.7273\n",
      "Epoch 1296/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5776 - accuracy: 0.7273\n",
      "Epoch 1297/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5775 - accuracy: 0.7273\n",
      "Epoch 1298/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5775 - accuracy: 0.7273\n",
      "Epoch 1299/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5774 - accuracy: 0.7273\n",
      "Epoch 1300/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5774 - accuracy: 0.7273\n",
      "Epoch 1301/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5773 - accuracy: 0.7273\n",
      "Epoch 1302/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5773 - accuracy: 0.7273\n",
      "Epoch 1303/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5772 - accuracy: 0.7273\n",
      "Epoch 1304/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5772 - accuracy: 0.7273\n",
      "Epoch 1305/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5771 - accuracy: 0.7273\n",
      "Epoch 1306/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5771 - accuracy: 0.7273\n",
      "Epoch 1307/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5770 - accuracy: 0.7273\n",
      "Epoch 1308/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5769 - accuracy: 0.7273\n",
      "Epoch 1309/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5769 - accuracy: 0.7273\n",
      "Epoch 1310/3500\n",
      "22/22 [==============================] - 0s 454us/step - loss: 0.5768 - accuracy: 0.7273\n",
      "Epoch 1311/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5768 - accuracy: 0.7273\n",
      "Epoch 1312/3500\n",
      "22/22 [==============================] - 0s 954us/step - loss: 0.5767 - accuracy: 0.7273\n",
      "Epoch 1313/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5767 - accuracy: 0.7273\n",
      "Epoch 1314/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5766 - accuracy: 0.7273\n",
      "Epoch 1315/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5766 - accuracy: 0.7273\n",
      "Epoch 1316/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5765 - accuracy: 0.7273\n",
      "Epoch 1317/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5765 - accuracy: 0.7273\n",
      "Epoch 1318/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5764 - accuracy: 0.7273\n",
      "Epoch 1319/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5764 - accuracy: 0.7273\n",
      "Epoch 1320/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5763 - accuracy: 0.7273\n",
      "Epoch 1321/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5762 - accuracy: 0.7273\n",
      "Epoch 1322/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5762 - accuracy: 0.7273\n",
      "Epoch 1323/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5761 - accuracy: 0.7273\n",
      "Epoch 1324/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5761 - accuracy: 0.7273\n",
      "Epoch 1325/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5760 - accuracy: 0.7273\n",
      "Epoch 1326/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5760 - accuracy: 0.7273\n",
      "Epoch 1327/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5759 - accuracy: 0.7273\n",
      "Epoch 1328/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5759 - accuracy: 0.7273\n",
      "Epoch 1329/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5758 - accuracy: 0.7273\n",
      "Epoch 1330/3500\n",
      "22/22 [==============================] - 0s 228us/step - loss: 0.5758 - accuracy: 0.7273\n",
      "Epoch 1331/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5757 - accuracy: 0.7273\n",
      "Epoch 1332/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5757 - accuracy: 0.7273\n",
      "Epoch 1333/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5756 - accuracy: 0.7273\n",
      "Epoch 1334/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5756 - accuracy: 0.7273\n",
      "Epoch 1335/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5755 - accuracy: 0.7273\n",
      "Epoch 1336/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5754 - accuracy: 0.7273\n",
      "Epoch 1337/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5754 - accuracy: 0.7273\n",
      "Epoch 1338/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5753 - accuracy: 0.7273\n",
      "Epoch 1339/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5753 - accuracy: 0.7273\n",
      "Epoch 1340/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5752 - accuracy: 0.7273\n",
      "Epoch 1341/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5752 - accuracy: 0.7273\n",
      "Epoch 1342/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5751 - accuracy: 0.7273\n",
      "Epoch 1343/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5751 - accuracy: 0.7273\n",
      "Epoch 1344/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5750 - accuracy: 0.7273\n",
      "Epoch 1345/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5750 - accuracy: 0.7273\n",
      "Epoch 1346/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5749 - accuracy: 0.7273\n",
      "Epoch 1347/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5749 - accuracy: 0.7273\n",
      "Epoch 1348/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.5748 - accuracy: 0.7273\n",
      "Epoch 1349/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5748 - accuracy: 0.7273\n",
      "Epoch 1350/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5747 - accuracy: 0.7273\n",
      "Epoch 1351/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5746 - accuracy: 0.7273\n",
      "Epoch 1352/3500\n",
      "22/22 [==============================] - 0s 955us/step - loss: 0.5746 - accuracy: 0.7273\n",
      "Epoch 1353/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5745 - accuracy: 0.7273\n",
      "Epoch 1354/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5745 - accuracy: 0.7273\n",
      "Epoch 1355/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5744 - accuracy: 0.7273\n",
      "Epoch 1356/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5744 - accuracy: 0.7273\n",
      "Epoch 1357/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5743 - accuracy: 0.7273\n",
      "Epoch 1358/3500\n",
      "22/22 [==============================] - 0s 864us/step - loss: 0.5743 - accuracy: 0.7273\n",
      "Epoch 1359/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5742 - accuracy: 0.7273\n",
      "Epoch 1360/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5742 - accuracy: 0.7273\n",
      "Epoch 1361/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5741 - accuracy: 0.7273\n",
      "Epoch 1362/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5741 - accuracy: 0.7273\n",
      "Epoch 1363/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5740 - accuracy: 0.7273\n",
      "Epoch 1364/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5740 - accuracy: 0.7273\n",
      "Epoch 1365/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5739 - accuracy: 0.7273\n",
      "Epoch 1366/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5738 - accuracy: 0.7273\n",
      "Epoch 1367/3500\n",
      "22/22 [==============================] - 0s 272us/step - loss: 0.5738 - accuracy: 0.7273\n",
      "Epoch 1368/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5737 - accuracy: 0.7273\n",
      "Epoch 1369/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5737 - accuracy: 0.7273\n",
      "Epoch 1370/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5736 - accuracy: 0.7273\n",
      "Epoch 1371/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5736 - accuracy: 0.7273\n",
      "Epoch 1372/3500\n",
      "22/22 [==============================] - 0s 500us/step - loss: 0.5735 - accuracy: 0.7273\n",
      "Epoch 1373/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5735 - accuracy: 0.7273\n",
      "Epoch 1374/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5734 - accuracy: 0.7273\n",
      "Epoch 1375/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5734 - accuracy: 0.7273\n",
      "Epoch 1376/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5733 - accuracy: 0.7273\n",
      "Epoch 1377/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5733 - accuracy: 0.7273\n",
      "Epoch 1378/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5732 - accuracy: 0.7273\n",
      "Epoch 1379/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5732 - accuracy: 0.7273\n",
      "Epoch 1380/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5731 - accuracy: 0.7273\n",
      "Epoch 1381/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5730 - accuracy: 0.7273\n",
      "Epoch 1382/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5730 - accuracy: 0.7273\n",
      "Epoch 1383/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5729 - accuracy: 0.7273\n",
      "Epoch 1384/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5729 - accuracy: 0.7273\n",
      "Epoch 1385/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5728 - accuracy: 0.7273\n",
      "Epoch 1386/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5728 - accuracy: 0.7273\n",
      "Epoch 1387/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5727 - accuracy: 0.7273\n",
      "Epoch 1388/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5727 - accuracy: 0.7273\n",
      "Epoch 1389/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5726 - accuracy: 0.7273\n",
      "Epoch 1390/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5726 - accuracy: 0.7273\n",
      "Epoch 1391/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5725 - accuracy: 0.7273\n",
      "Epoch 1392/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5725 - accuracy: 0.7273\n",
      "Epoch 1393/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5724 - accuracy: 0.7273\n",
      "Epoch 1394/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5724 - accuracy: 0.7273\n",
      "Epoch 1395/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5723 - accuracy: 0.7273\n",
      "Epoch 1396/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5723 - accuracy: 0.7273\n",
      "Epoch 1397/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5722 - accuracy: 0.7273\n",
      "Epoch 1398/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5721 - accuracy: 0.7273\n",
      "Epoch 1399/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5721 - accuracy: 0.7273\n",
      "Epoch 1400/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5720 - accuracy: 0.7273\n",
      "Epoch 1401/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5720 - accuracy: 0.7273\n",
      "Epoch 1402/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5719 - accuracy: 0.7273\n",
      "Epoch 1403/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5719 - accuracy: 0.7273\n",
      "Epoch 1404/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5718 - accuracy: 0.7273\n",
      "Epoch 1405/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.5718 - accuracy: 0.7273\n",
      "Epoch 1406/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5717 - accuracy: 0.7273\n",
      "Epoch 1407/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5717 - accuracy: 0.7273\n",
      "Epoch 1408/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5716 - accuracy: 0.7273\n",
      "Epoch 1409/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5716 - accuracy: 0.7273\n",
      "Epoch 1410/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5715 - accuracy: 0.7273\n",
      "Epoch 1411/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5715 - accuracy: 0.7273\n",
      "Epoch 1412/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5714 - accuracy: 0.7273\n",
      "Epoch 1413/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5714 - accuracy: 0.7273\n",
      "Epoch 1414/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5713 - accuracy: 0.7273\n",
      "Epoch 1415/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5712 - accuracy: 0.7273\n",
      "Epoch 1416/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5712 - accuracy: 0.7273\n",
      "Epoch 1417/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5711 - accuracy: 0.7273\n",
      "Epoch 1418/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5711 - accuracy: 0.7273\n",
      "Epoch 1419/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5710 - accuracy: 0.7273\n",
      "Epoch 1420/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5710 - accuracy: 0.7273\n",
      "Epoch 1421/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5709 - accuracy: 0.7273\n",
      "Epoch 1422/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5709 - accuracy: 0.7273\n",
      "Epoch 1423/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5708 - accuracy: 0.7273\n",
      "Epoch 1424/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5708 - accuracy: 0.7273\n",
      "Epoch 1425/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5707 - accuracy: 0.7273\n",
      "Epoch 1426/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5707 - accuracy: 0.7273\n",
      "Epoch 1427/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5706 - accuracy: 0.7273\n",
      "Epoch 1428/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5706 - accuracy: 0.7273\n",
      "Epoch 1429/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5705 - accuracy: 0.7273\n",
      "Epoch 1430/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5705 - accuracy: 0.7273\n",
      "Epoch 1431/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5704 - accuracy: 0.7273\n",
      "Epoch 1432/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5704 - accuracy: 0.7273\n",
      "Epoch 1433/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5703 - accuracy: 0.7273\n",
      "Epoch 1434/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5702 - accuracy: 0.7273\n",
      "Epoch 1435/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5702 - accuracy: 0.7273\n",
      "Epoch 1436/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5701 - accuracy: 0.7273\n",
      "Epoch 1437/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5701 - accuracy: 0.7273\n",
      "Epoch 1438/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5700 - accuracy: 0.7273\n",
      "Epoch 1439/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5700 - accuracy: 0.7273\n",
      "Epoch 1440/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5699 - accuracy: 0.7273\n",
      "Epoch 1441/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5699 - accuracy: 0.7273\n",
      "Epoch 1442/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5698 - accuracy: 0.7273\n",
      "Epoch 1443/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5698 - accuracy: 0.7273\n",
      "Epoch 1444/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5697 - accuracy: 0.7273\n",
      "Epoch 1445/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5697 - accuracy: 0.7273\n",
      "Epoch 1446/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5696 - accuracy: 0.7273\n",
      "Epoch 1447/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5696 - accuracy: 0.7273\n",
      "Epoch 1448/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5695 - accuracy: 0.7273\n",
      "Epoch 1449/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5695 - accuracy: 0.7273\n",
      "Epoch 1450/3500\n",
      "22/22 [==============================] - 0s 636us/step - loss: 0.5694 - accuracy: 0.7273\n",
      "Epoch 1451/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5694 - accuracy: 0.7273\n",
      "Epoch 1452/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5693 - accuracy: 0.7273\n",
      "Epoch 1453/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5692 - accuracy: 0.7273\n",
      "Epoch 1454/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5692 - accuracy: 0.7273\n",
      "Epoch 1455/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5691 - accuracy: 0.7273\n",
      "Epoch 1456/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5691 - accuracy: 0.7273\n",
      "Epoch 1457/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5690 - accuracy: 0.7273\n",
      "Epoch 1458/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5690 - accuracy: 0.7273\n",
      "Epoch 1459/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5689 - accuracy: 0.7273\n",
      "Epoch 1460/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5689 - accuracy: 0.7273\n",
      "Epoch 1461/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5688 - accuracy: 0.7273\n",
      "Epoch 1462/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5688 - accuracy: 0.7273\n",
      "Epoch 1463/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5687 - accuracy: 0.7273\n",
      "Epoch 1464/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5687 - accuracy: 0.7273\n",
      "Epoch 1465/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5686 - accuracy: 0.7273\n",
      "Epoch 1466/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5686 - accuracy: 0.7273\n",
      "Epoch 1467/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5685 - accuracy: 0.7273\n",
      "Epoch 1468/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5685 - accuracy: 0.7273\n",
      "Epoch 1469/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5684 - accuracy: 0.7273\n",
      "Epoch 1470/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5684 - accuracy: 0.7273\n",
      "Epoch 1471/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5683 - accuracy: 0.7273\n",
      "Epoch 1472/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5683 - accuracy: 0.7273\n",
      "Epoch 1473/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5682 - accuracy: 0.7273\n",
      "Epoch 1474/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5681 - accuracy: 0.7273\n",
      "Epoch 1475/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5681 - accuracy: 0.7273\n",
      "Epoch 1476/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5680 - accuracy: 0.7273\n",
      "Epoch 1477/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5680 - accuracy: 0.7273\n",
      "Epoch 1478/3500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.5679 - accuracy: 0.7273\n",
      "Epoch 1479/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5679 - accuracy: 0.7273\n",
      "Epoch 1480/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5678 - accuracy: 0.7273\n",
      "Epoch 1481/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5678 - accuracy: 0.7273\n",
      "Epoch 1482/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5677 - accuracy: 0.7273\n",
      "Epoch 1483/3500\n",
      "22/22 [==============================] - 0s 636us/step - loss: 0.5677 - accuracy: 0.7273\n",
      "Epoch 1484/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5676 - accuracy: 0.7273\n",
      "Epoch 1485/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5676 - accuracy: 0.7273\n",
      "Epoch 1486/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5675 - accuracy: 0.7273\n",
      "Epoch 1487/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5675 - accuracy: 0.7273\n",
      "Epoch 1488/3500\n",
      "22/22 [==============================] - 0s 272us/step - loss: 0.5674 - accuracy: 0.7273\n",
      "Epoch 1489/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5674 - accuracy: 0.7273\n",
      "Epoch 1490/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5673 - accuracy: 0.7273\n",
      "Epoch 1491/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5673 - accuracy: 0.7273\n",
      "Epoch 1492/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5672 - accuracy: 0.7273\n",
      "Epoch 1493/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5672 - accuracy: 0.7273\n",
      "Epoch 1494/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5671 - accuracy: 0.7273\n",
      "Epoch 1495/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5671 - accuracy: 0.7273\n",
      "Epoch 1496/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5670 - accuracy: 0.7273\n",
      "Epoch 1497/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5670 - accuracy: 0.7273\n",
      "Epoch 1498/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5669 - accuracy: 0.7273\n",
      "Epoch 1499/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5668 - accuracy: 0.7273\n",
      "Epoch 1500/3500\n",
      "22/22 [==============================] - 0s 636us/step - loss: 0.5668 - accuracy: 0.7273\n",
      "Epoch 1501/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5667 - accuracy: 0.7273\n",
      "Epoch 1502/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5667 - accuracy: 0.7273\n",
      "Epoch 1503/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5666 - accuracy: 0.7273\n",
      "Epoch 1504/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5666 - accuracy: 0.7273\n",
      "Epoch 1505/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5665 - accuracy: 0.7273\n",
      "Epoch 1506/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5665 - accuracy: 0.7273\n",
      "Epoch 1507/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5664 - accuracy: 0.7273\n",
      "Epoch 1508/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5664 - accuracy: 0.7273\n",
      "Epoch 1509/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5663 - accuracy: 0.7273\n",
      "Epoch 1510/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5663 - accuracy: 0.7273\n",
      "Epoch 1511/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5662 - accuracy: 0.7273\n",
      "Epoch 1512/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5662 - accuracy: 0.7273\n",
      "Epoch 1513/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5661 - accuracy: 0.7273\n",
      "Epoch 1514/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5661 - accuracy: 0.7273\n",
      "Epoch 1515/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5660 - accuracy: 0.7273\n",
      "Epoch 1516/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5660 - accuracy: 0.7273\n",
      "Epoch 1517/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5659 - accuracy: 0.7273\n",
      "Epoch 1518/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5659 - accuracy: 0.7273\n",
      "Epoch 1519/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5658 - accuracy: 0.7273\n",
      "Epoch 1520/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5658 - accuracy: 0.7273\n",
      "Epoch 1521/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5657 - accuracy: 0.7273\n",
      "Epoch 1522/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5657 - accuracy: 0.7273\n",
      "Epoch 1523/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5656 - accuracy: 0.7273\n",
      "Epoch 1524/3500\n",
      "22/22 [==============================] - 0s 545us/step - loss: 0.5656 - accuracy: 0.7273\n",
      "Epoch 1525/3500\n",
      "22/22 [==============================] - 0s 682us/step - loss: 0.5655 - accuracy: 0.7273\n",
      "Epoch 1526/3500\n",
      "22/22 [==============================] - 0s 500us/step - loss: 0.5654 - accuracy: 0.7273\n",
      "Epoch 1527/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5654 - accuracy: 0.7273\n",
      "Epoch 1528/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5653 - accuracy: 0.7273\n",
      "Epoch 1529/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5653 - accuracy: 0.7273\n",
      "Epoch 1530/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5652 - accuracy: 0.7273\n",
      "Epoch 1531/3500\n",
      "22/22 [==============================] - 0s 545us/step - loss: 0.5652 - accuracy: 0.7273\n",
      "Epoch 1532/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5651 - accuracy: 0.7273\n",
      "Epoch 1533/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5651 - accuracy: 0.7273\n",
      "Epoch 1534/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5650 - accuracy: 0.7273\n",
      "Epoch 1535/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5650 - accuracy: 0.7273\n",
      "Epoch 1536/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5649 - accuracy: 0.7273\n",
      "Epoch 1537/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5649 - accuracy: 0.7273\n",
      "Epoch 1538/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5648 - accuracy: 0.7273\n",
      "Epoch 1539/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5648 - accuracy: 0.7273\n",
      "Epoch 1540/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5647 - accuracy: 0.7273\n",
      "Epoch 1541/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5647 - accuracy: 0.7273\n",
      "Epoch 1542/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5646 - accuracy: 0.7273\n",
      "Epoch 1543/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5646 - accuracy: 0.7273\n",
      "Epoch 1544/3500\n",
      "22/22 [==============================] - 0s 864us/step - loss: 0.5645 - accuracy: 0.7273\n",
      "Epoch 1545/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5645 - accuracy: 0.7273\n",
      "Epoch 1546/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5644 - accuracy: 0.7273\n",
      "Epoch 1547/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5644 - accuracy: 0.7273\n",
      "Epoch 1548/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5643 - accuracy: 0.7273\n",
      "Epoch 1549/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5643 - accuracy: 0.7273\n",
      "Epoch 1550/3500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.5642 - accuracy: 0.7273\n",
      "Epoch 1551/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5642 - accuracy: 0.7273\n",
      "Epoch 1552/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5641 - accuracy: 0.7273\n",
      "Epoch 1553/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5641 - accuracy: 0.7273\n",
      "Epoch 1554/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5640 - accuracy: 0.7273\n",
      "Epoch 1555/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5640 - accuracy: 0.7273\n",
      "Epoch 1556/3500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.5639 - accuracy: 0.7273\n",
      "Epoch 1557/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5639 - accuracy: 0.7273\n",
      "Epoch 1558/3500\n",
      "22/22 [==============================] - 0s 363us/step - loss: 0.5638 - accuracy: 0.7273\n",
      "Epoch 1559/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5637 - accuracy: 0.7273\n",
      "Epoch 1560/3500\n",
      "22/22 [==============================] - 0s 500us/step - loss: 0.5637 - accuracy: 0.7273\n",
      "Epoch 1561/3500\n",
      "22/22 [==============================] - 0s 545us/step - loss: 0.5636 - accuracy: 0.7273\n",
      "Epoch 1562/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5636 - accuracy: 0.7273\n",
      "Epoch 1563/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5635 - accuracy: 0.7273\n",
      "Epoch 1564/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5635 - accuracy: 0.7273\n",
      "Epoch 1565/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5634 - accuracy: 0.7273\n",
      "Epoch 1566/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5634 - accuracy: 0.7273\n",
      "Epoch 1567/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5633 - accuracy: 0.7273\n",
      "Epoch 1568/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5633 - accuracy: 0.7273\n",
      "Epoch 1569/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5632 - accuracy: 0.7273\n",
      "Epoch 1570/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5632 - accuracy: 0.7273\n",
      "Epoch 1571/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5631 - accuracy: 0.7273\n",
      "Epoch 1572/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5631 - accuracy: 0.7273\n",
      "Epoch 1573/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5630 - accuracy: 0.7273\n",
      "Epoch 1574/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5630 - accuracy: 0.7273\n",
      "Epoch 1575/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5629 - accuracy: 0.7273\n",
      "Epoch 1576/3500\n",
      "22/22 [==============================] - 0s 500us/step - loss: 0.5629 - accuracy: 0.7273\n",
      "Epoch 1577/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5628 - accuracy: 0.7273\n",
      "Epoch 1578/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5628 - accuracy: 0.7273\n",
      "Epoch 1579/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5627 - accuracy: 0.7273\n",
      "Epoch 1580/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5627 - accuracy: 0.7273\n",
      "Epoch 1581/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5626 - accuracy: 0.7273\n",
      "Epoch 1582/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5626 - accuracy: 0.7273\n",
      "Epoch 1583/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5625 - accuracy: 0.7273\n",
      "Epoch 1584/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5625 - accuracy: 0.7273\n",
      "Epoch 1585/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5624 - accuracy: 0.7273\n",
      "Epoch 1586/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5624 - accuracy: 0.7273\n",
      "Epoch 1587/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5623 - accuracy: 0.7273\n",
      "Epoch 1588/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5623 - accuracy: 0.7273\n",
      "Epoch 1589/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5622 - accuracy: 0.7273\n",
      "Epoch 1590/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5622 - accuracy: 0.7273\n",
      "Epoch 1591/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5621 - accuracy: 0.7273\n",
      "Epoch 1592/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5621 - accuracy: 0.7273\n",
      "Epoch 1593/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5620 - accuracy: 0.7273\n",
      "Epoch 1594/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5620 - accuracy: 0.7273\n",
      "Epoch 1595/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5619 - accuracy: 0.7273\n",
      "Epoch 1596/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5619 - accuracy: 0.7273\n",
      "Epoch 1597/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5618 - accuracy: 0.7273\n",
      "Epoch 1598/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5618 - accuracy: 0.7273\n",
      "Epoch 1599/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5617 - accuracy: 0.7273\n",
      "Epoch 1600/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5617 - accuracy: 0.7273\n",
      "Epoch 1601/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5616 - accuracy: 0.7273\n",
      "Epoch 1602/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5616 - accuracy: 0.7273\n",
      "Epoch 1603/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5615 - accuracy: 0.7273\n",
      "Epoch 1604/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5615 - accuracy: 0.7273\n",
      "Epoch 1605/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5614 - accuracy: 0.7273\n",
      "Epoch 1606/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5614 - accuracy: 0.7273\n",
      "Epoch 1607/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5613 - accuracy: 0.7273\n",
      "Epoch 1608/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5612 - accuracy: 0.7273\n",
      "Epoch 1609/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5612 - accuracy: 0.7273\n",
      "Epoch 1610/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5611 - accuracy: 0.7273\n",
      "Epoch 1611/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5611 - accuracy: 0.7273\n",
      "Epoch 1612/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5610 - accuracy: 0.7273\n",
      "Epoch 1613/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5610 - accuracy: 0.7273\n",
      "Epoch 1614/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5609 - accuracy: 0.7273\n",
      "Epoch 1615/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5609 - accuracy: 0.7273\n",
      "Epoch 1616/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5608 - accuracy: 0.7273\n",
      "Epoch 1617/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5608 - accuracy: 0.7273\n",
      "Epoch 1618/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5607 - accuracy: 0.7273\n",
      "Epoch 1619/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5607 - accuracy: 0.7273\n",
      "Epoch 1620/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5606 - accuracy: 0.7273\n",
      "Epoch 1621/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5606 - accuracy: 0.7273\n",
      "Epoch 1622/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5605 - accuracy: 0.7273\n",
      "Epoch 1623/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5605 - accuracy: 0.7273\n",
      "Epoch 1624/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5604 - accuracy: 0.7273\n",
      "Epoch 1625/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5604 - accuracy: 0.7273\n",
      "Epoch 1626/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5603 - accuracy: 0.7273\n",
      "Epoch 1627/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5603 - accuracy: 0.7273\n",
      "Epoch 1628/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5602 - accuracy: 0.7273\n",
      "Epoch 1629/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5602 - accuracy: 0.7273\n",
      "Epoch 1630/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5601 - accuracy: 0.7273\n",
      "Epoch 1631/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5601 - accuracy: 0.7273\n",
      "Epoch 1632/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5600 - accuracy: 0.7273\n",
      "Epoch 1633/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5600 - accuracy: 0.7273\n",
      "Epoch 1634/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5599 - accuracy: 0.7273\n",
      "Epoch 1635/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5599 - accuracy: 0.7273\n",
      "Epoch 1636/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5598 - accuracy: 0.7273\n",
      "Epoch 1637/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5598 - accuracy: 0.7273\n",
      "Epoch 1638/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5597 - accuracy: 0.7273\n",
      "Epoch 1639/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5597 - accuracy: 0.7273\n",
      "Epoch 1640/3500\n",
      "22/22 [==============================] - 0s 363us/step - loss: 0.5596 - accuracy: 0.7273\n",
      "Epoch 1641/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5596 - accuracy: 0.7273\n",
      "Epoch 1642/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5595 - accuracy: 0.7273\n",
      "Epoch 1643/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5595 - accuracy: 0.7273\n",
      "Epoch 1644/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5594 - accuracy: 0.7273\n",
      "Epoch 1645/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5594 - accuracy: 0.7273\n",
      "Epoch 1646/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5593 - accuracy: 0.7273\n",
      "Epoch 1647/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5593 - accuracy: 0.7273\n",
      "Epoch 1648/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5592 - accuracy: 0.7273\n",
      "Epoch 1649/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5592 - accuracy: 0.7273\n",
      "Epoch 1650/3500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.5591 - accuracy: 0.7273\n",
      "Epoch 1651/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5591 - accuracy: 0.7273\n",
      "Epoch 1652/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5590 - accuracy: 0.7273\n",
      "Epoch 1653/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5590 - accuracy: 0.7273\n",
      "Epoch 1654/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5589 - accuracy: 0.7273\n",
      "Epoch 1655/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5589 - accuracy: 0.7273\n",
      "Epoch 1656/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5588 - accuracy: 0.7273\n",
      "Epoch 1657/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5588 - accuracy: 0.7273\n",
      "Epoch 1658/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5587 - accuracy: 0.7273\n",
      "Epoch 1659/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5587 - accuracy: 0.7273\n",
      "Epoch 1660/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5586 - accuracy: 0.7273\n",
      "Epoch 1661/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.7273\n",
      "Epoch 1662/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5585 - accuracy: 0.7273\n",
      "Epoch 1663/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5585 - accuracy: 0.7273\n",
      "Epoch 1664/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5584 - accuracy: 0.7273\n",
      "Epoch 1665/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5584 - accuracy: 0.7273\n",
      "Epoch 1666/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5583 - accuracy: 0.7273\n",
      "Epoch 1667/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5583 - accuracy: 0.7273\n",
      "Epoch 1668/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5582 - accuracy: 0.7273\n",
      "Epoch 1669/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5582 - accuracy: 0.7273\n",
      "Epoch 1670/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5581 - accuracy: 0.7273\n",
      "Epoch 1671/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5581 - accuracy: 0.7273\n",
      "Epoch 1672/3500\n",
      "22/22 [==============================] - 0s 228us/step - loss: 0.5580 - accuracy: 0.7273\n",
      "Epoch 1673/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5580 - accuracy: 0.7273\n",
      "Epoch 1674/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5579 - accuracy: 0.7273\n",
      "Epoch 1675/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5579 - accuracy: 0.7273\n",
      "Epoch 1676/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5578 - accuracy: 0.7273\n",
      "Epoch 1677/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5578 - accuracy: 0.7273\n",
      "Epoch 1678/3500\n",
      "22/22 [==============================] - 0s 863us/step - loss: 0.5577 - accuracy: 0.7273\n",
      "Epoch 1679/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5577 - accuracy: 0.7273\n",
      "Epoch 1680/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5576 - accuracy: 0.7273\n",
      "Epoch 1681/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5576 - accuracy: 0.7273\n",
      "Epoch 1682/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5575 - accuracy: 0.7273\n",
      "Epoch 1683/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5575 - accuracy: 0.7273\n",
      "Epoch 1684/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5574 - accuracy: 0.7273\n",
      "Epoch 1685/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5574 - accuracy: 0.7273\n",
      "Epoch 1686/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5573 - accuracy: 0.7273\n",
      "Epoch 1687/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5573 - accuracy: 0.7273\n",
      "Epoch 1688/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5572 - accuracy: 0.7273\n",
      "Epoch 1689/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5572 - accuracy: 0.7273\n",
      "Epoch 1690/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5571 - accuracy: 0.7273\n",
      "Epoch 1691/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5571 - accuracy: 0.7273\n",
      "Epoch 1692/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5570 - accuracy: 0.7273\n",
      "Epoch 1693/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5570 - accuracy: 0.7273\n",
      "Epoch 1694/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5569 - accuracy: 0.7273\n",
      "Epoch 1695/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5569 - accuracy: 0.7273\n",
      "Epoch 1696/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5568 - accuracy: 0.7273\n",
      "Epoch 1697/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5568 - accuracy: 0.7273\n",
      "Epoch 1698/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5567 - accuracy: 0.7273\n",
      "Epoch 1699/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5567 - accuracy: 0.7273\n",
      "Epoch 1700/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5566 - accuracy: 0.7273\n",
      "Epoch 1701/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5566 - accuracy: 0.7273\n",
      "Epoch 1702/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5565 - accuracy: 0.7273\n",
      "Epoch 1703/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5565 - accuracy: 0.7273\n",
      "Epoch 1704/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5564 - accuracy: 0.7273\n",
      "Epoch 1705/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5564 - accuracy: 0.7273\n",
      "Epoch 1706/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5563 - accuracy: 0.7273\n",
      "Epoch 1707/3500\n",
      "22/22 [==============================] - 0s 181us/step - loss: 0.5563 - accuracy: 0.7273\n",
      "Epoch 1708/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5562 - accuracy: 0.7273\n",
      "Epoch 1709/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5562 - accuracy: 0.7273\n",
      "Epoch 1710/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5561 - accuracy: 0.7273\n",
      "Epoch 1711/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5561 - accuracy: 0.7273\n",
      "Epoch 1712/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5560 - accuracy: 0.7273\n",
      "Epoch 1713/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5560 - accuracy: 0.7273\n",
      "Epoch 1714/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5559 - accuracy: 0.7273\n",
      "Epoch 1715/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5559 - accuracy: 0.7273\n",
      "Epoch 1716/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5558 - accuracy: 0.7273\n",
      "Epoch 1717/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5558 - accuracy: 0.7273\n",
      "Epoch 1718/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5557 - accuracy: 0.7273\n",
      "Epoch 1719/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5557 - accuracy: 0.7273\n",
      "Epoch 1720/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5556 - accuracy: 0.7273\n",
      "Epoch 1721/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5556 - accuracy: 0.7273\n",
      "Epoch 1722/3500\n",
      "22/22 [==============================] - 0s 181us/step - loss: 0.5555 - accuracy: 0.7273\n",
      "Epoch 1723/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5555 - accuracy: 0.7273\n",
      "Epoch 1724/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5554 - accuracy: 0.7273\n",
      "Epoch 1725/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5554 - accuracy: 0.7273\n",
      "Epoch 1726/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5553 - accuracy: 0.7273\n",
      "Epoch 1727/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5553 - accuracy: 0.7273\n",
      "Epoch 1728/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5552 - accuracy: 0.7273\n",
      "Epoch 1729/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5552 - accuracy: 0.7273\n",
      "Epoch 1730/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5551 - accuracy: 0.7273\n",
      "Epoch 1731/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5551 - accuracy: 0.7273\n",
      "Epoch 1732/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5550 - accuracy: 0.7273\n",
      "Epoch 1733/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5550 - accuracy: 0.7273\n",
      "Epoch 1734/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5549 - accuracy: 0.7273\n",
      "Epoch 1735/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5549 - accuracy: 0.7273\n",
      "Epoch 1736/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5548 - accuracy: 0.7273\n",
      "Epoch 1737/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5548 - accuracy: 0.7273\n",
      "Epoch 1738/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5547 - accuracy: 0.7273\n",
      "Epoch 1739/3500\n",
      "22/22 [==============================] - 0s 682us/step - loss: 0.5547 - accuracy: 0.7273\n",
      "Epoch 1740/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5546 - accuracy: 0.7273\n",
      "Epoch 1741/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5546 - accuracy: 0.7273\n",
      "Epoch 1742/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5545 - accuracy: 0.7273\n",
      "Epoch 1743/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5545 - accuracy: 0.7273\n",
      "Epoch 1744/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5544 - accuracy: 0.7273\n",
      "Epoch 1745/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5544 - accuracy: 0.7273\n",
      "Epoch 1746/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5544 - accuracy: 0.7273\n",
      "Epoch 1747/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5543 - accuracy: 0.7273\n",
      "Epoch 1748/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5543 - accuracy: 0.7273\n",
      "Epoch 1749/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5542 - accuracy: 0.7273\n",
      "Epoch 1750/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5542 - accuracy: 0.7273\n",
      "Epoch 1751/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5541 - accuracy: 0.7273\n",
      "Epoch 1752/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5541 - accuracy: 0.7273\n",
      "Epoch 1753/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5540 - accuracy: 0.7273\n",
      "Epoch 1754/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5540 - accuracy: 0.7273\n",
      "Epoch 1755/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5539 - accuracy: 0.7273\n",
      "Epoch 1756/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5539 - accuracy: 0.7273\n",
      "Epoch 1757/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5538 - accuracy: 0.7273\n",
      "Epoch 1758/3500\n",
      "22/22 [==============================] - 0s 682us/step - loss: 0.5538 - accuracy: 0.7273\n",
      "Epoch 1759/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5537 - accuracy: 0.7273\n",
      "Epoch 1760/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5537 - accuracy: 0.7273\n",
      "Epoch 1761/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5536 - accuracy: 0.7273\n",
      "Epoch 1762/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5536 - accuracy: 0.7273\n",
      "Epoch 1763/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5535 - accuracy: 0.7273\n",
      "Epoch 1764/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5535 - accuracy: 0.7273\n",
      "Epoch 1765/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5534 - accuracy: 0.7273\n",
      "Epoch 1766/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5534 - accuracy: 0.7273\n",
      "Epoch 1767/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5533 - accuracy: 0.7273\n",
      "Epoch 1768/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5533 - accuracy: 0.7273\n",
      "Epoch 1769/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5532 - accuracy: 0.7273\n",
      "Epoch 1770/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5532 - accuracy: 0.7273\n",
      "Epoch 1771/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5531 - accuracy: 0.7273\n",
      "Epoch 1772/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5531 - accuracy: 0.7273\n",
      "Epoch 1773/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5530 - accuracy: 0.7273\n",
      "Epoch 1774/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5530 - accuracy: 0.7273\n",
      "Epoch 1775/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5529 - accuracy: 0.7273\n",
      "Epoch 1776/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5529 - accuracy: 0.7273\n",
      "Epoch 1777/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5528 - accuracy: 0.7273\n",
      "Epoch 1778/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5528 - accuracy: 0.7273\n",
      "Epoch 1779/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5527 - accuracy: 0.7273\n",
      "Epoch 1780/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5527 - accuracy: 0.7273\n",
      "Epoch 1781/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5526 - accuracy: 0.7273\n",
      "Epoch 1782/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5526 - accuracy: 0.7273\n",
      "Epoch 1783/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5525 - accuracy: 0.7273\n",
      "Epoch 1784/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5525 - accuracy: 0.7273\n",
      "Epoch 1785/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5524 - accuracy: 0.7273\n",
      "Epoch 1786/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5524 - accuracy: 0.7273\n",
      "Epoch 1787/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5523 - accuracy: 0.7273\n",
      "Epoch 1788/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5523 - accuracy: 0.7273\n",
      "Epoch 1789/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5522 - accuracy: 0.7273\n",
      "Epoch 1790/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5522 - accuracy: 0.7273\n",
      "Epoch 1791/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5521 - accuracy: 0.7273\n",
      "Epoch 1792/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5521 - accuracy: 0.7273\n",
      "Epoch 1793/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5521 - accuracy: 0.7273\n",
      "Epoch 1794/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5520 - accuracy: 0.7273\n",
      "Epoch 1795/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5520 - accuracy: 0.7273\n",
      "Epoch 1796/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5519 - accuracy: 0.7273\n",
      "Epoch 1797/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5519 - accuracy: 0.7273\n",
      "Epoch 1798/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5518 - accuracy: 0.7273\n",
      "Epoch 1799/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5518 - accuracy: 0.7273\n",
      "Epoch 1800/3500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.5517 - accuracy: 0.7273\n",
      "Epoch 1801/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5517 - accuracy: 0.7273\n",
      "Epoch 1802/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5516 - accuracy: 0.7273\n",
      "Epoch 1803/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5516 - accuracy: 0.7273\n",
      "Epoch 1804/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5515 - accuracy: 0.7273\n",
      "Epoch 1805/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5515 - accuracy: 0.7273\n",
      "Epoch 1806/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5514 - accuracy: 0.7273\n",
      "Epoch 1807/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5514 - accuracy: 0.7273\n",
      "Epoch 1808/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5513 - accuracy: 0.7273\n",
      "Epoch 1809/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5513 - accuracy: 0.7273\n",
      "Epoch 1810/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5512 - accuracy: 0.7273\n",
      "Epoch 1811/3500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5512 - accuracy: 0.7273\n",
      "Epoch 1812/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5511 - accuracy: 0.7273\n",
      "Epoch 1813/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5511 - accuracy: 0.7273\n",
      "Epoch 1814/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5510 - accuracy: 0.7273\n",
      "Epoch 1815/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5510 - accuracy: 0.7273\n",
      "Epoch 1816/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5509 - accuracy: 0.7273\n",
      "Epoch 1817/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5509 - accuracy: 0.7273\n",
      "Epoch 1818/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5508 - accuracy: 0.7273\n",
      "Epoch 1819/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5508 - accuracy: 0.7273\n",
      "Epoch 1820/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5507 - accuracy: 0.7273\n",
      "Epoch 1821/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5507 - accuracy: 0.7273\n",
      "Epoch 1822/3500\n",
      "22/22 [==============================] - 0s 546us/step - loss: 0.5506 - accuracy: 0.7273\n",
      "Epoch 1823/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5506 - accuracy: 0.7273\n",
      "Epoch 1824/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5505 - accuracy: 0.7273\n",
      "Epoch 1825/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5505 - accuracy: 0.7273\n",
      "Epoch 1826/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5505 - accuracy: 0.7273\n",
      "Epoch 1827/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5504 - accuracy: 0.7273\n",
      "Epoch 1828/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5504 - accuracy: 0.7273\n",
      "Epoch 1829/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5503 - accuracy: 0.7273\n",
      "Epoch 1830/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5503 - accuracy: 0.7273\n",
      "Epoch 1831/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5502 - accuracy: 0.7273\n",
      "Epoch 1832/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5502 - accuracy: 0.7273\n",
      "Epoch 1833/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5501 - accuracy: 0.7273\n",
      "Epoch 1834/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5501 - accuracy: 0.7273\n",
      "Epoch 1835/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5500 - accuracy: 0.7273\n",
      "Epoch 1836/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5500 - accuracy: 0.7273\n",
      "Epoch 1837/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5499 - accuracy: 0.7273\n",
      "Epoch 1838/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5499 - accuracy: 0.7273\n",
      "Epoch 1839/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5498 - accuracy: 0.7273\n",
      "Epoch 1840/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5498 - accuracy: 0.7273\n",
      "Epoch 1841/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5497 - accuracy: 0.7273\n",
      "Epoch 1842/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5497 - accuracy: 0.7273\n",
      "Epoch 1843/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5496 - accuracy: 0.7273\n",
      "Epoch 1844/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5496 - accuracy: 0.7273\n",
      "Epoch 1845/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5495 - accuracy: 0.7273\n",
      "Epoch 1846/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5495 - accuracy: 0.7273\n",
      "Epoch 1847/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5494 - accuracy: 0.7273\n",
      "Epoch 1848/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5494 - accuracy: 0.7273\n",
      "Epoch 1849/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5493 - accuracy: 0.7273\n",
      "Epoch 1850/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5493 - accuracy: 0.7273\n",
      "Epoch 1851/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5493 - accuracy: 0.7273\n",
      "Epoch 1852/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5492 - accuracy: 0.7273\n",
      "Epoch 1853/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5492 - accuracy: 0.7273\n",
      "Epoch 1854/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5491 - accuracy: 0.7273\n",
      "Epoch 1855/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5491 - accuracy: 0.7273\n",
      "Epoch 1856/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5490 - accuracy: 0.7273\n",
      "Epoch 1857/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5490 - accuracy: 0.7273\n",
      "Epoch 1858/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5489 - accuracy: 0.7273\n",
      "Epoch 1859/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5489 - accuracy: 0.7273\n",
      "Epoch 1860/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5488 - accuracy: 0.7273\n",
      "Epoch 1861/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5488 - accuracy: 0.7273\n",
      "Epoch 1862/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5487 - accuracy: 0.7273\n",
      "Epoch 1863/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5487 - accuracy: 0.7273\n",
      "Epoch 1864/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5486 - accuracy: 0.7273\n",
      "Epoch 1865/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5486 - accuracy: 0.7273\n",
      "Epoch 1866/3500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.5485 - accuracy: 0.7273\n",
      "Epoch 1867/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5485 - accuracy: 0.7273\n",
      "Epoch 1868/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5484 - accuracy: 0.7273\n",
      "Epoch 1869/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5484 - accuracy: 0.7273\n",
      "Epoch 1870/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5483 - accuracy: 0.7273\n",
      "Epoch 1871/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5483 - accuracy: 0.7273\n",
      "Epoch 1872/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5482 - accuracy: 0.7273\n",
      "Epoch 1873/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5482 - accuracy: 0.7273\n",
      "Epoch 1874/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5482 - accuracy: 0.7273\n",
      "Epoch 1875/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5481 - accuracy: 0.7273\n",
      "Epoch 1876/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5481 - accuracy: 0.7273\n",
      "Epoch 1877/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5480 - accuracy: 0.7273\n",
      "Epoch 1878/3500\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.5480 - accuracy: 0.7273\n",
      "Epoch 1879/3500\n",
      "22/22 [==============================] - 0s 682us/step - loss: 0.5479 - accuracy: 0.7273\n",
      "Epoch 1880/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5479 - accuracy: 0.7273\n",
      "Epoch 1881/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5478 - accuracy: 0.7273\n",
      "Epoch 1882/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5478 - accuracy: 0.7273\n",
      "Epoch 1883/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5477 - accuracy: 0.7273\n",
      "Epoch 1884/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5477 - accuracy: 0.7273\n",
      "Epoch 1885/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5476 - accuracy: 0.7273\n",
      "Epoch 1886/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5476 - accuracy: 0.7273\n",
      "Epoch 1887/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5475 - accuracy: 0.7273\n",
      "Epoch 1888/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5475 - accuracy: 0.7273\n",
      "Epoch 1889/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5474 - accuracy: 0.7273\n",
      "Epoch 1890/3500\n",
      "22/22 [==============================] - 0s 727us/step - loss: 0.5474 - accuracy: 0.7273\n",
      "Epoch 1891/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5473 - accuracy: 0.7273\n",
      "Epoch 1892/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5473 - accuracy: 0.7273\n",
      "Epoch 1893/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5472 - accuracy: 0.7273\n",
      "Epoch 1894/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5472 - accuracy: 0.7273\n",
      "Epoch 1895/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5472 - accuracy: 0.7273\n",
      "Epoch 1896/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5471 - accuracy: 0.7273\n",
      "Epoch 1897/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5471 - accuracy: 0.7273\n",
      "Epoch 1898/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5470 - accuracy: 0.7273\n",
      "Epoch 1899/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5470 - accuracy: 0.7273\n",
      "Epoch 1900/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5469 - accuracy: 0.7273\n",
      "Epoch 1901/3500\n",
      "22/22 [==============================] - 0s 272us/step - loss: 0.5469 - accuracy: 0.7273\n",
      "Epoch 1902/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5468 - accuracy: 0.7273\n",
      "Epoch 1903/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5468 - accuracy: 0.7273\n",
      "Epoch 1904/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5467 - accuracy: 0.7273\n",
      "Epoch 1905/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5467 - accuracy: 0.7273\n",
      "Epoch 1906/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5466 - accuracy: 0.7273\n",
      "Epoch 1907/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5466 - accuracy: 0.7273\n",
      "Epoch 1908/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5465 - accuracy: 0.7273\n",
      "Epoch 1909/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5465 - accuracy: 0.7273\n",
      "Epoch 1910/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5464 - accuracy: 0.7273\n",
      "Epoch 1911/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5464 - accuracy: 0.7273\n",
      "Epoch 1912/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5463 - accuracy: 0.7273\n",
      "Epoch 1913/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5463 - accuracy: 0.7273\n",
      "Epoch 1914/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5463 - accuracy: 0.7273\n",
      "Epoch 1915/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5462 - accuracy: 0.7273\n",
      "Epoch 1916/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5462 - accuracy: 0.7273\n",
      "Epoch 1917/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5461 - accuracy: 0.7273\n",
      "Epoch 1918/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5461 - accuracy: 0.7273\n",
      "Epoch 1919/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5460 - accuracy: 0.7273\n",
      "Epoch 1920/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5460 - accuracy: 0.7273\n",
      "Epoch 1921/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5459 - accuracy: 0.7273\n",
      "Epoch 1922/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5459 - accuracy: 0.7273\n",
      "Epoch 1923/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5458 - accuracy: 0.7273\n",
      "Epoch 1924/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5458 - accuracy: 0.7273\n",
      "Epoch 1925/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5457 - accuracy: 0.7273\n",
      "Epoch 1926/3500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.5457 - accuracy: 0.7273\n",
      "Epoch 1927/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5456 - accuracy: 0.7273\n",
      "Epoch 1928/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5456 - accuracy: 0.7273\n",
      "Epoch 1929/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5455 - accuracy: 0.7273\n",
      "Epoch 1930/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5455 - accuracy: 0.7273\n",
      "Epoch 1931/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5455 - accuracy: 0.7273\n",
      "Epoch 1932/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5454 - accuracy: 0.7273\n",
      "Epoch 1933/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5454 - accuracy: 0.7273\n",
      "Epoch 1934/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5453 - accuracy: 0.7273\n",
      "Epoch 1935/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5453 - accuracy: 0.7273\n",
      "Epoch 1936/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5452 - accuracy: 0.7273\n",
      "Epoch 1937/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5452 - accuracy: 0.7273\n",
      "Epoch 1938/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5451 - accuracy: 0.7273\n",
      "Epoch 1939/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5451 - accuracy: 0.7273\n",
      "Epoch 1940/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5450 - accuracy: 0.7273\n",
      "Epoch 1941/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5450 - accuracy: 0.7273\n",
      "Epoch 1942/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5449 - accuracy: 0.7273\n",
      "Epoch 1943/3500\n",
      "22/22 [==============================] - 0s 773us/step - loss: 0.5449 - accuracy: 0.7273\n",
      "Epoch 1944/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5448 - accuracy: 0.7273\n",
      "Epoch 1945/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5448 - accuracy: 0.7273\n",
      "Epoch 1946/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5448 - accuracy: 0.7273\n",
      "Epoch 1947/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5447 - accuracy: 0.7273\n",
      "Epoch 1948/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5447 - accuracy: 0.7273\n",
      "Epoch 1949/3500\n",
      "22/22 [==============================] - 0s 954us/step - loss: 0.5446 - accuracy: 0.7273\n",
      "Epoch 1950/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5446 - accuracy: 0.7273\n",
      "Epoch 1951/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5445 - accuracy: 0.7273\n",
      "Epoch 1952/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5445 - accuracy: 0.7273\n",
      "Epoch 1953/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5444 - accuracy: 0.7273\n",
      "Epoch 1954/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5444 - accuracy: 0.7273\n",
      "Epoch 1955/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5443 - accuracy: 0.7273\n",
      "Epoch 1956/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5443 - accuracy: 0.7273\n",
      "Epoch 1957/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5442 - accuracy: 0.7273\n",
      "Epoch 1958/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5442 - accuracy: 0.7273\n",
      "Epoch 1959/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5441 - accuracy: 0.7273\n",
      "Epoch 1960/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5441 - accuracy: 0.7273\n",
      "Epoch 1961/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5440 - accuracy: 0.7273\n",
      "Epoch 1962/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5440 - accuracy: 0.7273\n",
      "Epoch 1963/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5440 - accuracy: 0.7273\n",
      "Epoch 1964/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5439 - accuracy: 0.7273\n",
      "Epoch 1965/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5439 - accuracy: 0.7273\n",
      "Epoch 1966/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5438 - accuracy: 0.7273\n",
      "Epoch 1967/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5438 - accuracy: 0.7273\n",
      "Epoch 1968/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5437 - accuracy: 0.7273\n",
      "Epoch 1969/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5437 - accuracy: 0.7273\n",
      "Epoch 1970/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5436 - accuracy: 0.7273\n",
      "Epoch 1971/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5436 - accuracy: 0.7273\n",
      "Epoch 1972/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5435 - accuracy: 0.7273\n",
      "Epoch 1973/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5435 - accuracy: 0.7273\n",
      "Epoch 1974/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5434 - accuracy: 0.7273\n",
      "Epoch 1975/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5434 - accuracy: 0.7273\n",
      "Epoch 1976/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5434 - accuracy: 0.7273\n",
      "Epoch 1977/3500\n",
      "22/22 [==============================] - 0s 1000us/step - loss: 0.5433 - accuracy: 0.7273\n",
      "Epoch 1978/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5433 - accuracy: 0.7273\n",
      "Epoch 1979/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5432 - accuracy: 0.7273\n",
      "Epoch 1980/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5432 - accuracy: 0.7273\n",
      "Epoch 1981/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5431 - accuracy: 0.7273\n",
      "Epoch 1982/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5431 - accuracy: 0.7273\n",
      "Epoch 1983/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5430 - accuracy: 0.7273\n",
      "Epoch 1984/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5430 - accuracy: 0.7273\n",
      "Epoch 1985/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5429 - accuracy: 0.7273\n",
      "Epoch 1986/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5429 - accuracy: 0.7273\n",
      "Epoch 1987/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5428 - accuracy: 0.7273\n",
      "Epoch 1988/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5428 - accuracy: 0.7273\n",
      "Epoch 1989/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5427 - accuracy: 0.7273\n",
      "Epoch 1990/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5427 - accuracy: 0.7273\n",
      "Epoch 1991/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5427 - accuracy: 0.7273\n",
      "Epoch 1992/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5426 - accuracy: 0.7273\n",
      "Epoch 1993/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5426 - accuracy: 0.7273\n",
      "Epoch 1994/3500\n",
      "22/22 [==============================] - 0s 773us/step - loss: 0.5425 - accuracy: 0.7273\n",
      "Epoch 1995/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5425 - accuracy: 0.7273\n",
      "Epoch 1996/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5424 - accuracy: 0.7273\n",
      "Epoch 1997/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5424 - accuracy: 0.7273\n",
      "Epoch 1998/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5423 - accuracy: 0.7273\n",
      "Epoch 1999/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5423 - accuracy: 0.7273\n",
      "Epoch 2000/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5422 - accuracy: 0.7273\n",
      "Epoch 2001/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5422 - accuracy: 0.7273\n",
      "Epoch 2002/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5421 - accuracy: 0.7273\n",
      "Epoch 2003/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5421 - accuracy: 0.7273\n",
      "Epoch 2004/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5421 - accuracy: 0.7273\n",
      "Epoch 2005/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5420 - accuracy: 0.7727\n",
      "Epoch 2006/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5420 - accuracy: 0.7727\n",
      "Epoch 2007/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5419 - accuracy: 0.7727\n",
      "Epoch 2008/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5419 - accuracy: 0.7727\n",
      "Epoch 2009/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5418 - accuracy: 0.7727\n",
      "Epoch 2010/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5418 - accuracy: 0.7727\n",
      "Epoch 2011/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5417 - accuracy: 0.7727\n",
      "Epoch 2012/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5417 - accuracy: 0.7727\n",
      "Epoch 2013/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5416 - accuracy: 0.7727\n",
      "Epoch 2014/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5416 - accuracy: 0.7727\n",
      "Epoch 2015/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5416 - accuracy: 0.7727\n",
      "Epoch 2016/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5415 - accuracy: 0.7727\n",
      "Epoch 2017/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5415 - accuracy: 0.7727\n",
      "Epoch 2018/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5414 - accuracy: 0.7727\n",
      "Epoch 2019/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5414 - accuracy: 0.7727\n",
      "Epoch 2020/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5413 - accuracy: 0.7727\n",
      "Epoch 2021/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5413 - accuracy: 0.7727\n",
      "Epoch 2022/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5412 - accuracy: 0.7727\n",
      "Epoch 2023/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5412 - accuracy: 0.7727\n",
      "Epoch 2024/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5411 - accuracy: 0.7727\n",
      "Epoch 2025/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5411 - accuracy: 0.7727\n",
      "Epoch 2026/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5410 - accuracy: 0.7727\n",
      "Epoch 2027/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5410 - accuracy: 0.7727\n",
      "Epoch 2028/3500\n",
      "22/22 [==============================] - 0s 682us/step - loss: 0.5410 - accuracy: 0.7727\n",
      "Epoch 2029/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5409 - accuracy: 0.7727\n",
      "Epoch 2030/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5409 - accuracy: 0.7727\n",
      "Epoch 2031/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5408 - accuracy: 0.7727\n",
      "Epoch 2032/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5408 - accuracy: 0.7727\n",
      "Epoch 2033/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5407 - accuracy: 0.7727\n",
      "Epoch 2034/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5407 - accuracy: 0.7727\n",
      "Epoch 2035/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5406 - accuracy: 0.7727\n",
      "Epoch 2036/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5406 - accuracy: 0.7727\n",
      "Epoch 2037/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5405 - accuracy: 0.7727\n",
      "Epoch 2038/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5405 - accuracy: 0.7727\n",
      "Epoch 2039/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5405 - accuracy: 0.7727\n",
      "Epoch 2040/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5404 - accuracy: 0.7727\n",
      "Epoch 2041/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5404 - accuracy: 0.7727\n",
      "Epoch 2042/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5403 - accuracy: 0.7727\n",
      "Epoch 2043/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5403 - accuracy: 0.7727\n",
      "Epoch 2044/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5402 - accuracy: 0.7727\n",
      "Epoch 2045/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5402 - accuracy: 0.7727\n",
      "Epoch 2046/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5401 - accuracy: 0.7727\n",
      "Epoch 2047/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5401 - accuracy: 0.7727\n",
      "Epoch 2048/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5400 - accuracy: 0.7727\n",
      "Epoch 2049/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5400 - accuracy: 0.7727\n",
      "Epoch 2050/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5399 - accuracy: 0.7727\n",
      "Epoch 2051/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5399 - accuracy: 0.7727\n",
      "Epoch 2052/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5399 - accuracy: 0.7727\n",
      "Epoch 2053/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5398 - accuracy: 0.7727\n",
      "Epoch 2054/3500\n",
      "22/22 [==============================] - 0s 546us/step - loss: 0.5398 - accuracy: 0.7727\n",
      "Epoch 2055/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5397 - accuracy: 0.7727\n",
      "Epoch 2056/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5397 - accuracy: 0.7727\n",
      "Epoch 2057/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5396 - accuracy: 0.7727\n",
      "Epoch 2058/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5396 - accuracy: 0.7727\n",
      "Epoch 2059/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5395 - accuracy: 0.7727\n",
      "Epoch 2060/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5395 - accuracy: 0.7727\n",
      "Epoch 2061/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5394 - accuracy: 0.7727\n",
      "Epoch 2062/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5394 - accuracy: 0.7727\n",
      "Epoch 2063/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5394 - accuracy: 0.7727\n",
      "Epoch 2064/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5393 - accuracy: 0.7727\n",
      "Epoch 2065/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5393 - accuracy: 0.7727\n",
      "Epoch 2066/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5392 - accuracy: 0.7727\n",
      "Epoch 2067/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5392 - accuracy: 0.7727\n",
      "Epoch 2068/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5391 - accuracy: 0.7727\n",
      "Epoch 2069/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5391 - accuracy: 0.7727\n",
      "Epoch 2070/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5390 - accuracy: 0.7727\n",
      "Epoch 2071/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5390 - accuracy: 0.7727\n",
      "Epoch 2072/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5390 - accuracy: 0.7727\n",
      "Epoch 2073/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5389 - accuracy: 0.7727\n",
      "Epoch 2074/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5389 - accuracy: 0.7727\n",
      "Epoch 2075/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5388 - accuracy: 0.7727\n",
      "Epoch 2076/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5388 - accuracy: 0.7727\n",
      "Epoch 2077/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5387 - accuracy: 0.7727\n",
      "Epoch 2078/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5387 - accuracy: 0.7727\n",
      "Epoch 2079/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5386 - accuracy: 0.7727\n",
      "Epoch 2080/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5386 - accuracy: 0.7727\n",
      "Epoch 2081/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5385 - accuracy: 0.7727\n",
      "Epoch 2082/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5385 - accuracy: 0.7727\n",
      "Epoch 2083/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5385 - accuracy: 0.7727\n",
      "Epoch 2084/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5384 - accuracy: 0.7727\n",
      "Epoch 2085/3500\n",
      "22/22 [==============================] - 0s 500us/step - loss: 0.5384 - accuracy: 0.7727\n",
      "Epoch 2086/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5383 - accuracy: 0.7727\n",
      "Epoch 2087/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5383 - accuracy: 0.7727\n",
      "Epoch 2088/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5382 - accuracy: 0.7727\n",
      "Epoch 2089/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5382 - accuracy: 0.7727\n",
      "Epoch 2090/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5381 - accuracy: 0.7727\n",
      "Epoch 2091/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5381 - accuracy: 0.7727\n",
      "Epoch 2092/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5380 - accuracy: 0.7727\n",
      "Epoch 2093/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5380 - accuracy: 0.7727\n",
      "Epoch 2094/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5380 - accuracy: 0.7727\n",
      "Epoch 2095/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5379 - accuracy: 0.7727\n",
      "Epoch 2096/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5379 - accuracy: 0.7727\n",
      "Epoch 2097/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5378 - accuracy: 0.7727\n",
      "Epoch 2098/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5378 - accuracy: 0.7727\n",
      "Epoch 2099/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5377 - accuracy: 0.7727\n",
      "Epoch 2100/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5377 - accuracy: 0.7727\n",
      "Epoch 2101/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5376 - accuracy: 0.7727\n",
      "Epoch 2102/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5376 - accuracy: 0.7727\n",
      "Epoch 2103/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5376 - accuracy: 0.7727\n",
      "Epoch 2104/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5375 - accuracy: 0.7727\n",
      "Epoch 2105/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5375 - accuracy: 0.7727\n",
      "Epoch 2106/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5374 - accuracy: 0.7727\n",
      "Epoch 2107/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5374 - accuracy: 0.7727\n",
      "Epoch 2108/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5373 - accuracy: 0.7727\n",
      "Epoch 2109/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5373 - accuracy: 0.7727\n",
      "Epoch 2110/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5372 - accuracy: 0.7727\n",
      "Epoch 2111/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5372 - accuracy: 0.7727\n",
      "Epoch 2112/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5372 - accuracy: 0.7727\n",
      "Epoch 2113/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5371 - accuracy: 0.7727\n",
      "Epoch 2114/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5371 - accuracy: 0.7727\n",
      "Epoch 2115/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5370 - accuracy: 0.7727\n",
      "Epoch 2116/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5370 - accuracy: 0.7727\n",
      "Epoch 2117/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5369 - accuracy: 0.7727\n",
      "Epoch 2118/3500\n",
      "22/22 [==============================] - 0s 864us/step - loss: 0.5369 - accuracy: 0.7727\n",
      "Epoch 2119/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5368 - accuracy: 0.7727\n",
      "Epoch 2120/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5368 - accuracy: 0.7727\n",
      "Epoch 2121/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5367 - accuracy: 0.7727\n",
      "Epoch 2122/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5367 - accuracy: 0.7727\n",
      "Epoch 2123/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5367 - accuracy: 0.7727\n",
      "Epoch 2124/3500\n",
      "22/22 [==============================] - 0s 591us/step - loss: 0.5366 - accuracy: 0.7727\n",
      "Epoch 2125/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5366 - accuracy: 0.7727\n",
      "Epoch 2126/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5365 - accuracy: 0.7727\n",
      "Epoch 2127/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5365 - accuracy: 0.7727\n",
      "Epoch 2128/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5364 - accuracy: 0.7727\n",
      "Epoch 2129/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5364 - accuracy: 0.7727\n",
      "Epoch 2130/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5363 - accuracy: 0.7727\n",
      "Epoch 2131/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5363 - accuracy: 0.7727\n",
      "Epoch 2132/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5363 - accuracy: 0.7727\n",
      "Epoch 2133/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5362 - accuracy: 0.7727\n",
      "Epoch 2134/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5362 - accuracy: 0.7727\n",
      "Epoch 2135/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5361 - accuracy: 0.7727\n",
      "Epoch 2136/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5361 - accuracy: 0.7727\n",
      "Epoch 2137/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5360 - accuracy: 0.7727\n",
      "Epoch 2138/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5360 - accuracy: 0.7727\n",
      "Epoch 2139/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5359 - accuracy: 0.7727\n",
      "Epoch 2140/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5359 - accuracy: 0.7727\n",
      "Epoch 2141/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5359 - accuracy: 0.7727\n",
      "Epoch 2142/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5358 - accuracy: 0.7727\n",
      "Epoch 2143/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5358 - accuracy: 0.7727\n",
      "Epoch 2144/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5357 - accuracy: 0.7727\n",
      "Epoch 2145/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5357 - accuracy: 0.7727\n",
      "Epoch 2146/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5356 - accuracy: 0.7727\n",
      "Epoch 2147/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5356 - accuracy: 0.7727\n",
      "Epoch 2148/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5355 - accuracy: 0.7727\n",
      "Epoch 2149/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5355 - accuracy: 0.7727\n",
      "Epoch 2150/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5355 - accuracy: 0.7727\n",
      "Epoch 2151/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5354 - accuracy: 0.7727\n",
      "Epoch 2152/3500\n",
      "22/22 [==============================] - 0s 818us/step - loss: 0.5354 - accuracy: 0.7727\n",
      "Epoch 2153/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5353 - accuracy: 0.7727\n",
      "Epoch 2154/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5353 - accuracy: 0.7727\n",
      "Epoch 2155/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5352 - accuracy: 0.7727\n",
      "Epoch 2156/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5352 - accuracy: 0.7727\n",
      "Epoch 2157/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5351 - accuracy: 0.7727\n",
      "Epoch 2158/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5351 - accuracy: 0.7727\n",
      "Epoch 2159/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5351 - accuracy: 0.7727\n",
      "Epoch 2160/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5350 - accuracy: 0.7727\n",
      "Epoch 2161/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5350 - accuracy: 0.7727\n",
      "Epoch 2162/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5349 - accuracy: 0.7727\n",
      "Epoch 2163/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5349 - accuracy: 0.7727\n",
      "Epoch 2164/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5348 - accuracy: 0.7727\n",
      "Epoch 2165/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5348 - accuracy: 0.7727\n",
      "Epoch 2166/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5348 - accuracy: 0.7727\n",
      "Epoch 2167/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5347 - accuracy: 0.7727\n",
      "Epoch 2168/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5347 - accuracy: 0.7727\n",
      "Epoch 2169/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5346 - accuracy: 0.7727\n",
      "Epoch 2170/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5346 - accuracy: 0.7727\n",
      "Epoch 2171/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5345 - accuracy: 0.7727\n",
      "Epoch 2172/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5345 - accuracy: 0.7727\n",
      "Epoch 2173/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5344 - accuracy: 0.7727\n",
      "Epoch 2174/3500\n",
      "22/22 [==============================] - 0s 228us/step - loss: 0.5344 - accuracy: 0.7727\n",
      "Epoch 2175/3500\n",
      "22/22 [==============================] - 0s 545us/step - loss: 0.5344 - accuracy: 0.7727\n",
      "Epoch 2176/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5343 - accuracy: 0.7727\n",
      "Epoch 2177/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5343 - accuracy: 0.7727\n",
      "Epoch 2178/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5342 - accuracy: 0.7727\n",
      "Epoch 2179/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5342 - accuracy: 0.7727\n",
      "Epoch 2180/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5341 - accuracy: 0.7727\n",
      "Epoch 2181/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5341 - accuracy: 0.7727\n",
      "Epoch 2182/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5340 - accuracy: 0.7727\n",
      "Epoch 2183/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5340 - accuracy: 0.7727\n",
      "Epoch 2184/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5340 - accuracy: 0.7727\n",
      "Epoch 2185/3500\n",
      "22/22 [==============================] - 0s 363us/step - loss: 0.5339 - accuracy: 0.7727\n",
      "Epoch 2186/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5339 - accuracy: 0.7727\n",
      "Epoch 2187/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5338 - accuracy: 0.7727\n",
      "Epoch 2188/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5338 - accuracy: 0.7727\n",
      "Epoch 2189/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5337 - accuracy: 0.7727\n",
      "Epoch 2190/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5337 - accuracy: 0.7727\n",
      "Epoch 2191/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5337 - accuracy: 0.7727\n",
      "Epoch 2192/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5336 - accuracy: 0.7727\n",
      "Epoch 2193/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5336 - accuracy: 0.7727\n",
      "Epoch 2194/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5335 - accuracy: 0.7727\n",
      "Epoch 2195/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5335 - accuracy: 0.7727\n",
      "Epoch 2196/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5334 - accuracy: 0.7727\n",
      "Epoch 2197/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5334 - accuracy: 0.7727\n",
      "Epoch 2198/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5333 - accuracy: 0.7727\n",
      "Epoch 2199/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5333 - accuracy: 0.7727\n",
      "Epoch 2200/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5333 - accuracy: 0.7727\n",
      "Epoch 2201/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5332 - accuracy: 0.7727\n",
      "Epoch 2202/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5332 - accuracy: 0.7727\n",
      "Epoch 2203/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5331 - accuracy: 0.7727\n",
      "Epoch 2204/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5331 - accuracy: 0.7727\n",
      "Epoch 2205/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5330 - accuracy: 0.7727\n",
      "Epoch 2206/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5330 - accuracy: 0.7727\n",
      "Epoch 2207/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5330 - accuracy: 0.7727\n",
      "Epoch 2208/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5329 - accuracy: 0.7727\n",
      "Epoch 2209/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5329 - accuracy: 0.7727\n",
      "Epoch 2210/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5328 - accuracy: 0.7727\n",
      "Epoch 2211/3500\n",
      "22/22 [==============================] - 0s 500us/step - loss: 0.5328 - accuracy: 0.7727\n",
      "Epoch 2212/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5327 - accuracy: 0.7727\n",
      "Epoch 2213/3500\n",
      "22/22 [==============================] - 0s 228us/step - loss: 0.5327 - accuracy: 0.7727\n",
      "Epoch 2214/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5326 - accuracy: 0.7727\n",
      "Epoch 2215/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5326 - accuracy: 0.7727\n",
      "Epoch 2216/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5326 - accuracy: 0.7727\n",
      "Epoch 2217/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5325 - accuracy: 0.7727\n",
      "Epoch 2218/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5325 - accuracy: 0.7727\n",
      "Epoch 2219/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5324 - accuracy: 0.7727\n",
      "Epoch 2220/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5324 - accuracy: 0.7727\n",
      "Epoch 2221/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5323 - accuracy: 0.7727\n",
      "Epoch 2222/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5323 - accuracy: 0.7727\n",
      "Epoch 2223/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5323 - accuracy: 0.7727\n",
      "Epoch 2224/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5322 - accuracy: 0.7727\n",
      "Epoch 2225/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5322 - accuracy: 0.7727\n",
      "Epoch 2226/3500\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.5321 - accuracy: 0.7727\n",
      "Epoch 2227/3500\n",
      "22/22 [==============================] - 0s 591us/step - loss: 0.5321 - accuracy: 0.7727\n",
      "Epoch 2228/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5320 - accuracy: 0.7727\n",
      "Epoch 2229/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5320 - accuracy: 0.7727\n",
      "Epoch 2230/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5320 - accuracy: 0.7727\n",
      "Epoch 2231/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5319 - accuracy: 0.7727\n",
      "Epoch 2232/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5319 - accuracy: 0.7727\n",
      "Epoch 2233/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5318 - accuracy: 0.7727\n",
      "Epoch 2234/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5318 - accuracy: 0.7727\n",
      "Epoch 2235/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5317 - accuracy: 0.7727\n",
      "Epoch 2236/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5317 - accuracy: 0.7727\n",
      "Epoch 2237/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5316 - accuracy: 0.7727\n",
      "Epoch 2238/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5316 - accuracy: 0.7727\n",
      "Epoch 2239/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5316 - accuracy: 0.7727\n",
      "Epoch 2240/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5315 - accuracy: 0.7727\n",
      "Epoch 2241/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5315 - accuracy: 0.7727\n",
      "Epoch 2242/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5314 - accuracy: 0.7727\n",
      "Epoch 2243/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5314 - accuracy: 0.7727\n",
      "Epoch 2244/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5313 - accuracy: 0.7727\n",
      "Epoch 2245/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5313 - accuracy: 0.7727\n",
      "Epoch 2246/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5313 - accuracy: 0.7727\n",
      "Epoch 2247/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5312 - accuracy: 0.7727\n",
      "Epoch 2248/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5312 - accuracy: 0.7727\n",
      "Epoch 2249/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5311 - accuracy: 0.7727\n",
      "Epoch 2250/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5311 - accuracy: 0.7727\n",
      "Epoch 2251/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5310 - accuracy: 0.7727\n",
      "Epoch 2252/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5310 - accuracy: 0.7727\n",
      "Epoch 2253/3500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.5310 - accuracy: 0.7727\n",
      "Epoch 2254/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5309 - accuracy: 0.7727\n",
      "Epoch 2255/3500\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.5309 - accuracy: 0.7727\n",
      "Epoch 2256/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5308 - accuracy: 0.7727\n",
      "Epoch 2257/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5308 - accuracy: 0.7727\n",
      "Epoch 2258/3500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5307 - accuracy: 0.7727\n",
      "Epoch 2259/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5307 - accuracy: 0.7727\n",
      "Epoch 2260/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5307 - accuracy: 0.7727\n",
      "Epoch 2261/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5306 - accuracy: 0.7727\n",
      "Epoch 2262/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5306 - accuracy: 0.7727\n",
      "Epoch 2263/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5305 - accuracy: 0.7727\n",
      "Epoch 2264/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5305 - accuracy: 0.7727\n",
      "Epoch 2265/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5304 - accuracy: 0.7727\n",
      "Epoch 2266/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5304 - accuracy: 0.7727\n",
      "Epoch 2267/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5304 - accuracy: 0.7727\n",
      "Epoch 2268/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5303 - accuracy: 0.7727\n",
      "Epoch 2269/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5303 - accuracy: 0.7727\n",
      "Epoch 2270/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5302 - accuracy: 0.7727\n",
      "Epoch 2271/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5302 - accuracy: 0.7727\n",
      "Epoch 2272/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5301 - accuracy: 0.7727\n",
      "Epoch 2273/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5301 - accuracy: 0.7727\n",
      "Epoch 2274/3500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.5301 - accuracy: 0.7727\n",
      "Epoch 2275/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5300 - accuracy: 0.7727\n",
      "Epoch 2276/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5300 - accuracy: 0.7727\n",
      "Epoch 2277/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5299 - accuracy: 0.7727\n",
      "Epoch 2278/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5299 - accuracy: 0.7727\n",
      "Epoch 2279/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5298 - accuracy: 0.7727\n",
      "Epoch 2280/3500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.5298 - accuracy: 0.7727\n",
      "Epoch 2281/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5298 - accuracy: 0.7727\n",
      "Epoch 2282/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5297 - accuracy: 0.7727\n",
      "Epoch 2283/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5297 - accuracy: 0.7727\n",
      "Epoch 2284/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5296 - accuracy: 0.7727\n",
      "Epoch 2285/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5296 - accuracy: 0.7727\n",
      "Epoch 2286/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5295 - accuracy: 0.7727\n",
      "Epoch 2287/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5295 - accuracy: 0.7727\n",
      "Epoch 2288/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5295 - accuracy: 0.7727\n",
      "Epoch 2289/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5294 - accuracy: 0.7727\n",
      "Epoch 2290/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5294 - accuracy: 0.7727\n",
      "Epoch 2291/3500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.5293 - accuracy: 0.7727\n",
      "Epoch 2292/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5293 - accuracy: 0.7727\n",
      "Epoch 2293/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5292 - accuracy: 0.7727\n",
      "Epoch 2294/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5292 - accuracy: 0.7727\n",
      "Epoch 2295/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5292 - accuracy: 0.7727\n",
      "Epoch 2296/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5291 - accuracy: 0.7727\n",
      "Epoch 2297/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5291 - accuracy: 0.7727\n",
      "Epoch 2298/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5290 - accuracy: 0.7727\n",
      "Epoch 2299/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5290 - accuracy: 0.7727\n",
      "Epoch 2300/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5289 - accuracy: 0.7727\n",
      "Epoch 2301/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5289 - accuracy: 0.7727\n",
      "Epoch 2302/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5289 - accuracy: 0.7727\n",
      "Epoch 2303/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5288 - accuracy: 0.7727\n",
      "Epoch 2304/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5288 - accuracy: 0.7727\n",
      "Epoch 2305/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5287 - accuracy: 0.7727\n",
      "Epoch 2306/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5287 - accuracy: 0.7727\n",
      "Epoch 2307/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5287 - accuracy: 0.7727\n",
      "Epoch 2308/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5286 - accuracy: 0.7727\n",
      "Epoch 2309/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5286 - accuracy: 0.7727\n",
      "Epoch 2310/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5285 - accuracy: 0.7727\n",
      "Epoch 2311/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5285 - accuracy: 0.7727\n",
      "Epoch 2312/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5284 - accuracy: 0.7727\n",
      "Epoch 2313/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5284 - accuracy: 0.7727\n",
      "Epoch 2314/3500\n",
      "22/22 [==============================] - 0s 727us/step - loss: 0.5284 - accuracy: 0.7727\n",
      "Epoch 2315/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5283 - accuracy: 0.7727\n",
      "Epoch 2316/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5283 - accuracy: 0.7727\n",
      "Epoch 2317/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5282 - accuracy: 0.7727\n",
      "Epoch 2318/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5282 - accuracy: 0.7727\n",
      "Epoch 2319/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5281 - accuracy: 0.7727\n",
      "Epoch 2320/3500\n",
      "22/22 [==============================] - 0s 909us/step - loss: 0.5281 - accuracy: 0.7727\n",
      "Epoch 2321/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5281 - accuracy: 0.7727\n",
      "Epoch 2322/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5280 - accuracy: 0.7727\n",
      "Epoch 2323/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5280 - accuracy: 0.7727\n",
      "Epoch 2324/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5279 - accuracy: 0.7727\n",
      "Epoch 2325/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5279 - accuracy: 0.7727\n",
      "Epoch 2326/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5278 - accuracy: 0.7727\n",
      "Epoch 2327/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5278 - accuracy: 0.7727\n",
      "Epoch 2328/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5278 - accuracy: 0.7727\n",
      "Epoch 2329/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5277 - accuracy: 0.7727\n",
      "Epoch 2330/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5277 - accuracy: 0.7727\n",
      "Epoch 2331/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5276 - accuracy: 0.7727\n",
      "Epoch 2332/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5276 - accuracy: 0.7727\n",
      "Epoch 2333/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5276 - accuracy: 0.7727\n",
      "Epoch 2334/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5275 - accuracy: 0.7727\n",
      "Epoch 2335/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5275 - accuracy: 0.7727\n",
      "Epoch 2336/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5274 - accuracy: 0.7727\n",
      "Epoch 2337/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5274 - accuracy: 0.7727\n",
      "Epoch 2338/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5273 - accuracy: 0.7727\n",
      "Epoch 2339/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5273 - accuracy: 0.7727\n",
      "Epoch 2340/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5273 - accuracy: 0.7727\n",
      "Epoch 2341/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5272 - accuracy: 0.7727\n",
      "Epoch 2342/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5272 - accuracy: 0.7727\n",
      "Epoch 2343/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5271 - accuracy: 0.7727\n",
      "Epoch 2344/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5271 - accuracy: 0.7727\n",
      "Epoch 2345/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5270 - accuracy: 0.7727\n",
      "Epoch 2346/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5270 - accuracy: 0.7727\n",
      "Epoch 2347/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5270 - accuracy: 0.7727\n",
      "Epoch 2348/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5269 - accuracy: 0.7727\n",
      "Epoch 2349/3500\n",
      "22/22 [==============================] - 0s 546us/step - loss: 0.5269 - accuracy: 0.7727\n",
      "Epoch 2350/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5268 - accuracy: 0.7727\n",
      "Epoch 2351/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5268 - accuracy: 0.7727\n",
      "Epoch 2352/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5268 - accuracy: 0.7727\n",
      "Epoch 2353/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5267 - accuracy: 0.7727\n",
      "Epoch 2354/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5267 - accuracy: 0.7727\n",
      "Epoch 2355/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5266 - accuracy: 0.7727\n",
      "Epoch 2356/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5266 - accuracy: 0.7727\n",
      "Epoch 2357/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5265 - accuracy: 0.7727\n",
      "Epoch 2358/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5265 - accuracy: 0.7727\n",
      "Epoch 2359/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5265 - accuracy: 0.7727\n",
      "Epoch 2360/3500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.5264 - accuracy: 0.7727\n",
      "Epoch 2361/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5264 - accuracy: 0.7727\n",
      "Epoch 2362/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5263 - accuracy: 0.7727\n",
      "Epoch 2363/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5263 - accuracy: 0.7727\n",
      "Epoch 2364/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5263 - accuracy: 0.7727\n",
      "Epoch 2365/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5262 - accuracy: 0.7727\n",
      "Epoch 2366/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5262 - accuracy: 0.7727\n",
      "Epoch 2367/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5261 - accuracy: 0.7727\n",
      "Epoch 2368/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5261 - accuracy: 0.7727\n",
      "Epoch 2369/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5260 - accuracy: 0.7727\n",
      "Epoch 2370/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5260 - accuracy: 0.7727\n",
      "Epoch 2371/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5260 - accuracy: 0.7727\n",
      "Epoch 2372/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5259 - accuracy: 0.7727\n",
      "Epoch 2373/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5259 - accuracy: 0.7727\n",
      "Epoch 2374/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5258 - accuracy: 0.7727\n",
      "Epoch 2375/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5258 - accuracy: 0.7727\n",
      "Epoch 2376/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5258 - accuracy: 0.7727\n",
      "Epoch 2377/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5257 - accuracy: 0.7727\n",
      "Epoch 2378/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5257 - accuracy: 0.7727\n",
      "Epoch 2379/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5256 - accuracy: 0.7727\n",
      "Epoch 2380/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5256 - accuracy: 0.7727\n",
      "Epoch 2381/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5255 - accuracy: 0.7727\n",
      "Epoch 2382/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5255 - accuracy: 0.7727\n",
      "Epoch 2383/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5255 - accuracy: 0.7727\n",
      "Epoch 2384/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5254 - accuracy: 0.7727\n",
      "Epoch 2385/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5254 - accuracy: 0.7727\n",
      "Epoch 2386/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5253 - accuracy: 0.7727\n",
      "Epoch 2387/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5253 - accuracy: 0.7727\n",
      "Epoch 2388/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5253 - accuracy: 0.7727\n",
      "Epoch 2389/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5252 - accuracy: 0.7727\n",
      "Epoch 2390/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5252 - accuracy: 0.7727\n",
      "Epoch 2391/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5251 - accuracy: 0.7727\n",
      "Epoch 2392/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5251 - accuracy: 0.7727\n",
      "Epoch 2393/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5250 - accuracy: 0.7727\n",
      "Epoch 2394/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5250 - accuracy: 0.7727\n",
      "Epoch 2395/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5250 - accuracy: 0.7727\n",
      "Epoch 2396/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5249 - accuracy: 0.7727\n",
      "Epoch 2397/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5249 - accuracy: 0.7727\n",
      "Epoch 2398/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5248 - accuracy: 0.7727\n",
      "Epoch 2399/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5248 - accuracy: 0.7727\n",
      "Epoch 2400/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5248 - accuracy: 0.7727\n",
      "Epoch 2401/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5247 - accuracy: 0.7727\n",
      "Epoch 2402/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5247 - accuracy: 0.7727\n",
      "Epoch 2403/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5246 - accuracy: 0.7727\n",
      "Epoch 2404/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5246 - accuracy: 0.7727\n",
      "Epoch 2405/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5246 - accuracy: 0.7727\n",
      "Epoch 2406/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5245 - accuracy: 0.7727\n",
      "Epoch 2407/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5245 - accuracy: 0.7727\n",
      "Epoch 2408/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5244 - accuracy: 0.7727\n",
      "Epoch 2409/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5244 - accuracy: 0.7727\n",
      "Epoch 2410/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5243 - accuracy: 0.7727\n",
      "Epoch 2411/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5243 - accuracy: 0.7727\n",
      "Epoch 2412/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5243 - accuracy: 0.7727\n",
      "Epoch 2413/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5242 - accuracy: 0.7727\n",
      "Epoch 2414/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5242 - accuracy: 0.7727\n",
      "Epoch 2415/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5241 - accuracy: 0.7727\n",
      "Epoch 2416/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5241 - accuracy: 0.7727\n",
      "Epoch 2417/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5241 - accuracy: 0.7727\n",
      "Epoch 2418/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5240 - accuracy: 0.7727\n",
      "Epoch 2419/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5240 - accuracy: 0.7727\n",
      "Epoch 2420/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5239 - accuracy: 0.7727\n",
      "Epoch 2421/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5239 - accuracy: 0.7727\n",
      "Epoch 2422/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5239 - accuracy: 0.7727\n",
      "Epoch 2423/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5238 - accuracy: 0.7727\n",
      "Epoch 2424/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5238 - accuracy: 0.7727\n",
      "Epoch 2425/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5237 - accuracy: 0.7727\n",
      "Epoch 2426/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5237 - accuracy: 0.7727\n",
      "Epoch 2427/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5236 - accuracy: 0.7727\n",
      "Epoch 2428/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5236 - accuracy: 0.7727\n",
      "Epoch 2429/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5236 - accuracy: 0.7727\n",
      "Epoch 2430/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5235 - accuracy: 0.7727\n",
      "Epoch 2431/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5235 - accuracy: 0.7727\n",
      "Epoch 2432/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5234 - accuracy: 0.7727\n",
      "Epoch 2433/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5234 - accuracy: 0.7727\n",
      "Epoch 2434/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5234 - accuracy: 0.7727\n",
      "Epoch 2435/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5233 - accuracy: 0.7727\n",
      "Epoch 2436/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5233 - accuracy: 0.7727\n",
      "Epoch 2437/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5232 - accuracy: 0.7727\n",
      "Epoch 2438/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5232 - accuracy: 0.7727\n",
      "Epoch 2439/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5232 - accuracy: 0.7727\n",
      "Epoch 2440/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5231 - accuracy: 0.7727\n",
      "Epoch 2441/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5231 - accuracy: 0.7727\n",
      "Epoch 2442/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5230 - accuracy: 0.7727\n",
      "Epoch 2443/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5230 - accuracy: 0.7727\n",
      "Epoch 2444/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5230 - accuracy: 0.7727\n",
      "Epoch 2445/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5229 - accuracy: 0.7727\n",
      "Epoch 2446/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5229 - accuracy: 0.7727\n",
      "Epoch 2447/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5228 - accuracy: 0.7727\n",
      "Epoch 2448/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5228 - accuracy: 0.7727\n",
      "Epoch 2449/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5227 - accuracy: 0.7727\n",
      "Epoch 2450/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5227 - accuracy: 0.7727\n",
      "Epoch 2451/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5227 - accuracy: 0.7727\n",
      "Epoch 2452/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5226 - accuracy: 0.7727\n",
      "Epoch 2453/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5226 - accuracy: 0.7727\n",
      "Epoch 2454/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5225 - accuracy: 0.7727\n",
      "Epoch 2455/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5225 - accuracy: 0.7727\n",
      "Epoch 2456/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5225 - accuracy: 0.7727\n",
      "Epoch 2457/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5224 - accuracy: 0.7727\n",
      "Epoch 2458/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5224 - accuracy: 0.7727\n",
      "Epoch 2459/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5223 - accuracy: 0.7727\n",
      "Epoch 2460/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5223 - accuracy: 0.7727\n",
      "Epoch 2461/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5223 - accuracy: 0.7727\n",
      "Epoch 2462/3500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.5222 - accuracy: 0.7727\n",
      "Epoch 2463/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5222 - accuracy: 0.7727\n",
      "Epoch 2464/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5221 - accuracy: 0.7727\n",
      "Epoch 2465/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5221 - accuracy: 0.7727\n",
      "Epoch 2466/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5221 - accuracy: 0.7727\n",
      "Epoch 2467/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5220 - accuracy: 0.7727\n",
      "Epoch 2468/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5220 - accuracy: 0.7727\n",
      "Epoch 2469/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5219 - accuracy: 0.7727\n",
      "Epoch 2470/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5219 - accuracy: 0.7727\n",
      "Epoch 2471/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5219 - accuracy: 0.7727\n",
      "Epoch 2472/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5218 - accuracy: 0.7727\n",
      "Epoch 2473/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5218 - accuracy: 0.7727\n",
      "Epoch 2474/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5217 - accuracy: 0.7727\n",
      "Epoch 2475/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5217 - accuracy: 0.7727\n",
      "Epoch 2476/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5217 - accuracy: 0.7727\n",
      "Epoch 2477/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5216 - accuracy: 0.7727\n",
      "Epoch 2478/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5216 - accuracy: 0.7727\n",
      "Epoch 2479/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5215 - accuracy: 0.7727\n",
      "Epoch 2480/3500\n",
      "22/22 [==============================] - 0s 454us/step - loss: 0.5215 - accuracy: 0.7727\n",
      "Epoch 2481/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5214 - accuracy: 0.7727\n",
      "Epoch 2482/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5214 - accuracy: 0.7727\n",
      "Epoch 2483/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5214 - accuracy: 0.7727\n",
      "Epoch 2484/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5213 - accuracy: 0.7727\n",
      "Epoch 2485/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5213 - accuracy: 0.8182\n",
      "Epoch 2486/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5212 - accuracy: 0.8182\n",
      "Epoch 2487/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5212 - accuracy: 0.8182\n",
      "Epoch 2488/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5212 - accuracy: 0.8182\n",
      "Epoch 2489/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5211 - accuracy: 0.8182\n",
      "Epoch 2490/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5211 - accuracy: 0.8182\n",
      "Epoch 2491/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5210 - accuracy: 0.8182\n",
      "Epoch 2492/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5210 - accuracy: 0.8182\n",
      "Epoch 2493/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5210 - accuracy: 0.8182\n",
      "Epoch 2494/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5209 - accuracy: 0.8182\n",
      "Epoch 2495/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5209 - accuracy: 0.8182\n",
      "Epoch 2496/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5208 - accuracy: 0.8182\n",
      "Epoch 2497/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5208 - accuracy: 0.8182\n",
      "Epoch 2498/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5208 - accuracy: 0.8182\n",
      "Epoch 2499/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5207 - accuracy: 0.8182\n",
      "Epoch 2500/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5207 - accuracy: 0.8182\n",
      "Epoch 2501/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5206 - accuracy: 0.8182\n",
      "Epoch 2502/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5206 - accuracy: 0.8182\n",
      "Epoch 2503/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5206 - accuracy: 0.8182\n",
      "Epoch 2504/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5205 - accuracy: 0.8182\n",
      "Epoch 2505/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5205 - accuracy: 0.8182\n",
      "Epoch 2506/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5204 - accuracy: 0.8182\n",
      "Epoch 2507/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5204 - accuracy: 0.8182\n",
      "Epoch 2508/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5204 - accuracy: 0.8182\n",
      "Epoch 2509/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5203 - accuracy: 0.8182\n",
      "Epoch 2510/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5203 - accuracy: 0.8182\n",
      "Epoch 2511/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5202 - accuracy: 0.8182\n",
      "Epoch 2512/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5202 - accuracy: 0.8182\n",
      "Epoch 2513/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5202 - accuracy: 0.8182\n",
      "Epoch 2514/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5201 - accuracy: 0.8182\n",
      "Epoch 2515/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5201 - accuracy: 0.8182\n",
      "Epoch 2516/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5200 - accuracy: 0.8182\n",
      "Epoch 2517/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5200 - accuracy: 0.8182\n",
      "Epoch 2518/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5200 - accuracy: 0.8182\n",
      "Epoch 2519/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5199 - accuracy: 0.8182\n",
      "Epoch 2520/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5199 - accuracy: 0.8182\n",
      "Epoch 2521/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5198 - accuracy: 0.8182\n",
      "Epoch 2522/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5198 - accuracy: 0.8182\n",
      "Epoch 2523/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5198 - accuracy: 0.8182\n",
      "Epoch 2524/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5197 - accuracy: 0.8182\n",
      "Epoch 2525/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5197 - accuracy: 0.8182\n",
      "Epoch 2526/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5196 - accuracy: 0.8182\n",
      "Epoch 2527/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5196 - accuracy: 0.8182\n",
      "Epoch 2528/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5196 - accuracy: 0.8182\n",
      "Epoch 2529/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5195 - accuracy: 0.8182\n",
      "Epoch 2530/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5195 - accuracy: 0.8182\n",
      "Epoch 2531/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5194 - accuracy: 0.8182\n",
      "Epoch 2532/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5194 - accuracy: 0.8182\n",
      "Epoch 2533/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5194 - accuracy: 0.8182\n",
      "Epoch 2534/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5193 - accuracy: 0.8182\n",
      "Epoch 2535/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5193 - accuracy: 0.8182\n",
      "Epoch 2536/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5192 - accuracy: 0.8182\n",
      "Epoch 2537/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5192 - accuracy: 0.8182\n",
      "Epoch 2538/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5192 - accuracy: 0.8182\n",
      "Epoch 2539/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5191 - accuracy: 0.8182\n",
      "Epoch 2540/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5191 - accuracy: 0.8182\n",
      "Epoch 2541/3500\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.5190 - accuracy: 0.8182\n",
      "Epoch 2542/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5190 - accuracy: 0.8182\n",
      "Epoch 2543/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5190 - accuracy: 0.8182\n",
      "Epoch 2544/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5189 - accuracy: 0.8182\n",
      "Epoch 2545/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5189 - accuracy: 0.8182\n",
      "Epoch 2546/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5188 - accuracy: 0.8182\n",
      "Epoch 2547/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5188 - accuracy: 0.8182\n",
      "Epoch 2548/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5188 - accuracy: 0.8182\n",
      "Epoch 2549/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5187 - accuracy: 0.8182\n",
      "Epoch 2550/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5187 - accuracy: 0.8182\n",
      "Epoch 2551/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5187 - accuracy: 0.8182\n",
      "Epoch 2552/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5186 - accuracy: 0.8182\n",
      "Epoch 2553/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5186 - accuracy: 0.8182\n",
      "Epoch 2554/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5185 - accuracy: 0.8182\n",
      "Epoch 2555/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5185 - accuracy: 0.8182\n",
      "Epoch 2556/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5185 - accuracy: 0.8182\n",
      "Epoch 2557/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5184 - accuracy: 0.8182\n",
      "Epoch 2558/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5184 - accuracy: 0.8182\n",
      "Epoch 2559/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5183 - accuracy: 0.8182\n",
      "Epoch 2560/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5183 - accuracy: 0.8182\n",
      "Epoch 2561/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5183 - accuracy: 0.8182\n",
      "Epoch 2562/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5182 - accuracy: 0.8182\n",
      "Epoch 2563/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5182 - accuracy: 0.8182\n",
      "Epoch 2564/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5181 - accuracy: 0.8182\n",
      "Epoch 2565/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5181 - accuracy: 0.8182\n",
      "Epoch 2566/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5181 - accuracy: 0.8182\n",
      "Epoch 2567/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5180 - accuracy: 0.8182\n",
      "Epoch 2568/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5180 - accuracy: 0.8182\n",
      "Epoch 2569/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5179 - accuracy: 0.8182\n",
      "Epoch 2570/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5179 - accuracy: 0.8182\n",
      "Epoch 2571/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5179 - accuracy: 0.8182\n",
      "Epoch 2572/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5178 - accuracy: 0.8182\n",
      "Epoch 2573/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5178 - accuracy: 0.8182\n",
      "Epoch 2574/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5177 - accuracy: 0.8182\n",
      "Epoch 2575/3500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.5177 - accuracy: 0.8182\n",
      "Epoch 2576/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5177 - accuracy: 0.8182\n",
      "Epoch 2577/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5176 - accuracy: 0.8182\n",
      "Epoch 2578/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5176 - accuracy: 0.8182\n",
      "Epoch 2579/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5175 - accuracy: 0.8182\n",
      "Epoch 2580/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5175 - accuracy: 0.8182\n",
      "Epoch 2581/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5175 - accuracy: 0.8182\n",
      "Epoch 2582/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5174 - accuracy: 0.8182\n",
      "Epoch 2583/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5174 - accuracy: 0.8182\n",
      "Epoch 2584/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5174 - accuracy: 0.8182\n",
      "Epoch 2585/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5173 - accuracy: 0.8182\n",
      "Epoch 2586/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5173 - accuracy: 0.8182\n",
      "Epoch 2587/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5172 - accuracy: 0.8182\n",
      "Epoch 2588/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5172 - accuracy: 0.8182\n",
      "Epoch 2589/3500\n",
      "22/22 [==============================] - 0s 546us/step - loss: 0.5172 - accuracy: 0.8182\n",
      "Epoch 2590/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5171 - accuracy: 0.8182\n",
      "Epoch 2591/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5171 - accuracy: 0.8182\n",
      "Epoch 2592/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5170 - accuracy: 0.8182\n",
      "Epoch 2593/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5170 - accuracy: 0.8182\n",
      "Epoch 2594/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5170 - accuracy: 0.8182\n",
      "Epoch 2595/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5169 - accuracy: 0.8182\n",
      "Epoch 2596/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5169 - accuracy: 0.8182\n",
      "Epoch 2597/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5168 - accuracy: 0.8182\n",
      "Epoch 2598/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5168 - accuracy: 0.8182\n",
      "Epoch 2599/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5168 - accuracy: 0.8182\n",
      "Epoch 2600/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5167 - accuracy: 0.8182\n",
      "Epoch 2601/3500\n",
      "22/22 [==============================] - 0s 909us/step - loss: 0.5167 - accuracy: 0.8182\n",
      "Epoch 2602/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5166 - accuracy: 0.8182\n",
      "Epoch 2603/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5166 - accuracy: 0.8182\n",
      "Epoch 2604/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5166 - accuracy: 0.8182\n",
      "Epoch 2605/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5165 - accuracy: 0.8182\n",
      "Epoch 2606/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5165 - accuracy: 0.8182\n",
      "Epoch 2607/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5165 - accuracy: 0.8182\n",
      "Epoch 2608/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5164 - accuracy: 0.8182\n",
      "Epoch 2609/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5164 - accuracy: 0.8182\n",
      "Epoch 2610/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5163 - accuracy: 0.8182\n",
      "Epoch 2611/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5163 - accuracy: 0.8182\n",
      "Epoch 2612/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5163 - accuracy: 0.8182\n",
      "Epoch 2613/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5162 - accuracy: 0.8182\n",
      "Epoch 2614/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5162 - accuracy: 0.8182\n",
      "Epoch 2615/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5161 - accuracy: 0.8182\n",
      "Epoch 2616/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5161 - accuracy: 0.8182\n",
      "Epoch 2617/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5161 - accuracy: 0.8182\n",
      "Epoch 2618/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5160 - accuracy: 0.8182\n",
      "Epoch 2619/3500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.5160 - accuracy: 0.8182\n",
      "Epoch 2620/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5159 - accuracy: 0.8182\n",
      "Epoch 2621/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5159 - accuracy: 0.8182\n",
      "Epoch 2622/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5159 - accuracy: 0.8182\n",
      "Epoch 2623/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5158 - accuracy: 0.8182\n",
      "Epoch 2624/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5158 - accuracy: 0.8182\n",
      "Epoch 2625/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5158 - accuracy: 0.8182\n",
      "Epoch 2626/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5157 - accuracy: 0.8182\n",
      "Epoch 2627/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5157 - accuracy: 0.8182\n",
      "Epoch 2628/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5156 - accuracy: 0.8182\n",
      "Epoch 2629/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5156 - accuracy: 0.8182\n",
      "Epoch 2630/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5156 - accuracy: 0.8182\n",
      "Epoch 2631/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5155 - accuracy: 0.8182\n",
      "Epoch 2632/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5155 - accuracy: 0.8182\n",
      "Epoch 2633/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5154 - accuracy: 0.8182\n",
      "Epoch 2634/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5154 - accuracy: 0.8182\n",
      "Epoch 2635/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5154 - accuracy: 0.8182\n",
      "Epoch 2636/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5153 - accuracy: 0.8182\n",
      "Epoch 2637/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5153 - accuracy: 0.8182\n",
      "Epoch 2638/3500\n",
      "22/22 [==============================] - 0s 500us/step - loss: 0.5153 - accuracy: 0.8182\n",
      "Epoch 2639/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5152 - accuracy: 0.8636\n",
      "Epoch 2640/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5152 - accuracy: 0.8636\n",
      "Epoch 2641/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5151 - accuracy: 0.8636\n",
      "Epoch 2642/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5151 - accuracy: 0.8636\n",
      "Epoch 2643/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5151 - accuracy: 0.8636\n",
      "Epoch 2644/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5150 - accuracy: 0.8636\n",
      "Epoch 2645/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5150 - accuracy: 0.8636\n",
      "Epoch 2646/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5149 - accuracy: 0.8636\n",
      "Epoch 2647/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5149 - accuracy: 0.8636\n",
      "Epoch 2648/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5149 - accuracy: 0.8636\n",
      "Epoch 2649/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5148 - accuracy: 0.8636\n",
      "Epoch 2650/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5148 - accuracy: 0.8636\n",
      "Epoch 2651/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5148 - accuracy: 0.8636\n",
      "Epoch 2652/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5147 - accuracy: 0.8636\n",
      "Epoch 2653/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5147 - accuracy: 0.8636\n",
      "Epoch 2654/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5146 - accuracy: 0.8636\n",
      "Epoch 2655/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5146 - accuracy: 0.8636\n",
      "Epoch 2656/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5146 - accuracy: 0.8636\n",
      "Epoch 2657/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5145 - accuracy: 0.8636\n",
      "Epoch 2658/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5145 - accuracy: 0.8636\n",
      "Epoch 2659/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5144 - accuracy: 0.8636\n",
      "Epoch 2660/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5144 - accuracy: 0.8636\n",
      "Epoch 2661/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5144 - accuracy: 0.8636\n",
      "Epoch 2662/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5143 - accuracy: 0.8636\n",
      "Epoch 2663/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5143 - accuracy: 0.8636\n",
      "Epoch 2664/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5143 - accuracy: 0.8636\n",
      "Epoch 2665/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5142 - accuracy: 0.8636\n",
      "Epoch 2666/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5142 - accuracy: 0.8636\n",
      "Epoch 2667/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5141 - accuracy: 0.8636\n",
      "Epoch 2668/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5141 - accuracy: 0.8636\n",
      "Epoch 2669/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5141 - accuracy: 0.8636\n",
      "Epoch 2670/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5140 - accuracy: 0.8636\n",
      "Epoch 2671/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5140 - accuracy: 0.8636\n",
      "Epoch 2672/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5139 - accuracy: 0.8636\n",
      "Epoch 2673/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5139 - accuracy: 0.8636\n",
      "Epoch 2674/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5139 - accuracy: 0.8636\n",
      "Epoch 2675/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5138 - accuracy: 0.8636\n",
      "Epoch 2676/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5138 - accuracy: 0.8636\n",
      "Epoch 2677/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5138 - accuracy: 0.8636\n",
      "Epoch 2678/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5137 - accuracy: 0.8636\n",
      "Epoch 2679/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5137 - accuracy: 0.8636\n",
      "Epoch 2680/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5136 - accuracy: 0.8636\n",
      "Epoch 2681/3500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.5136 - accuracy: 0.8636\n",
      "Epoch 2682/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5136 - accuracy: 0.8636\n",
      "Epoch 2683/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5135 - accuracy: 0.8636\n",
      "Epoch 2684/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5135 - accuracy: 0.8636\n",
      "Epoch 2685/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5135 - accuracy: 0.8636\n",
      "Epoch 2686/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5134 - accuracy: 0.8636\n",
      "Epoch 2687/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5134 - accuracy: 0.8636\n",
      "Epoch 2688/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5133 - accuracy: 0.8636\n",
      "Epoch 2689/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5133 - accuracy: 0.8636\n",
      "Epoch 2690/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5133 - accuracy: 0.8636\n",
      "Epoch 2691/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5132 - accuracy: 0.8636\n",
      "Epoch 2692/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5132 - accuracy: 0.8636\n",
      "Epoch 2693/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5131 - accuracy: 0.8636\n",
      "Epoch 2694/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5131 - accuracy: 0.8636\n",
      "Epoch 2695/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5131 - accuracy: 0.8636\n",
      "Epoch 2696/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5130 - accuracy: 0.8636\n",
      "Epoch 2697/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5130 - accuracy: 0.8636\n",
      "Epoch 2698/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5130 - accuracy: 0.8636\n",
      "Epoch 2699/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5129 - accuracy: 0.8636\n",
      "Epoch 2700/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5129 - accuracy: 0.8636\n",
      "Epoch 2701/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5128 - accuracy: 0.8636\n",
      "Epoch 2702/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5128 - accuracy: 0.8636\n",
      "Epoch 2703/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5128 - accuracy: 0.8636\n",
      "Epoch 2704/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5127 - accuracy: 0.8636\n",
      "Epoch 2705/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5127 - accuracy: 0.8636\n",
      "Epoch 2706/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5127 - accuracy: 0.8636\n",
      "Epoch 2707/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5126 - accuracy: 0.8636\n",
      "Epoch 2708/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5126 - accuracy: 0.8636\n",
      "Epoch 2709/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5125 - accuracy: 0.8636\n",
      "Epoch 2710/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5125 - accuracy: 0.8636\n",
      "Epoch 2711/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5125 - accuracy: 0.8636\n",
      "Epoch 2712/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5124 - accuracy: 0.8636\n",
      "Epoch 2713/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5124 - accuracy: 0.8636\n",
      "Epoch 2714/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5124 - accuracy: 0.8636\n",
      "Epoch 2715/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5123 - accuracy: 0.8636\n",
      "Epoch 2716/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5123 - accuracy: 0.8636\n",
      "Epoch 2717/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5122 - accuracy: 0.8636\n",
      "Epoch 2718/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5122 - accuracy: 0.8636\n",
      "Epoch 2719/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5122 - accuracy: 0.8636\n",
      "Epoch 2720/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5121 - accuracy: 0.8636\n",
      "Epoch 2721/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5121 - accuracy: 0.8636\n",
      "Epoch 2722/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5121 - accuracy: 0.8636\n",
      "Epoch 2723/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5120 - accuracy: 0.8636\n",
      "Epoch 2724/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5120 - accuracy: 0.8636\n",
      "Epoch 2725/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5119 - accuracy: 0.8636\n",
      "Epoch 2726/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5119 - accuracy: 0.8636\n",
      "Epoch 2727/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.8636\n",
      "Epoch 2728/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5118 - accuracy: 0.8636\n",
      "Epoch 2729/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5118 - accuracy: 0.8636\n",
      "Epoch 2730/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5117 - accuracy: 0.8636\n",
      "Epoch 2731/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5117 - accuracy: 0.8636\n",
      "Epoch 2732/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5117 - accuracy: 0.8636\n",
      "Epoch 2733/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5116 - accuracy: 0.8636\n",
      "Epoch 2734/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5116 - accuracy: 0.8636\n",
      "Epoch 2735/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5116 - accuracy: 0.8636\n",
      "Epoch 2736/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5115 - accuracy: 0.8636\n",
      "Epoch 2737/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5115 - accuracy: 0.8636\n",
      "Epoch 2738/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5114 - accuracy: 0.8636\n",
      "Epoch 2739/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5114 - accuracy: 0.8636\n",
      "Epoch 2740/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5114 - accuracy: 0.8636\n",
      "Epoch 2741/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5113 - accuracy: 0.8636\n",
      "Epoch 2742/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5113 - accuracy: 0.8636\n",
      "Epoch 2743/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5113 - accuracy: 0.8636\n",
      "Epoch 2744/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5112 - accuracy: 0.8636\n",
      "Epoch 2745/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5112 - accuracy: 0.8636\n",
      "Epoch 2746/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5111 - accuracy: 0.8636\n",
      "Epoch 2747/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5111 - accuracy: 0.8636\n",
      "Epoch 2748/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5111 - accuracy: 0.8636\n",
      "Epoch 2749/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5110 - accuracy: 0.8636\n",
      "Epoch 2750/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5110 - accuracy: 0.8636\n",
      "Epoch 2751/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5110 - accuracy: 0.8636\n",
      "Epoch 2752/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5109 - accuracy: 0.8636\n",
      "Epoch 2753/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5109 - accuracy: 0.8636\n",
      "Epoch 2754/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5109 - accuracy: 0.8636\n",
      "Epoch 2755/3500\n",
      "22/22 [==============================] - 0s 454us/step - loss: 0.5108 - accuracy: 0.8636\n",
      "Epoch 2756/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5108 - accuracy: 0.8636\n",
      "Epoch 2757/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5107 - accuracy: 0.8636\n",
      "Epoch 2758/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5107 - accuracy: 0.8636\n",
      "Epoch 2759/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5107 - accuracy: 0.8636\n",
      "Epoch 2760/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5106 - accuracy: 0.8636\n",
      "Epoch 2761/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5106 - accuracy: 0.8636\n",
      "Epoch 2762/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5106 - accuracy: 0.8636\n",
      "Epoch 2763/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5105 - accuracy: 0.8636\n",
      "Epoch 2764/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5105 - accuracy: 0.8636\n",
      "Epoch 2765/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5104 - accuracy: 0.8636\n",
      "Epoch 2766/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5104 - accuracy: 0.8636\n",
      "Epoch 2767/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5104 - accuracy: 0.8636\n",
      "Epoch 2768/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5103 - accuracy: 0.8636\n",
      "Epoch 2769/3500\n",
      "22/22 [==============================] - 0s 591us/step - loss: 0.5103 - accuracy: 0.8636\n",
      "Epoch 2770/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5103 - accuracy: 0.8636\n",
      "Epoch 2771/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5102 - accuracy: 0.8636\n",
      "Epoch 2772/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5102 - accuracy: 0.8636\n",
      "Epoch 2773/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5101 - accuracy: 0.8636\n",
      "Epoch 2774/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5101 - accuracy: 0.8636\n",
      "Epoch 2775/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5101 - accuracy: 0.8636\n",
      "Epoch 2776/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5100 - accuracy: 0.8636\n",
      "Epoch 2777/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.8636\n",
      "Epoch 2778/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5100 - accuracy: 0.8636\n",
      "Epoch 2779/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5099 - accuracy: 0.8636\n",
      "Epoch 2780/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5099 - accuracy: 0.8636\n",
      "Epoch 2781/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5098 - accuracy: 0.8636\n",
      "Epoch 2782/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5098 - accuracy: 0.8636\n",
      "Epoch 2783/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5098 - accuracy: 0.8636\n",
      "Epoch 2784/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5097 - accuracy: 0.8636\n",
      "Epoch 2785/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5097 - accuracy: 0.8636\n",
      "Epoch 2786/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5097 - accuracy: 0.8636\n",
      "Epoch 2787/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5096 - accuracy: 0.8636\n",
      "Epoch 2788/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5096 - accuracy: 0.8636\n",
      "Epoch 2789/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5096 - accuracy: 0.8636\n",
      "Epoch 2790/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5095 - accuracy: 0.8636\n",
      "Epoch 2791/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5095 - accuracy: 0.8636\n",
      "Epoch 2792/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5094 - accuracy: 0.8636\n",
      "Epoch 2793/3500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.5094 - accuracy: 0.8636\n",
      "Epoch 2794/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5094 - accuracy: 0.8636\n",
      "Epoch 2795/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5093 - accuracy: 0.8636\n",
      "Epoch 2796/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5093 - accuracy: 0.8636\n",
      "Epoch 2797/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5093 - accuracy: 0.8636\n",
      "Epoch 2798/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5092 - accuracy: 0.8636\n",
      "Epoch 2799/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5092 - accuracy: 0.8636\n",
      "Epoch 2800/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5091 - accuracy: 0.8636\n",
      "Epoch 2801/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5091 - accuracy: 0.8636\n",
      "Epoch 2802/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5091 - accuracy: 0.8636\n",
      "Epoch 2803/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5090 - accuracy: 0.8636\n",
      "Epoch 2804/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5090 - accuracy: 0.8636\n",
      "Epoch 2805/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5090 - accuracy: 0.8636\n",
      "Epoch 2806/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5089 - accuracy: 0.8636\n",
      "Epoch 2807/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5089 - accuracy: 0.8636\n",
      "Epoch 2808/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5089 - accuracy: 0.8636\n",
      "Epoch 2809/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5088 - accuracy: 0.8636\n",
      "Epoch 2810/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5088 - accuracy: 0.8636\n",
      "Epoch 2811/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5087 - accuracy: 0.8636\n",
      "Epoch 2812/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5087 - accuracy: 0.8636\n",
      "Epoch 2813/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5087 - accuracy: 0.8636\n",
      "Epoch 2814/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5086 - accuracy: 0.8636\n",
      "Epoch 2815/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.8636\n",
      "Epoch 2816/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5086 - accuracy: 0.8636\n",
      "Epoch 2817/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5085 - accuracy: 0.8636\n",
      "Epoch 2818/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5085 - accuracy: 0.8636\n",
      "Epoch 2819/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5084 - accuracy: 0.8636\n",
      "Epoch 2820/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5084 - accuracy: 0.8636\n",
      "Epoch 2821/3500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.5084 - accuracy: 0.8636\n",
      "Epoch 2822/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5083 - accuracy: 0.8636\n",
      "Epoch 2823/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5083 - accuracy: 0.8636\n",
      "Epoch 2824/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5083 - accuracy: 0.8636\n",
      "Epoch 2825/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5082 - accuracy: 0.8636\n",
      "Epoch 2826/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5082 - accuracy: 0.8636\n",
      "Epoch 2827/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5082 - accuracy: 0.8636\n",
      "Epoch 2828/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5081 - accuracy: 0.8636\n",
      "Epoch 2829/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5081 - accuracy: 0.8636\n",
      "Epoch 2830/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5080 - accuracy: 0.8636\n",
      "Epoch 2831/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5080 - accuracy: 0.8636\n",
      "Epoch 2832/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5080 - accuracy: 0.8636\n",
      "Epoch 2833/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5079 - accuracy: 0.8636\n",
      "Epoch 2834/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5079 - accuracy: 0.8636\n",
      "Epoch 2835/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5079 - accuracy: 0.8636\n",
      "Epoch 2836/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5078 - accuracy: 0.8636\n",
      "Epoch 2837/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5078 - accuracy: 0.8636\n",
      "Epoch 2838/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5078 - accuracy: 0.8636\n",
      "Epoch 2839/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5077 - accuracy: 0.8636\n",
      "Epoch 2840/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5077 - accuracy: 0.8636\n",
      "Epoch 2841/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5076 - accuracy: 0.8636\n",
      "Epoch 2842/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5076 - accuracy: 0.8636\n",
      "Epoch 2843/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5076 - accuracy: 0.8636\n",
      "Epoch 2844/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5075 - accuracy: 0.8636\n",
      "Epoch 2845/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5075 - accuracy: 0.8636\n",
      "Epoch 2846/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5075 - accuracy: 0.8636\n",
      "Epoch 2847/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5074 - accuracy: 0.8636\n",
      "Epoch 2848/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5074 - accuracy: 0.8636\n",
      "Epoch 2849/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5074 - accuracy: 0.8636\n",
      "Epoch 2850/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5073 - accuracy: 0.8636\n",
      "Epoch 2851/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5073 - accuracy: 0.8636\n",
      "Epoch 2852/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5072 - accuracy: 0.8636\n",
      "Epoch 2853/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5072 - accuracy: 0.8636\n",
      "Epoch 2854/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5072 - accuracy: 0.8636\n",
      "Epoch 2855/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5071 - accuracy: 0.8636\n",
      "Epoch 2856/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5071 - accuracy: 0.8636\n",
      "Epoch 2857/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5071 - accuracy: 0.8636\n",
      "Epoch 2858/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5070 - accuracy: 0.8636\n",
      "Epoch 2859/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5070 - accuracy: 0.8636\n",
      "Epoch 2860/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5070 - accuracy: 0.8636\n",
      "Epoch 2861/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5069 - accuracy: 0.8636\n",
      "Epoch 2862/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5069 - accuracy: 0.8636\n",
      "Epoch 2863/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5068 - accuracy: 0.8636\n",
      "Epoch 2864/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5068 - accuracy: 0.8636\n",
      "Epoch 2865/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5068 - accuracy: 0.8636\n",
      "Epoch 2866/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5067 - accuracy: 0.8636\n",
      "Epoch 2867/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5067 - accuracy: 0.8636\n",
      "Epoch 2868/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5067 - accuracy: 0.8636\n",
      "Epoch 2869/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5066 - accuracy: 0.8636\n",
      "Epoch 2870/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5066 - accuracy: 0.8636\n",
      "Epoch 2871/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5066 - accuracy: 0.8636\n",
      "Epoch 2872/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5065 - accuracy: 0.8636\n",
      "Epoch 2873/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5065 - accuracy: 0.8636\n",
      "Epoch 2874/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5065 - accuracy: 0.8636\n",
      "Epoch 2875/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5064 - accuracy: 0.8636\n",
      "Epoch 2876/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5064 - accuracy: 0.8636\n",
      "Epoch 2877/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5063 - accuracy: 0.8636\n",
      "Epoch 2878/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5063 - accuracy: 0.8636\n",
      "Epoch 2879/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5063 - accuracy: 0.8636\n",
      "Epoch 2880/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5062 - accuracy: 0.8636\n",
      "Epoch 2881/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5062 - accuracy: 0.8636\n",
      "Epoch 2882/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5062 - accuracy: 0.8636\n",
      "Epoch 2883/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5061 - accuracy: 0.8636\n",
      "Epoch 2884/3500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.5061 - accuracy: 0.8636\n",
      "Epoch 2885/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5061 - accuracy: 0.8636\n",
      "Epoch 2886/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5060 - accuracy: 0.8636\n",
      "Epoch 2887/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5060 - accuracy: 0.8636\n",
      "Epoch 2888/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5059 - accuracy: 0.8636\n",
      "Epoch 2889/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5059 - accuracy: 0.8636\n",
      "Epoch 2890/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.5059 - accuracy: 0.8636\n",
      "Epoch 2891/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5058 - accuracy: 0.8636\n",
      "Epoch 2892/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5058 - accuracy: 0.8636\n",
      "Epoch 2893/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5058 - accuracy: 0.8636\n",
      "Epoch 2894/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5057 - accuracy: 0.8636\n",
      "Epoch 2895/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5057 - accuracy: 0.8636\n",
      "Epoch 2896/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5057 - accuracy: 0.8636\n",
      "Epoch 2897/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5056 - accuracy: 0.8636\n",
      "Epoch 2898/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5056 - accuracy: 0.8636\n",
      "Epoch 2899/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5056 - accuracy: 0.8636\n",
      "Epoch 2900/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5055 - accuracy: 0.8636\n",
      "Epoch 2901/3500\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.5055 - accuracy: 0.8636\n",
      "Epoch 2902/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5054 - accuracy: 0.8636\n",
      "Epoch 2903/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5054 - accuracy: 0.8636\n",
      "Epoch 2904/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5054 - accuracy: 0.8636\n",
      "Epoch 2905/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5053 - accuracy: 0.8636\n",
      "Epoch 2906/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5053 - accuracy: 0.8636\n",
      "Epoch 2907/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5053 - accuracy: 0.8636\n",
      "Epoch 2908/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5052 - accuracy: 0.8636\n",
      "Epoch 2909/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5052 - accuracy: 0.8636\n",
      "Epoch 2910/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5052 - accuracy: 0.8636\n",
      "Epoch 2911/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5051 - accuracy: 0.8636\n",
      "Epoch 2912/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5051 - accuracy: 0.8636\n",
      "Epoch 2913/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5051 - accuracy: 0.8636\n",
      "Epoch 2914/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5050 - accuracy: 0.8636\n",
      "Epoch 2915/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5050 - accuracy: 0.8636\n",
      "Epoch 2916/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5049 - accuracy: 0.8636\n",
      "Epoch 2917/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5049 - accuracy: 0.8636\n",
      "Epoch 2918/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5049 - accuracy: 0.8636\n",
      "Epoch 2919/3500\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.5048 - accuracy: 0.8636\n",
      "Epoch 2920/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5048 - accuracy: 0.8636\n",
      "Epoch 2921/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5048 - accuracy: 0.8636\n",
      "Epoch 2922/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5047 - accuracy: 0.8636\n",
      "Epoch 2923/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5047 - accuracy: 0.8636\n",
      "Epoch 2924/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5047 - accuracy: 0.8636\n",
      "Epoch 2925/3500\n",
      "22/22 [==============================] - 0s 500us/step - loss: 0.5046 - accuracy: 0.8636\n",
      "Epoch 2926/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5046 - accuracy: 0.8636\n",
      "Epoch 2927/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5046 - accuracy: 0.8636\n",
      "Epoch 2928/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5045 - accuracy: 0.8636\n",
      "Epoch 2929/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5045 - accuracy: 0.8636\n",
      "Epoch 2930/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5044 - accuracy: 0.8636\n",
      "Epoch 2931/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5044 - accuracy: 0.8636\n",
      "Epoch 2932/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5044 - accuracy: 0.8636\n",
      "Epoch 2933/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5043 - accuracy: 0.8636\n",
      "Epoch 2934/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5043 - accuracy: 0.8636\n",
      "Epoch 2935/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5043 - accuracy: 0.8636\n",
      "Epoch 2936/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5042 - accuracy: 0.8636\n",
      "Epoch 2937/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5042 - accuracy: 0.8636\n",
      "Epoch 2938/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5042 - accuracy: 0.8636\n",
      "Epoch 2939/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5041 - accuracy: 0.8636\n",
      "Epoch 2940/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5041 - accuracy: 0.8636\n",
      "Epoch 2941/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5041 - accuracy: 0.8636\n",
      "Epoch 2942/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5040 - accuracy: 0.8636\n",
      "Epoch 2943/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5040 - accuracy: 0.8636\n",
      "Epoch 2944/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5040 - accuracy: 0.8636\n",
      "Epoch 2945/3500\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.5039 - accuracy: 0.8636\n",
      "Epoch 2946/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5039 - accuracy: 0.8636\n",
      "Epoch 2947/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5038 - accuracy: 0.8636\n",
      "Epoch 2948/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5038 - accuracy: 0.8636\n",
      "Epoch 2949/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5038 - accuracy: 0.8636\n",
      "Epoch 2950/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5037 - accuracy: 0.8636\n",
      "Epoch 2951/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5037 - accuracy: 0.8636\n",
      "Epoch 2952/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5037 - accuracy: 0.8636\n",
      "Epoch 2953/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5036 - accuracy: 0.8636\n",
      "Epoch 2954/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5036 - accuracy: 0.8636\n",
      "Epoch 2955/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5036 - accuracy: 0.8636\n",
      "Epoch 2956/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5035 - accuracy: 0.8636\n",
      "Epoch 2957/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5035 - accuracy: 0.8636\n",
      "Epoch 2958/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5035 - accuracy: 0.8636\n",
      "Epoch 2959/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5034 - accuracy: 0.8636\n",
      "Epoch 2960/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5034 - accuracy: 0.8636\n",
      "Epoch 2961/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5034 - accuracy: 0.8636\n",
      "Epoch 2962/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5033 - accuracy: 0.8636\n",
      "Epoch 2963/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5033 - accuracy: 0.8636\n",
      "Epoch 2964/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5033 - accuracy: 0.8636\n",
      "Epoch 2965/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5032 - accuracy: 0.8636\n",
      "Epoch 2966/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5032 - accuracy: 0.8636\n",
      "Epoch 2967/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5031 - accuracy: 0.8636\n",
      "Epoch 2968/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5031 - accuracy: 0.8636\n",
      "Epoch 2969/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5031 - accuracy: 0.8636\n",
      "Epoch 2970/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5030 - accuracy: 0.8636\n",
      "Epoch 2971/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5030 - accuracy: 0.8636\n",
      "Epoch 2972/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5030 - accuracy: 0.8636\n",
      "Epoch 2973/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5029 - accuracy: 0.8636\n",
      "Epoch 2974/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5029 - accuracy: 0.8636\n",
      "Epoch 2975/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5029 - accuracy: 0.8636\n",
      "Epoch 2976/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5028 - accuracy: 0.8636\n",
      "Epoch 2977/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5028 - accuracy: 0.8636\n",
      "Epoch 2978/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5028 - accuracy: 0.8636\n",
      "Epoch 2979/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5027 - accuracy: 0.8636\n",
      "Epoch 2980/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5027 - accuracy: 0.8636\n",
      "Epoch 2981/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5027 - accuracy: 0.8636\n",
      "Epoch 2982/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5026 - accuracy: 0.8636\n",
      "Epoch 2983/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5026 - accuracy: 0.8636\n",
      "Epoch 2984/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5026 - accuracy: 0.8636\n",
      "Epoch 2985/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5025 - accuracy: 0.8636\n",
      "Epoch 2986/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5025 - accuracy: 0.8636\n",
      "Epoch 2987/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5024 - accuracy: 0.8636\n",
      "Epoch 2988/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5024 - accuracy: 0.8636\n",
      "Epoch 2989/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5024 - accuracy: 0.8636\n",
      "Epoch 2990/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5023 - accuracy: 0.8636\n",
      "Epoch 2991/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5023 - accuracy: 0.8636\n",
      "Epoch 2992/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5023 - accuracy: 0.8636\n",
      "Epoch 2993/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5022 - accuracy: 0.8636\n",
      "Epoch 2994/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5022 - accuracy: 0.8636\n",
      "Epoch 2995/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5022 - accuracy: 0.8636\n",
      "Epoch 2996/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5021 - accuracy: 0.8636\n",
      "Epoch 2997/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5021 - accuracy: 0.8636\n",
      "Epoch 2998/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5021 - accuracy: 0.8636\n",
      "Epoch 2999/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5020 - accuracy: 0.8636\n",
      "Epoch 3000/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5020 - accuracy: 0.8636\n",
      "Epoch 3001/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5020 - accuracy: 0.8636\n",
      "Epoch 3002/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5019 - accuracy: 0.8636\n",
      "Epoch 3003/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5019 - accuracy: 0.8636\n",
      "Epoch 3004/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.5019 - accuracy: 0.8636\n",
      "Epoch 3005/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5018 - accuracy: 0.8636\n",
      "Epoch 3006/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5018 - accuracy: 0.8636\n",
      "Epoch 3007/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5018 - accuracy: 0.8636\n",
      "Epoch 3008/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.8636\n",
      "Epoch 3009/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5017 - accuracy: 0.8636\n",
      "Epoch 3010/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5016 - accuracy: 0.8636\n",
      "Epoch 3011/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5016 - accuracy: 0.8636\n",
      "Epoch 3012/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5016 - accuracy: 0.8636\n",
      "Epoch 3013/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5015 - accuracy: 0.8636\n",
      "Epoch 3014/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5015 - accuracy: 0.8636\n",
      "Epoch 3015/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5015 - accuracy: 0.8636\n",
      "Epoch 3016/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5014 - accuracy: 0.8636\n",
      "Epoch 3017/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5014 - accuracy: 0.8636\n",
      "Epoch 3018/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5014 - accuracy: 0.8636\n",
      "Epoch 3019/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.5013 - accuracy: 0.8636\n",
      "Epoch 3020/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5013 - accuracy: 0.8636\n",
      "Epoch 3021/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5013 - accuracy: 0.8636\n",
      "Epoch 3022/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5012 - accuracy: 0.8636\n",
      "Epoch 3023/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5012 - accuracy: 0.8636\n",
      "Epoch 3024/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5012 - accuracy: 0.8636\n",
      "Epoch 3025/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5011 - accuracy: 0.8636\n",
      "Epoch 3026/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5011 - accuracy: 0.8636\n",
      "Epoch 3027/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5011 - accuracy: 0.8636\n",
      "Epoch 3028/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5010 - accuracy: 0.8636\n",
      "Epoch 3029/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5010 - accuracy: 0.8636\n",
      "Epoch 3030/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5010 - accuracy: 0.8636\n",
      "Epoch 3031/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5009 - accuracy: 0.8636\n",
      "Epoch 3032/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5009 - accuracy: 0.8636\n",
      "Epoch 3033/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5009 - accuracy: 0.8636\n",
      "Epoch 3034/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5008 - accuracy: 0.8636\n",
      "Epoch 3035/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.5008 - accuracy: 0.8636\n",
      "Epoch 3036/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.5007 - accuracy: 0.8636\n",
      "Epoch 3037/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5007 - accuracy: 0.8636\n",
      "Epoch 3038/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5007 - accuracy: 0.8636\n",
      "Epoch 3039/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5006 - accuracy: 0.8636\n",
      "Epoch 3040/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5006 - accuracy: 0.8636\n",
      "Epoch 3041/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5006 - accuracy: 0.8636\n",
      "Epoch 3042/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5005 - accuracy: 0.8636\n",
      "Epoch 3043/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5005 - accuracy: 0.8636\n",
      "Epoch 3044/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5005 - accuracy: 0.8636\n",
      "Epoch 3045/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5004 - accuracy: 0.8636\n",
      "Epoch 3046/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5004 - accuracy: 0.8636\n",
      "Epoch 3047/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5004 - accuracy: 0.8636\n",
      "Epoch 3048/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5003 - accuracy: 0.8636\n",
      "Epoch 3049/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5003 - accuracy: 0.8636\n",
      "Epoch 3050/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5003 - accuracy: 0.8636\n",
      "Epoch 3051/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5002 - accuracy: 0.8636\n",
      "Epoch 3052/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5002 - accuracy: 0.8636\n",
      "Epoch 3053/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.5002 - accuracy: 0.8636\n",
      "Epoch 3054/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.5001 - accuracy: 0.8636\n",
      "Epoch 3055/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5001 - accuracy: 0.8636\n",
      "Epoch 3056/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.5001 - accuracy: 0.8636\n",
      "Epoch 3057/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.5000 - accuracy: 0.8636\n",
      "Epoch 3058/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.5000 - accuracy: 0.8636\n",
      "Epoch 3059/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5000 - accuracy: 0.8636\n",
      "Epoch 3060/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4999 - accuracy: 0.8636\n",
      "Epoch 3061/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4999 - accuracy: 0.8636\n",
      "Epoch 3062/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4999 - accuracy: 0.8636\n",
      "Epoch 3063/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4998 - accuracy: 0.8636\n",
      "Epoch 3064/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4998 - accuracy: 0.8636\n",
      "Epoch 3065/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4998 - accuracy: 0.8636\n",
      "Epoch 3066/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4997 - accuracy: 0.8636\n",
      "Epoch 3067/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4997 - accuracy: 0.8636\n",
      "Epoch 3068/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4997 - accuracy: 0.8636\n",
      "Epoch 3069/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4996 - accuracy: 0.8636\n",
      "Epoch 3070/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4996 - accuracy: 0.8636\n",
      "Epoch 3071/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.4996 - accuracy: 0.8636\n",
      "Epoch 3072/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4995 - accuracy: 0.8636\n",
      "Epoch 3073/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4995 - accuracy: 0.8636\n",
      "Epoch 3074/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4994 - accuracy: 0.8636\n",
      "Epoch 3075/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4994 - accuracy: 0.8636\n",
      "Epoch 3076/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4994 - accuracy: 0.8636\n",
      "Epoch 3077/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4993 - accuracy: 0.8636\n",
      "Epoch 3078/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4993 - accuracy: 0.8636\n",
      "Epoch 3079/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4993 - accuracy: 0.8636\n",
      "Epoch 3080/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4992 - accuracy: 0.8636\n",
      "Epoch 3081/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4992 - accuracy: 0.8636\n",
      "Epoch 3082/3500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.4992 - accuracy: 0.8636\n",
      "Epoch 3083/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4991 - accuracy: 0.8636\n",
      "Epoch 3084/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4991 - accuracy: 0.8636\n",
      "Epoch 3085/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4991 - accuracy: 0.8636\n",
      "Epoch 3086/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4990 - accuracy: 0.8636\n",
      "Epoch 3087/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4990 - accuracy: 0.8636\n",
      "Epoch 3088/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4990 - accuracy: 0.8636\n",
      "Epoch 3089/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4989 - accuracy: 0.8636\n",
      "Epoch 3090/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4989 - accuracy: 0.8636\n",
      "Epoch 3091/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4989 - accuracy: 0.8636\n",
      "Epoch 3092/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4988 - accuracy: 0.8636\n",
      "Epoch 3093/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4988 - accuracy: 0.8636\n",
      "Epoch 3094/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4988 - accuracy: 0.8636\n",
      "Epoch 3095/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4987 - accuracy: 0.8636\n",
      "Epoch 3096/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4987 - accuracy: 0.8636\n",
      "Epoch 3097/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4987 - accuracy: 0.8636\n",
      "Epoch 3098/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4986 - accuracy: 0.8636\n",
      "Epoch 3099/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4986 - accuracy: 0.8636\n",
      "Epoch 3100/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4986 - accuracy: 0.8636\n",
      "Epoch 3101/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4985 - accuracy: 0.8636\n",
      "Epoch 3102/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4985 - accuracy: 0.8636\n",
      "Epoch 3103/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4985 - accuracy: 0.8636\n",
      "Epoch 3104/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4984 - accuracy: 0.8636\n",
      "Epoch 3105/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4984 - accuracy: 0.8636\n",
      "Epoch 3106/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.4984 - accuracy: 0.8636\n",
      "Epoch 3107/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4983 - accuracy: 0.8636\n",
      "Epoch 3108/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4983 - accuracy: 0.8636\n",
      "Epoch 3109/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4983 - accuracy: 0.8636\n",
      "Epoch 3110/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4982 - accuracy: 0.8636\n",
      "Epoch 3111/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4982 - accuracy: 0.8636\n",
      "Epoch 3112/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4982 - accuracy: 0.8636\n",
      "Epoch 3113/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4981 - accuracy: 0.8636\n",
      "Epoch 3114/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4981 - accuracy: 0.8636\n",
      "Epoch 3115/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4981 - accuracy: 0.8636\n",
      "Epoch 3116/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4980 - accuracy: 0.8636\n",
      "Epoch 3117/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4980 - accuracy: 0.8636\n",
      "Epoch 3118/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4980 - accuracy: 0.8636\n",
      "Epoch 3119/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4979 - accuracy: 0.8636\n",
      "Epoch 3120/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4979 - accuracy: 0.8636\n",
      "Epoch 3121/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4979 - accuracy: 0.8636\n",
      "Epoch 3122/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4978 - accuracy: 0.8636\n",
      "Epoch 3123/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4978 - accuracy: 0.8636\n",
      "Epoch 3124/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4978 - accuracy: 0.8636\n",
      "Epoch 3125/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4977 - accuracy: 0.8636\n",
      "Epoch 3126/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4977 - accuracy: 0.8636\n",
      "Epoch 3127/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4977 - accuracy: 0.8636\n",
      "Epoch 3128/3500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.4976 - accuracy: 0.8636\n",
      "Epoch 3129/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4976 - accuracy: 0.8636\n",
      "Epoch 3130/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4976 - accuracy: 0.8636\n",
      "Epoch 3131/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4975 - accuracy: 0.8636\n",
      "Epoch 3132/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4975 - accuracy: 0.8636\n",
      "Epoch 3133/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4975 - accuracy: 0.8636\n",
      "Epoch 3134/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4974 - accuracy: 0.8636\n",
      "Epoch 3135/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4974 - accuracy: 0.8636\n",
      "Epoch 3136/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4974 - accuracy: 0.8636\n",
      "Epoch 3137/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4973 - accuracy: 0.8636\n",
      "Epoch 3138/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4973 - accuracy: 0.8636\n",
      "Epoch 3139/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4973 - accuracy: 0.8636\n",
      "Epoch 3140/3500\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.4972 - accuracy: 0.8636\n",
      "Epoch 3141/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4972 - accuracy: 0.8636\n",
      "Epoch 3142/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4972 - accuracy: 0.8636\n",
      "Epoch 3143/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4971 - accuracy: 0.8636\n",
      "Epoch 3144/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4971 - accuracy: 0.8636\n",
      "Epoch 3145/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4971 - accuracy: 0.8636\n",
      "Epoch 3146/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4970 - accuracy: 0.8636\n",
      "Epoch 3147/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.4970 - accuracy: 0.8636\n",
      "Epoch 3148/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4970 - accuracy: 0.8636\n",
      "Epoch 3149/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4969 - accuracy: 0.8636\n",
      "Epoch 3150/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4969 - accuracy: 0.8636\n",
      "Epoch 3151/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4969 - accuracy: 0.8636\n",
      "Epoch 3152/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4968 - accuracy: 0.8636\n",
      "Epoch 3153/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4968 - accuracy: 0.8636\n",
      "Epoch 3154/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.4968 - accuracy: 0.8636\n",
      "Epoch 3155/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4967 - accuracy: 0.8636\n",
      "Epoch 3156/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4967 - accuracy: 0.8636\n",
      "Epoch 3157/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4967 - accuracy: 0.8636\n",
      "Epoch 3158/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4966 - accuracy: 0.8636\n",
      "Epoch 3159/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4966 - accuracy: 0.8636\n",
      "Epoch 3160/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4966 - accuracy: 0.8636\n",
      "Epoch 3161/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4965 - accuracy: 0.8636\n",
      "Epoch 3162/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4965 - accuracy: 0.8636\n",
      "Epoch 3163/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4965 - accuracy: 0.8636\n",
      "Epoch 3164/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4964 - accuracy: 0.8636\n",
      "Epoch 3165/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4964 - accuracy: 0.8636\n",
      "Epoch 3166/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4964 - accuracy: 0.8636\n",
      "Epoch 3167/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4963 - accuracy: 0.8636\n",
      "Epoch 3168/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4963 - accuracy: 0.8636\n",
      "Epoch 3169/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4963 - accuracy: 0.8636\n",
      "Epoch 3170/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4962 - accuracy: 0.8636\n",
      "Epoch 3171/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4962 - accuracy: 0.8636\n",
      "Epoch 3172/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4962 - accuracy: 0.8636\n",
      "Epoch 3173/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4961 - accuracy: 0.8636\n",
      "Epoch 3174/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4961 - accuracy: 0.8636\n",
      "Epoch 3175/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4961 - accuracy: 0.8636\n",
      "Epoch 3176/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4960 - accuracy: 0.8636\n",
      "Epoch 3177/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4960 - accuracy: 0.8636\n",
      "Epoch 3178/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4960 - accuracy: 0.8636\n",
      "Epoch 3179/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4959 - accuracy: 0.8636\n",
      "Epoch 3180/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4959 - accuracy: 0.8636\n",
      "Epoch 3181/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4959 - accuracy: 0.8636\n",
      "Epoch 3182/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4958 - accuracy: 0.8636\n",
      "Epoch 3183/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4958 - accuracy: 0.8636\n",
      "Epoch 3184/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4958 - accuracy: 0.8636\n",
      "Epoch 3185/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4957 - accuracy: 0.8636\n",
      "Epoch 3186/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4957 - accuracy: 0.8636\n",
      "Epoch 3187/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4957 - accuracy: 0.8636\n",
      "Epoch 3188/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4956 - accuracy: 0.8636\n",
      "Epoch 3189/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4956 - accuracy: 0.8636\n",
      "Epoch 3190/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4956 - accuracy: 0.8636\n",
      "Epoch 3191/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4955 - accuracy: 0.8636\n",
      "Epoch 3192/3500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4955 - accuracy: 0.8636\n",
      "Epoch 3193/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4955 - accuracy: 0.8636\n",
      "Epoch 3194/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4954 - accuracy: 0.8636\n",
      "Epoch 3195/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4954 - accuracy: 0.8636\n",
      "Epoch 3196/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4954 - accuracy: 0.8636\n",
      "Epoch 3197/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4953 - accuracy: 0.8636\n",
      "Epoch 3198/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4953 - accuracy: 0.8636\n",
      "Epoch 3199/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4953 - accuracy: 0.8636\n",
      "Epoch 3200/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.4952 - accuracy: 0.8636\n",
      "Epoch 3201/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4952 - accuracy: 0.8636\n",
      "Epoch 3202/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4952 - accuracy: 0.8636\n",
      "Epoch 3203/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4951 - accuracy: 0.8636\n",
      "Epoch 3204/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4951 - accuracy: 0.8636\n",
      "Epoch 3205/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4951 - accuracy: 0.8636\n",
      "Epoch 3206/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4950 - accuracy: 0.8636\n",
      "Epoch 3207/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4950 - accuracy: 0.8636\n",
      "Epoch 3208/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4950 - accuracy: 0.8636\n",
      "Epoch 3209/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4949 - accuracy: 0.8636\n",
      "Epoch 3210/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4949 - accuracy: 0.8636\n",
      "Epoch 3211/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4949 - accuracy: 0.8636\n",
      "Epoch 3212/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4948 - accuracy: 0.8636\n",
      "Epoch 3213/3500\n",
      "22/22 [==============================] - 0s 91us/step - loss: 0.4948 - accuracy: 0.8636\n",
      "Epoch 3214/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4948 - accuracy: 0.8636\n",
      "Epoch 3215/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4948 - accuracy: 0.8636\n",
      "Epoch 3216/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4947 - accuracy: 0.8636\n",
      "Epoch 3217/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4947 - accuracy: 0.8636\n",
      "Epoch 3218/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4947 - accuracy: 0.8636\n",
      "Epoch 3219/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4946 - accuracy: 0.8636\n",
      "Epoch 3220/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4946 - accuracy: 0.8636\n",
      "Epoch 3221/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4946 - accuracy: 0.8636\n",
      "Epoch 3222/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4945 - accuracy: 0.8636\n",
      "Epoch 3223/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4945 - accuracy: 0.8636\n",
      "Epoch 3224/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4945 - accuracy: 0.8636\n",
      "Epoch 3225/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4944 - accuracy: 0.8636\n",
      "Epoch 3226/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4944 - accuracy: 0.8636\n",
      "Epoch 3227/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4944 - accuracy: 0.8636\n",
      "Epoch 3228/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4943 - accuracy: 0.8636\n",
      "Epoch 3229/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4943 - accuracy: 0.8636\n",
      "Epoch 3230/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4943 - accuracy: 0.8636\n",
      "Epoch 3231/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4942 - accuracy: 0.8636\n",
      "Epoch 3232/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4942 - accuracy: 0.8636\n",
      "Epoch 3233/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4942 - accuracy: 0.8636\n",
      "Epoch 3234/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4941 - accuracy: 0.8636\n",
      "Epoch 3235/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4941 - accuracy: 0.8636\n",
      "Epoch 3236/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4941 - accuracy: 0.8636\n",
      "Epoch 3237/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4940 - accuracy: 0.8636\n",
      "Epoch 3238/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4940 - accuracy: 0.8636\n",
      "Epoch 3239/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4940 - accuracy: 0.8636\n",
      "Epoch 3240/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4939 - accuracy: 0.8636\n",
      "Epoch 3241/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4939 - accuracy: 0.8636\n",
      "Epoch 3242/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4939 - accuracy: 0.8636\n",
      "Epoch 3243/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4938 - accuracy: 0.8636\n",
      "Epoch 3244/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4938 - accuracy: 0.8636\n",
      "Epoch 3245/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.4938 - accuracy: 0.8636\n",
      "Epoch 3246/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4937 - accuracy: 0.8636\n",
      "Epoch 3247/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4937 - accuracy: 0.8636\n",
      "Epoch 3248/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4937 - accuracy: 0.8636\n",
      "Epoch 3249/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4936 - accuracy: 0.8636\n",
      "Epoch 3250/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.4936 - accuracy: 0.8636\n",
      "Epoch 3251/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4936 - accuracy: 0.8636\n",
      "Epoch 3252/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4936 - accuracy: 0.8636\n",
      "Epoch 3253/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4935 - accuracy: 0.8636\n",
      "Epoch 3254/3500\n",
      "22/22 [==============================] - 0s 818us/step - loss: 0.4935 - accuracy: 0.8636\n",
      "Epoch 3255/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4935 - accuracy: 0.8636\n",
      "Epoch 3256/3500\n",
      "22/22 [==============================] - 0s 272us/step - loss: 0.4934 - accuracy: 0.8636\n",
      "Epoch 3257/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.4934 - accuracy: 0.8636\n",
      "Epoch 3258/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4934 - accuracy: 0.8636\n",
      "Epoch 3259/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4933 - accuracy: 0.8636\n",
      "Epoch 3260/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4933 - accuracy: 0.8636\n",
      "Epoch 3261/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4933 - accuracy: 0.8636\n",
      "Epoch 3262/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4932 - accuracy: 0.8636\n",
      "Epoch 3263/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4932 - accuracy: 0.8636\n",
      "Epoch 3264/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4932 - accuracy: 0.8636\n",
      "Epoch 3265/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.4931 - accuracy: 0.8636\n",
      "Epoch 3266/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4931 - accuracy: 0.8636\n",
      "Epoch 3267/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4931 - accuracy: 0.8636\n",
      "Epoch 3268/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4930 - accuracy: 0.8636\n",
      "Epoch 3269/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4930 - accuracy: 0.8636\n",
      "Epoch 3270/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4930 - accuracy: 0.8636\n",
      "Epoch 3271/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4929 - accuracy: 0.8636\n",
      "Epoch 3272/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4929 - accuracy: 0.8636\n",
      "Epoch 3273/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4929 - accuracy: 0.8636\n",
      "Epoch 3274/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4928 - accuracy: 0.8636\n",
      "Epoch 3275/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4928 - accuracy: 0.8636\n",
      "Epoch 3276/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4928 - accuracy: 0.8636\n",
      "Epoch 3277/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4927 - accuracy: 0.8636\n",
      "Epoch 3278/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4927 - accuracy: 0.8636\n",
      "Epoch 3279/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4927 - accuracy: 0.8636\n",
      "Epoch 3280/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4926 - accuracy: 0.8636\n",
      "Epoch 3281/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4926 - accuracy: 0.8636\n",
      "Epoch 3282/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4926 - accuracy: 0.8636\n",
      "Epoch 3283/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4926 - accuracy: 0.8636\n",
      "Epoch 3284/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4925 - accuracy: 0.8636\n",
      "Epoch 3285/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4925 - accuracy: 0.8636\n",
      "Epoch 3286/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4925 - accuracy: 0.8636\n",
      "Epoch 3287/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4924 - accuracy: 0.8636\n",
      "Epoch 3288/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4924 - accuracy: 0.8636\n",
      "Epoch 3289/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4924 - accuracy: 0.8636\n",
      "Epoch 3290/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4923 - accuracy: 0.8636\n",
      "Epoch 3291/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4923 - accuracy: 0.8636\n",
      "Epoch 3292/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4923 - accuracy: 0.8636\n",
      "Epoch 3293/3500\n",
      "22/22 [==============================] - 0s 363us/step - loss: 0.4922 - accuracy: 0.8636\n",
      "Epoch 3294/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4922 - accuracy: 0.8636\n",
      "Epoch 3295/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4922 - accuracy: 0.8636\n",
      "Epoch 3296/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4921 - accuracy: 0.8636\n",
      "Epoch 3297/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4921 - accuracy: 0.8636\n",
      "Epoch 3298/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4921 - accuracy: 0.8636\n",
      "Epoch 3299/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4920 - accuracy: 0.8636\n",
      "Epoch 3300/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4920 - accuracy: 0.8636\n",
      "Epoch 3301/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4920 - accuracy: 0.8636\n",
      "Epoch 3302/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4919 - accuracy: 0.8636\n",
      "Epoch 3303/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4919 - accuracy: 0.8636\n",
      "Epoch 3304/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4919 - accuracy: 0.8636\n",
      "Epoch 3305/3500\n",
      "22/22 [==============================] - 0s 1000us/step - loss: 0.4919 - accuracy: 0.8636\n",
      "Epoch 3306/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4918 - accuracy: 0.8636\n",
      "Epoch 3307/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4918 - accuracy: 0.8636\n",
      "Epoch 3308/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4918 - accuracy: 0.8636\n",
      "Epoch 3309/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4917 - accuracy: 0.8636\n",
      "Epoch 3310/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4917 - accuracy: 0.8636\n",
      "Epoch 3311/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4917 - accuracy: 0.8636\n",
      "Epoch 3312/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4916 - accuracy: 0.8636\n",
      "Epoch 3313/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4916 - accuracy: 0.8636\n",
      "Epoch 3314/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4916 - accuracy: 0.8636\n",
      "Epoch 3315/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4915 - accuracy: 0.8636\n",
      "Epoch 3316/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4915 - accuracy: 0.8636\n",
      "Epoch 3317/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4915 - accuracy: 0.8636\n",
      "Epoch 3318/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4914 - accuracy: 0.8636\n",
      "Epoch 3319/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4914 - accuracy: 0.8636\n",
      "Epoch 3320/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4914 - accuracy: 0.8636\n",
      "Epoch 3321/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4913 - accuracy: 0.8636\n",
      "Epoch 3322/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4913 - accuracy: 0.8636\n",
      "Epoch 3323/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4913 - accuracy: 0.8636\n",
      "Epoch 3324/3500\n",
      "22/22 [==============================] - 0s 455us/step - loss: 0.4912 - accuracy: 0.8636\n",
      "Epoch 3325/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4912 - accuracy: 0.8636\n",
      "Epoch 3326/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4912 - accuracy: 0.8636\n",
      "Epoch 3327/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4912 - accuracy: 0.8636\n",
      "Epoch 3328/3500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.4911 - accuracy: 0.8636\n",
      "Epoch 3329/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4911 - accuracy: 0.8636\n",
      "Epoch 3330/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4911 - accuracy: 0.8636\n",
      "Epoch 3331/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4910 - accuracy: 0.8636\n",
      "Epoch 3332/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4910 - accuracy: 0.8636\n",
      "Epoch 3333/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4910 - accuracy: 0.8636\n",
      "Epoch 3334/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4909 - accuracy: 0.8636\n",
      "Epoch 3335/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4909 - accuracy: 0.8636\n",
      "Epoch 3336/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4909 - accuracy: 0.8636\n",
      "Epoch 3337/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.4908 - accuracy: 0.8636\n",
      "Epoch 3338/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4908 - accuracy: 0.8636\n",
      "Epoch 3339/3500\n",
      "22/22 [==============================] - 0s 818us/step - loss: 0.4908 - accuracy: 0.8636\n",
      "Epoch 3340/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4907 - accuracy: 0.8636\n",
      "Epoch 3341/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4907 - accuracy: 0.8636\n",
      "Epoch 3342/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4907 - accuracy: 0.8636\n",
      "Epoch 3343/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4906 - accuracy: 0.8636\n",
      "Epoch 3344/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.4906 - accuracy: 0.8636\n",
      "Epoch 3345/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4906 - accuracy: 0.8636\n",
      "Epoch 3346/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4906 - accuracy: 0.8636\n",
      "Epoch 3347/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4905 - accuracy: 0.8636\n",
      "Epoch 3348/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4905 - accuracy: 0.8636\n",
      "Epoch 3349/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4905 - accuracy: 0.8636\n",
      "Epoch 3350/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4904 - accuracy: 0.8636\n",
      "Epoch 3351/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4904 - accuracy: 0.8636\n",
      "Epoch 3352/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4904 - accuracy: 0.8636\n",
      "Epoch 3353/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4903 - accuracy: 0.8636\n",
      "Epoch 3354/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4903 - accuracy: 0.8636\n",
      "Epoch 3355/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.4903 - accuracy: 0.8636\n",
      "Epoch 3356/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4902 - accuracy: 0.8636\n",
      "Epoch 3357/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4902 - accuracy: 0.8636\n",
      "Epoch 3358/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4902 - accuracy: 0.8636\n",
      "Epoch 3359/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4901 - accuracy: 0.8636\n",
      "Epoch 3360/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4901 - accuracy: 0.8636\n",
      "Epoch 3361/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4901 - accuracy: 0.8636\n",
      "Epoch 3362/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4901 - accuracy: 0.8636\n",
      "Epoch 3363/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4900 - accuracy: 0.8636\n",
      "Epoch 3364/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4900 - accuracy: 0.8636\n",
      "Epoch 3365/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4900 - accuracy: 0.8636\n",
      "Epoch 3366/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4899 - accuracy: 0.8636\n",
      "Epoch 3367/3500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.4899 - accuracy: 0.8636\n",
      "Epoch 3368/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4899 - accuracy: 0.8636\n",
      "Epoch 3369/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4898 - accuracy: 0.8636\n",
      "Epoch 3370/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4898 - accuracy: 0.8636\n",
      "Epoch 3371/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4898 - accuracy: 0.8636\n",
      "Epoch 3372/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4897 - accuracy: 0.8636\n",
      "Epoch 3373/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4897 - accuracy: 0.8636\n",
      "Epoch 3374/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4897 - accuracy: 0.8636\n",
      "Epoch 3375/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4896 - accuracy: 0.8636\n",
      "Epoch 3376/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4896 - accuracy: 0.8636\n",
      "Epoch 3377/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4896 - accuracy: 0.8636\n",
      "Epoch 3378/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4896 - accuracy: 0.8636\n",
      "Epoch 3379/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4895 - accuracy: 0.8636\n",
      "Epoch 3380/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4895 - accuracy: 0.8636\n",
      "Epoch 3381/3500\n",
      "22/22 [==============================] - 0s 149us/step - loss: 0.4895 - accuracy: 0.8636\n",
      "Epoch 3382/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4894 - accuracy: 0.8636\n",
      "Epoch 3383/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4894 - accuracy: 0.8636\n",
      "Epoch 3384/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4894 - accuracy: 0.8636\n",
      "Epoch 3385/3500\n",
      "22/22 [==============================] - 0s 272us/step - loss: 0.4893 - accuracy: 0.8636\n",
      "Epoch 3386/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4893 - accuracy: 0.8636\n",
      "Epoch 3387/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4893 - accuracy: 0.8636\n",
      "Epoch 3388/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4892 - accuracy: 0.8636\n",
      "Epoch 3389/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.4892 - accuracy: 0.8636\n",
      "Epoch 3390/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4892 - accuracy: 0.8636\n",
      "Epoch 3391/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4891 - accuracy: 0.8636\n",
      "Epoch 3392/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4891 - accuracy: 0.8636\n",
      "Epoch 3393/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4891 - accuracy: 0.8636\n",
      "Epoch 3394/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4891 - accuracy: 0.8636\n",
      "Epoch 3395/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4890 - accuracy: 0.8636\n",
      "Epoch 3396/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4890 - accuracy: 0.8636\n",
      "Epoch 3397/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4890 - accuracy: 0.8636\n",
      "Epoch 3398/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4889 - accuracy: 0.8636\n",
      "Epoch 3399/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4889 - accuracy: 0.8636\n",
      "Epoch 3400/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4889 - accuracy: 0.8636\n",
      "Epoch 3401/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4888 - accuracy: 0.8636\n",
      "Epoch 3402/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4888 - accuracy: 0.8636\n",
      "Epoch 3403/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4888 - accuracy: 0.8636\n",
      "Epoch 3404/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4887 - accuracy: 0.8636\n",
      "Epoch 3405/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4887 - accuracy: 0.8636\n",
      "Epoch 3406/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4887 - accuracy: 0.8636\n",
      "Epoch 3407/3500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.4887 - accuracy: 0.8636\n",
      "Epoch 3408/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4886 - accuracy: 0.8636\n",
      "Epoch 3409/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4886 - accuracy: 0.8636\n",
      "Epoch 3410/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4886 - accuracy: 0.8636\n",
      "Epoch 3411/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.4885 - accuracy: 0.8636\n",
      "Epoch 3412/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4885 - accuracy: 0.8636\n",
      "Epoch 3413/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4885 - accuracy: 0.8636\n",
      "Epoch 3414/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4884 - accuracy: 0.8636\n",
      "Epoch 3415/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4884 - accuracy: 0.8636\n",
      "Epoch 3416/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4884 - accuracy: 0.8636\n",
      "Epoch 3417/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4883 - accuracy: 0.8636\n",
      "Epoch 3418/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.4883 - accuracy: 0.8636\n",
      "Epoch 3419/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4883 - accuracy: 0.8636\n",
      "Epoch 3420/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4883 - accuracy: 0.8636\n",
      "Epoch 3421/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4882 - accuracy: 0.8636\n",
      "Epoch 3422/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4882 - accuracy: 0.8636\n",
      "Epoch 3423/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.4882 - accuracy: 0.8636\n",
      "Epoch 3424/3500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.8636\n",
      "Epoch 3425/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4881 - accuracy: 0.8636\n",
      "Epoch 3426/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.4881 - accuracy: 0.8636\n",
      "Epoch 3427/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4880 - accuracy: 0.8636\n",
      "Epoch 3428/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4880 - accuracy: 0.8636\n",
      "Epoch 3429/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4880 - accuracy: 0.8636\n",
      "Epoch 3430/3500\n",
      "22/22 [==============================] - 0s 364us/step - loss: 0.4879 - accuracy: 0.8636\n",
      "Epoch 3431/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4879 - accuracy: 0.8636\n",
      "Epoch 3432/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4879 - accuracy: 0.8636\n",
      "Epoch 3433/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4879 - accuracy: 0.8636\n",
      "Epoch 3434/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4878 - accuracy: 0.8636\n",
      "Epoch 3435/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4878 - accuracy: 0.8636\n",
      "Epoch 3436/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4878 - accuracy: 0.8636\n",
      "Epoch 3437/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4877 - accuracy: 0.8636\n",
      "Epoch 3438/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4877 - accuracy: 0.8636\n",
      "Epoch 3439/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4877 - accuracy: 0.8636\n",
      "Epoch 3440/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4876 - accuracy: 0.8636\n",
      "Epoch 3441/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4876 - accuracy: 0.8636\n",
      "Epoch 3442/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4876 - accuracy: 0.8636\n",
      "Epoch 3443/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4875 - accuracy: 0.8636\n",
      "Epoch 3444/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4875 - accuracy: 0.8636\n",
      "Epoch 3445/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4875 - accuracy: 0.8636\n",
      "Epoch 3446/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4875 - accuracy: 0.8636\n",
      "Epoch 3447/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4874 - accuracy: 0.8636\n",
      "Epoch 3448/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4874 - accuracy: 0.8636\n",
      "Epoch 3449/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4874 - accuracy: 0.8636\n",
      "Epoch 3450/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4873 - accuracy: 0.8636\n",
      "Epoch 3451/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4873 - accuracy: 0.8636\n",
      "Epoch 3452/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4873 - accuracy: 0.8636\n",
      "Epoch 3453/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4872 - accuracy: 0.8636\n",
      "Epoch 3454/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4872 - accuracy: 0.8636\n",
      "Epoch 3455/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4872 - accuracy: 0.8636\n",
      "Epoch 3456/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4872 - accuracy: 0.8636\n",
      "Epoch 3457/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.4871 - accuracy: 0.8636\n",
      "Epoch 3458/3500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.4871 - accuracy: 0.8636\n",
      "Epoch 3459/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4871 - accuracy: 0.8636\n",
      "Epoch 3460/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4870 - accuracy: 0.8636\n",
      "Epoch 3461/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4870 - accuracy: 0.8636\n",
      "Epoch 3462/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.4870 - accuracy: 0.8636\n",
      "Epoch 3463/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4869 - accuracy: 0.8636\n",
      "Epoch 3464/3500\n",
      "22/22 [==============================] - 0s 409us/step - loss: 0.4869 - accuracy: 0.8636\n",
      "Epoch 3465/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4869 - accuracy: 0.8636\n",
      "Epoch 3466/3500\n",
      "22/22 [==============================] - 0s 227us/step - loss: 0.4868 - accuracy: 0.8636\n",
      "Epoch 3467/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.4868 - accuracy: 0.8636\n",
      "Epoch 3468/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4868 - accuracy: 0.8636\n",
      "Epoch 3469/3500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.4868 - accuracy: 0.8636\n",
      "Epoch 3470/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4867 - accuracy: 0.8636\n",
      "Epoch 3471/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4867 - accuracy: 0.8636\n",
      "Epoch 3472/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.4867 - accuracy: 0.8636\n",
      "Epoch 3473/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4866 - accuracy: 0.8636\n",
      "Epoch 3474/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4866 - accuracy: 0.8636\n",
      "Epoch 3475/3500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4866 - accuracy: 0.8636\n",
      "Epoch 3476/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4865 - accuracy: 0.8636\n",
      "Epoch 3477/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4865 - accuracy: 0.8636\n",
      "Epoch 3478/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4865 - accuracy: 0.8636\n",
      "Epoch 3479/3500\n",
      "22/22 [==============================] - 0s 137us/step - loss: 0.4865 - accuracy: 0.8636\n",
      "Epoch 3480/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4864 - accuracy: 0.8636\n",
      "Epoch 3481/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4864 - accuracy: 0.8636\n",
      "Epoch 3482/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4864 - accuracy: 0.8636\n",
      "Epoch 3483/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4863 - accuracy: 0.8636\n",
      "Epoch 3484/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4863 - accuracy: 0.8636\n",
      "Epoch 3485/3500\n",
      "22/22 [==============================] - 0s 273us/step - loss: 0.4863 - accuracy: 0.8636\n",
      "Epoch 3486/3500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.4862 - accuracy: 0.8636\n",
      "Epoch 3487/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4862 - accuracy: 0.8636\n",
      "Epoch 3488/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4862 - accuracy: 0.8636\n",
      "Epoch 3489/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4861 - accuracy: 0.8636\n",
      "Epoch 3490/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4861 - accuracy: 0.8636\n",
      "Epoch 3491/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4861 - accuracy: 0.8636\n",
      "Epoch 3492/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4861 - accuracy: 0.8636\n",
      "Epoch 3493/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4860 - accuracy: 0.8636\n",
      "Epoch 3494/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4860 - accuracy: 0.8636\n",
      "Epoch 3495/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4860 - accuracy: 0.8636\n",
      "Epoch 3496/3500\n",
      "22/22 [==============================] - 0s 318us/step - loss: 0.4859 - accuracy: 0.8636\n",
      "Epoch 3497/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4859 - accuracy: 0.8636\n",
      "Epoch 3498/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4859 - accuracy: 0.8636\n",
      "Epoch 3499/3500\n",
      "22/22 [==============================] - 0s 136us/step - loss: 0.4858 - accuracy: 0.8636\n",
      "Epoch 3500/3500\n",
      "22/22 [==============================] - 0s 182us/step - loss: 0.4858 - accuracy: 0.8636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x23af8151f08>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs= 3500) # increased epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 38ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4519183933734894, 1.0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:[3.861938], W2:[1.0980022], bias:[-2.1415591]\n"
     ]
    }
   ],
   "source": [
    "# weights of age, affordibility and bias after 3500 epochs\n",
    "weights, bias= model.get_weights() \n",
    "print(f'W1:{weights[0]}, W2:{weights[1]}, bias:{bias}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>affordibility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  affordibility\n",
       "24  0.50              1\n",
       "17  0.58              1\n",
       "19  0.18              1\n",
       "20  0.21              1\n",
       "14  0.49              1\n",
       "3   0.52              0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7083558 ],\n",
       "       [0.7678779 ],\n",
       "       [0.41376856],\n",
       "       [0.44212312],\n",
       "       [0.7003138 ],\n",
       "       [0.4667115 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yp= model.predict(x_test)\n",
    "yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24    1\n",
       "17    1\n",
       "19    0\n",
       "20    0\n",
       "14    1\n",
       "3     0\n",
       "Name: bought_insurance, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instead of model.predict, wrote our own prediction function that uses w1,w2 and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_func(x1, x2):\n",
    "    weighted_sum= weights[0]*x1 + weights[1]*x2 + bias  # Used weights from above model.\n",
    "    return sigmoid(weighted_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4667114835204448"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_func(0.52,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemented Gradient Descent from scratch\n",
    "The goal is to come up with same w1, w2 and bias that keras model calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigoid func for arrays.\n",
    "\n",
    "def sigmoid_np(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+00, 4.65888615e-15, 9.52574127e-01])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_np(np.array([42,-33,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implemented Log Loss or Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce(y_true, y_predicted):\n",
    "    epsilon= 1e-15\n",
    "    error= 0  \n",
    "    y_predicted_new= [ max(i, epsilon) for i in y_predicted]\n",
    "    y_predicted_new= [ min(i, 1-epsilon) for i in y_predicted_new]  \n",
    "    y_predicted_new= np.array(y_predicted_new)\n",
    "    bce= -np.mean(y_true * np.log(y_predicted_new) +(1-y_true) * np.log(1-y_predicted_new))        \n",
    "    return bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x1,x2,y_true,epochs,loss_threshold):\n",
    "    \n",
    "    w1=w2=1\n",
    "    bias=0\n",
    "    learning_rate= 0.1\n",
    "    n= len(x1)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        weighted_sum= w1*x1 + w2*x2 + bias \n",
    "        y_predicted= sigmoid_np(weighted_sum)\n",
    "        loss= bce(y_true, y_predicted)\n",
    "    \n",
    "        w1d= (1/n)*(np.dot(np.transpose(x1),(y_predicted-y_true)))\n",
    "        w2d= (1/n)*(np.dot(np.transpose(x2),(y_predicted-y_true)))\n",
    "        bias_d= np.mean(y_predicted-y_true)\n",
    "    \n",
    "        w1= w1-learning_rate*w1d\n",
    "        w2= w2-learning_rate*w2d\n",
    "        bias= bias-learning_rate*bias_d\n",
    "        \n",
    "        if loss <= loss_threshold: # as need to compare weights and bias with tensorflow model.\n",
    "            break\n",
    "    \n",
    "        print(f'W1:{w1}, W2:{w2}, bias:{bias}, loss:{loss}')\n",
    "    return w1, w2, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:0.9955324184480593, W2:0.9896158041440797, bias:-0.022453413341607935, loss:0.7067871004847728\n",
      "W1:0.9912814042232098, W2:0.9795794051967283, bias:-0.044348014675807935, loss:0.7005530386735819\n",
      "W1:0.9872442107768984, W2:0.9698878053452621, bias:-0.06569161003700413, loss:0.6946532581994433\n",
      "W1:0.9834178723446488, W2:0.9605375326271981, bias:-0.08649251207608602, loss:0.68907402868994\n",
      "W1:0.9797992216041957, W2:0.9515246711861695, bias:-0.10675949473661692, loss:0.6838016738344731\n",
      "W1:0.9763849076033503, W2:0.9428448927540957, bias:-0.1265017476649895, loss:0.6788226288002475\n",
      "W1:0.9731714137777066, W2:0.9344934889653662, bias:-0.145728830775256, loss:0.6741234928228154\n",
      "W1:0.9701550758915072, W2:0.9264654041310006, bias:-0.16445062935380161, loss:0.6696910767742831\n",
      "W1:0.9673320997498964, W2:0.9187552681276246, bias:-0.1826773100501186, loss:0.665512445608952\n",
      "W1:0.9646985785468727, W2:0.9113574290864684, bias:-0.20041927805891058, loss:0.6615749556744612\n",
      "W1:0.9622505097300104, W2:0.9042659856003015, bias:-0.21768713575677648, loss:0.6578662869551205\n",
      "W1:0.9599838112799918, W2:0.8974748182002261, bias:-0.23449164301482792, loss:0.6543744703823018\n",
      "W1:0.9578943373197883, W2:0.890977619888616, bias:-0.2508436793676769, loss:0.6510879104041182\n",
      "W1:0.9559778929846053, W2:0.8847679255483945, bias:-0.2667542081800412, loss:0.6479954030532447\n",
      "W1:0.9542302484991885, W2:0.8788391400816244, bias:-0.28223424291534327, loss:0.6450861497879311\n",
      "W1:0.9526471524235665, W2:0.8731845651614822, bias:-0.2972948155765678, loss:0.6423497674076942\n",
      "W1:0.9512243440416174, W2:0.8677974245106963, bias:-0.3119469473586085, loss:0.6397762943626163\n",
      "W1:0.9499575648788919, W2:0.8626708876461587, bias:-0.3262016215235495, loss:0.6373561937845964\n",
      "W1:0.9488425693468534, W2:0.8577980920534767, bias:-0.34006975848587234, loss:0.6350803535712622\n",
      "W1:0.947875134520087, W2:0.8531721637766271, bias:-0.35356219307343734, loss:0.6329400838496295\n",
      "W1:0.9470510690611077, W2:0.8487862364266107, bias:-0.3666896539121595, loss:0.6309271121379765\n",
      "W1:0.9463662213142072, W2:0.8446334686291193, bias:-0.3794627448674405, loss:0.6290335765117872\n",
      "W1:0.945816486595394, W2:0.840707059944841, bias:-0.3918919284634228, loss:0.6272520170638863\n",
      "W1:0.9453978137099798, W2:0.8370002653072853, bias:-0.40398751119178117, loss:0.6255753659309025\n",
      "W1:0.9451062107328458, W2:0.8335064080320772, bias:-0.4157596306148095, loss:0.6239969361386465\n",
      "W1:0.9449377500889806, W2:0.8302188914587465, bias:-0.4272182441627424, loss:0.6225104094985537\n",
      "W1:0.9448885729736235, W2:0.8271312092913253, bias:-0.4383731195223198, loss:0.6211098237665358\n",
      "W1:0.9449548931523717, W2:0.8242369547077549, bias:-0.4492338265123039, loss:0.6197895592548668\n",
      "W1:0.9451330001820116, W2:0.8215298283104084, bias:-0.45980973034176037, loss:0.6185443250674733\n",
      "W1:0.9454192620927132, W2:0.8190036449911346, bias:-0.47010998614818705, loss:0.6173691451094742\n",
      "W1:0.9458101275716597, W2:0.8166523397843052, bias:-0.4801435347148206, loss:0.6162593440032588\n",
      "W1:0.9463021276872591, W2:0.8144699727805728, bias:-0.48991909926946975, loss:0.6152105330259311\n",
      "W1:0.9468918771918643, W2:0.8124507331725682, bias:-0.49944518327086496, loss:0.6142185961667241\n",
      "W1:0.9475760754394863, W2:0.8105889425017189, bias:-0.5087300690926091, loss:0.613279676388014\n",
      "W1:0.9483515069533681, W2:0.8088790571728857, bias:-0.5177818175192503, loss:0.6123901621598904\n",
      "W1:0.9492150416765515, W2:0.8073156703006912, bias:-0.5266082679736495, loss:0.6115466743258607\n",
      "W1:0.9501636349367546, W2:0.8058935129483497, bias:-0.5352170393995929, loss:0.6107460533461112\n",
      "W1:0.9511943271550201, W2:0.8046074548165814, bias:-0.5436155317284241, loss:0.6099853469548332\n",
      "W1:0.9523042433257233, W2:0.8034525044368783, bias:-0.5518109278632667, loss:0.609261798259311\n",
      "W1:0.9534905922936734, W2:0.8024238089200325, bias:-0.5598101961191309, loss:0.6085728343007529\n",
      "W1:0.9547506658522125, W2:0.8015166533074963, bias:-0.5676200930618011, loss:0.60791605509011\n",
      "W1:0.9560818376844447, W2:0.8007264595698526, bias:-0.5752471666928424, loss:0.6072892231263133\n",
      "W1:0.9574815621680096, W2:0.8000487852934597, bias:-0.5826977599323344, loss:0.6066902533993669\n",
      "W1:0.9589473730621696, W2:0.7994793220932321, bias:-0.5899780143550051, loss:0.6061172038765167\n",
      "W1:0.9604768820944141, W2:0.7990138937865291, bias:-0.5970938741392942, loss:0.605568266466159\n",
      "W1:0.9620677774622979, W2:0.7986484543602792, bias:-0.604051090192515, loss:0.605041758451218\n",
      "W1:0.9637178222648283, W2:0.7983790857607608, bias:-0.6108552244186981, loss:0.604536114381318\n",
      "W1:0.9654248528763988, W2:0.798201995532906, bias:-0.6175116540988969, loss:0.6040498784111586\n",
      "W1:0.9671867772750381, W2:0.7981135143335915, bias:-0.6240255763567145, loss:0.6035816970709921\n",
      "W1:0.9690015733355918, W2:0.7981100933411259, bias:-0.630402012684577, loss:0.6031303124539655\n",
      "W1:0.9708672870973896, W2:0.7981883015810478, bias:-0.6366458135088361, loss:0.602694555804264\n",
      "W1:0.972782031014963, W2:0.7983448231863851, bias:-0.642761662774153, loss:0.6022733414894416\n",
      "W1:0.9747439821994649, W2:0.7985764546087194, bias:-0.6487540825297856, loss:0.6018656613399975\n",
      "W1:0.9767513806576034, W2:0.7988801017947167, bias:-0.6546274375024018, loss:0.6014705793391404\n",
      "W1:0.9788025275341288, W2:0.7992527773412409, bias:-0.6603859396418706, loss:0.6010872266457128\n",
      "W1:0.9808957833632072, W2:0.7996915976407396, bias:-0.6660336526281522, loss:0.600714796933427\n",
      "W1:0.9830295663333648, W2:0.8001937800272857, bias:-0.6715744963289327, loss:0.6003525420298502\n",
      "W1:0.9852023505700982, W2:0.8007566399324589, bias:-0.6770122511990349, loss:0.5999997678389557\n",
      "W1:0.9874126644397085, W2:0.8013775880591578, bias:-0.682350562613891, loss:0.5996558305314995\n",
      "W1:0.9896590888774282, W2:0.802054127580431, bias:-0.6875929451305055, loss:0.5993201329879949\n",
      "W1:0.9919402557424675, W2:0.8027838513695098, bias:-0.6927427866703587, loss:0.5989921214795971\n",
      "W1:0.9942548462022088, W2:0.8035644392663954, bias:-0.6978033526196356, loss:0.5986712825727863\n",
      "W1:0.9966015891474117, W2:0.8043936553856061, bias:-0.7027777898429903, loss:0.5983571402443361\n",
      "W1:0.9989792596399705, W2:0.8052693454690106, bias:-0.7076691306078119, loss:0.5980492531936519\n",
      "W1:1.001386677394472, W2:0.8061894342870602, bias:-0.71248029641662, loss:0.5977472123401875\n",
      "W1:1.0038227052945379, W2:0.8071519230911831, bias:-0.7172141017458225, loss:0.5974506384942353\n",
      "W1:1.0062862479447043, W2:0.8081548871196029, bias:-0.7218732576895941, loss:0.597159180190005\n",
      "W1:1.0087762502583788, W2:0.8091964731584016, bias:-0.7264603755081132, loss:0.5968725116704839\n",
      "W1:1.0112916960822316, W2:0.810274897159244, bias:-0.7309779700798051, loss:0.5965903310141454\n",
      "W1:1.0138316068572117, W2:0.8113884419148272, bias:-0.7354284632576173, loss:0.5963123583941372\n",
      "W1:1.0163950403162336, W2:0.8125354547927977, bias:-0.7398141871296675, loss:0.5960383344611184\n",
      "W1:1.0189810892184488, W2:0.8137143455285973, bias:-0.7441373871848961, loss:0.5957680188414216\n",
      "W1:1.0215888801199084, W2:0.8149235840774487, bias:-0.748400225384597, loss:0.5955011887427373\n",
      "W1:1.0242175721803208, W2:0.8161616985254684, bias:-0.7526047831409169, loss:0.5952376376599718\n",
      "W1:1.0268663560055225, W2:0.8174272730597038, bias:-0.7567530642035942, loss:0.5949771741743983\n",
      "W1:1.0295344525252108, W2:0.8187189459967146, bias:-0.7608469974563699, loss:0.5947196208396496\n",
      "W1:1.0322211119054192, W2:0.8200354078691763, bias:-0.7648884396246312, loss:0.5944648131485072\n",
      "W1:1.0349256124951618, W2:0.8213753995698486, bias:-0.7688791778959633, loss:0.5942125985748425\n",
      "W1:1.0376472598066329, W2:0.8227377105521444, bias:-0.7728209324553742, loss:0.59396283568542\n",
      "W1:1.0403853855283043, W2:0.8241211770864358, bias:-0.7767153589370336, loss:0.593715393316631\n",
      "W1:1.0431393465702348, W2:0.8255246805711559, bias:-0.7805640507944237, loss:0.5934701498115508\n",
      "W1:1.0459085241408805, W2:0.826947145897684, bias:-0.7843685415908488, loss:0.5932269923130182\n",
      "W1:1.048692322854677, W2:0.8283875398679479, bias:-0.788130307212278, loss:0.5929858161087341\n",
      "W1:1.0514901698696448, W2:0.8298448696636279, bias:-0.7918507680045228, loss:0.59274652402464\n",
      "W1:1.0543015140542673, W2:0.8313181813658139, bias:-0.7955312908367611, loss:0.5925090258631008\n",
      "W1:1.0571258251828735, W2:0.8328065585239341, bias:-0.7991731910934224, loss:0.592273237882655\n",
      "W1:1.0599625931587655, W2:0.8343091207727575, bias:-0.8027777345964485, loss:0.5920390823163121\n",
      "W1:1.0628113272643227, W2:0.8358250224962528, bias:-0.8063461394599319, loss:0.5918064869256038\n",
      "W1:1.065671555437319, W2:0.8373534515370818, bias:-0.8098795778791194, loss:0.5915753845877745\n",
      "W1:1.0685428235726981, W2:0.8388936279504997, bias:-0.8133791778557469, loss:0.5913457129136934\n",
      "W1:1.0714246948490516, W2:0.8404448028014339, bias:-0.8168460248616505, loss:0.5911174138942358\n",
      "W1:1.0743167490790593, W2:0.8420062570035226, bias:-0.8202811634425647, loss:0.5908904335730452\n",
      "W1:1.077218582083158, W2:0.8435773001988984, bias:-0.8236855987639929, loss:0.5906647217437307\n",
      "W1:1.0801298050857155, W2:0.8451572696775143, bias:-0.8270602981009981, loss:0.5904402316697039\n",
      "W1:1.0830500441330027, W2:0.8467455293348255, bias:-0.830406192273727, loss:0.5902169198249786\n",
      "W1:1.085978939532263, W2:0.8483414686666553, bias:-0.8337241770304432, loss:0.5899947456543836\n",
      "W1:1.0889161453112015, W2:0.8499445018000901, bias:-0.837015114379804, loss:0.5897736713517505\n",
      "W1:1.0918613286972207, W2:0.8515540665592717, bias:-0.8402798338740785, loss:0.5895536616547404\n",
      "W1:1.0948141696157532, W2:0.8531696235649734, bias:-0.8435191338449589, loss:0.5893346836550738\n",
      "W1:1.0977743602070502, W2:0.8547906553668693, bias:-0.8467337825935808, loss:0.5891167066230163\n",
      "W1:1.100741604360807, W2:0.8564166656074311, bias:-0.8499245195363202, loss:0.5888997018450574\n",
      "W1:1.103715617268015, W2:0.8580471782164082, bias:-0.8530920563078978, loss:0.5886836424737985\n",
      "W1:1.1066961249894538, W2:0.859681736634873, bias:-0.8562370778232757, loss:0.5884685033891355\n",
      "W1:1.1096828640402452, W2:0.8613199030678381, bias:-0.8593602432997913, loss:0.5882542610698935\n",
      "W1:1.1126755809899116, W2:0.8629612577644766, bias:-0.8624621872409289, loss:0.5880408934751294\n",
      "W1:1.115674032077397, W2:0.8646053983250028, bias:-0.8655435203830919, loss:0.5878283799343789\n",
      "W1:1.118677982840523, W2:0.8662519390332958, bias:-0.8686048306066931, loss:0.5876167010461743\n",
      "W1:1.1216872077593674, W2:0.8679005102143734, bias:-0.8716466838128437, loss:0.587405838584215\n",
      "W1:1.1247014899130694, W2:0.8695507576158504, bias:-0.8746696247668789, loss:0.5871957754106143\n",
      "W1:1.127720620649584, W2:0.8712023418125384, bias:-0.8776741779099213, loss:0.5869864953956889\n",
      "W1:1.1307443992679154, W2:0.872854937633371, bias:-0.8806608481396428, loss:0.5867779833438012\n",
      "W1:1.1337726327123803, W2:0.8745082336098617, bias:-0.8836301215613493, loss:0.586570224924795\n",
      "W1:1.1368051352784652, W2:0.8761619314453273, bias:-0.8865824662104749, loss:0.586363206610609\n",
      "W1:1.1398417283298545, W2:0.8778157455041307, bias:-0.8895183327475362, loss:0.5861569156166696\n",
      "W1:1.1428822400262209, W2:0.8794694023202243, bias:-0.892438155126564, loss:0.5859513398477089\n",
      "W1:1.1459265050613823, W2:0.8811226401242951, bias:-0.895342351237991, loss:0.5857464678476704\n",
      "W1:1.1489743644114456, W2:0.882775208388836, bias:-0.8982313235269462, loss:0.5855422887533965\n",
      "W1:1.152025665092566, W2:0.884426867390491, bias:-0.9011054595878697, loss:0.5853387922518082\n",
      "W1:1.1550802599279668, W2:0.8860773877890414, bias:-0.903965132736331, loss:0.5851359685403148\n",
      "W1:1.1581380073238754, W2:0.8877265502224236, bias:-0.906810702558906, loss:0.5849338082902128\n",
      "W1:1.1611987710540432, W2:0.8893741449171874, bias:-0.9096425154419328, loss:0.5847323026128405\n",
      "W1:1.164262420052528, W2:0.8910199713138242, bias:-0.9124609050799418, loss:0.5845314430282859\n",
      "W1:1.1673288282144314, W2:0.8926638377064166, bias:-0.9152661929645257, loss:0.5843312214364534\n",
      "W1:1.1703978742042898, W2:0.8943055608960749, bias:-0.9180586888543858, loss:0.5841316300903078\n",
      "W1:1.173469441271834, W2:0.8959449658576493, bias:-0.9208386912272695, loss:0.5839326615711344\n",
      "W1:1.1765434170748381, W2:0.8975818854192209, bias:-0.9236064877144822, loss:0.5837343087656595\n",
      "W1:1.1796196935087893, W2:0.8992161599538949, bias:-0.9263623555186364, loss:0.5835365648448925\n",
      "W1:1.1826981665431227, W2:0.9008476370834336, bias:-0.9291065618152752, loss:0.5833394232445589\n",
      "W1:1.1857787360637704, W2:0.9024761713932852, bias:-0.9318393641389826, loss:0.5831428776470005\n",
      "W1:1.1888613057217863, W2:0.9041016241585788, bias:-0.9345610107545745, loss:0.5829469219644371\n",
      "W1:1.1919457827878146, W2:0.9057238630806732, bias:-0.9372717410139378, loss:0.5827515503234811\n",
      "W1:1.1950320780121801, W2:0.9073427620338596, bias:-0.9399717856990673, loss:0.5825567570508133\n",
      "W1:1.198120105490387, W2:0.908958200821834, bias:-0.942661367351828, loss:0.5823625366599313\n",
      "W1:1.2012097825338166, W2:0.9105700649435705, bias:-0.9453407005909537, loss:0.5821688838388874\n",
      "W1:1.2043010295454293, W2:0.9121782453682371, bias:-0.9480099924167669, loss:0.581975793438943\n",
      "W1:1.2073937699002752, W2:0.9137826383188102, bias:-0.9506694425040975, loss:0.5817832604640709\n",
      "W1:1.2104879298306326, W2:0.9153831450640585, bias:-0.9533192434838489, loss:0.5815912800612361\n",
      "W1:1.2135834383155941, W2:0.9169796717185749, bias:-0.9559595812136511, loss:0.5813998475114027\n",
      "W1:1.2166802269749317, W2:0.918572129050551, bias:-0.9585906350380203, loss:0.5812089582212076\n",
      "W1:1.2197782299670745, W2:0.9201604322969977, bias:-0.9612125780384295, loss:0.581018607715251\n",
      "W1:1.2228773838910425, W2:0.9217445009861277, bias:-0.963825577273679, loss:0.5808287916289585\n",
      "W1:1.225977627692183, W2:0.9233242587666237, bias:-0.966429794010942, loss:0.5806395057019705\n",
      "W1:1.2290789025715634, W2:0.9248996332435317, bias:-0.9690253839478444, loss:0.580450745772017\n",
      "W1:1.2321811518988794, W2:0.9264705558205223, bias:-0.9716124974259271, loss:0.5802625077692465\n",
      "W1:1.2352843211287439, W2:0.9280369615482766, bias:-0.9741912796358223, loss:0.5800747877109678\n",
      "W1:1.2383883577202233, W2:0.9295987889787619, bias:-0.9767618708144652, loss:0.5798875816967779\n",
      "W1:1.2414932110594994, W2:0.9311559800251692, bias:-0.9793244064346504, loss:0.5797008859040461\n",
      "W1:1.2445988323855324, W2:0.9327084798272963, bias:-0.9818790173872283, loss:0.5795146965837267\n",
      "W1:1.2477051747186123, W2:0.9342562366221654, bias:-0.9844258301562276, loss:0.579329010056474\n",
      "W1:1.250812192791684, W2:0.9357992016196736, bias:-0.9869649669871782, loss:0.5791438227090404\n",
      "W1:1.2539198429843401, W2:0.937337328883083, bias:-0.9894965460488974, loss:0.5789591309909329\n",
      "W1:1.2570280832593783, W2:0.9388705752141628, bias:-0.9920206815889951, loss:0.5787749314113095\n",
      "W1:1.260136873101823, W2:0.9403989000428042, bias:-0.99453748408334, loss:0.5785912205360987\n",
      "W1:1.2632461734603158, W2:0.9419222653209346, bias:-0.9970470603797216, loss:0.5784079949853221\n",
      "W1:1.2663559466907834, W2:0.9434406354205656, bias:-0.9995495138359353, loss:0.5782252514306093\n",
      "W1:1.269466156502295, W2:0.9449539770358152, bias:-1.002044944452505, loss:0.5780429865928857\n",
      "W1:1.272576767905022, W2:0.946462259088749, bias:-1.0045334490002535, loss:0.5778611972402237\n",
      "W1:1.2756877471602204, W2:0.947965452638894, bias:-1.00701512114292, loss:0.5776798801858439\n",
      "W1:1.2787990617321578, W2:0.9494635307962819, bias:-1.0094900515550171, loss:0.5774990322862551\n",
      "W1:1.2819106802419065, W2:0.9509564686378857, bias:-1.0119583280351157, loss:0.5773186504395202\n",
      "W1:1.2850225724229327, W2:0.9524442431273176, bias:-1.0144200356147293, loss:0.5771387315836453\n",
      "W1:1.2881347090784119, W2:0.9539268330376615, bias:-1.0168752566629766, loss:0.5769592726950733\n",
      "W1:1.291247062040202, W2:0.9554042188773201, bias:-1.019324070987181, loss:0.5767802707872829\n",
      "W1:1.2943596041294094, W2:0.9568763828187576, bias:-1.0217665559295674, loss:0.5766017229094799\n",
      "W1:1.29747230911849, W2:0.9583433086300267, bias:-1.0242027864602097, loss:0.5764236261453742\n",
      "W1:1.3005851516948188, W2:0.9598049816089714, bias:-1.026632835266371, loss:0.5762459776120398\n",
      "W1:1.3036981074256762, W2:0.9612613885200026, bias:-1.0290567728383813, loss:0.5760687744588466\n",
      "W1:1.306811152724595, W2:0.9627125175333455, bias:-1.031474667552186, loss:0.5758920138664629\n",
      "W1:1.3099242648190126, W2:0.964158358166663, bias:-1.0338865857486932, loss:0.57571569304592\n",
      "W1:1.3130374217191807, W2:0.9655989012289627, bias:-1.0362925918100472, loss:0.5755398092377372\n",
      "W1:1.3161506021882807, W2:0.9670341387667005, bias:-1.0386927482329462, loss:0.5753643597110997\n",
      "W1:1.3192637857137008, W2:0.9684640640119916, bias:-1.0410871156991204, loss:0.575189341763089\n",
      "W1:1.3223769524794267, W2:0.9698886713328514, bias:-1.0434757531430803, loss:0.5750147527179587\n",
      "W1:1.3254900833395045, W2:0.9713079561853838, bias:-1.045858717817242, loss:0.5748405899264531\n",
      "W1:1.3286031597925332, W2:0.9727219150678436, bias:-1.0482360653545324, loss:0.574666850765169\n",
      "W1:1.3317161639571473, W2:0.9741305454764981, bias:-1.0506078498285707, loss:0.5744935326359496\n",
      "W1:1.3348290785484493, W2:0.9755338458632203, bias:-1.0529741238115222, loss:0.5743206329653181\n",
      "W1:1.3379418868553574, W2:0.9769318155947435, bias:-1.0553349384297153, loss:0.5741481492039402\n",
      "W1:1.3410545727188317, W2:0.978324454913515, bias:-1.057690343417107, loss:0.5739760788261168\n",
      "W1:1.3441671205109436, W2:0.9797117649000858, bias:-1.0600403871666828, loss:0.5738044193293036\n",
      "W1:1.3472795151147572, W2:0.9810937474369761, bias:-1.0623851167798706, loss:0.5736331682336586\n",
      "W1:1.3503917419049907, W2:0.9824704051739598, bias:-1.0647245781140453, loss:0.5734623230816103\n",
      "W1:1.3535037867294266, W2:0.9838417414947128, bias:-1.0670588158282, loss:0.573291881437449\n",
      "W1:1.3566156358910424, W2:0.9852077604847717, bias:-1.0693878734268536, loss:0.5731218408869386\n",
      "W1:1.3597272761308332, W2:0.986568466900751, bias:-1.0717117933022655, loss:0.5729521990369484\n",
      "W1:1.3628386946112998, W2:0.9879238661407725, bias:-1.0740306167750204, loss:0.5727829535150986\n",
      "W1:1.3659498789005757, W2:0.989273964216057, bias:-1.0763443841330498, loss:0.5726141019694279\n",
      "W1:1.3690608169571685, W2:0.9906187677236331, bias:-1.078653134669149, loss:0.5724456420680702\n",
      "W1:1.3721714971152918, W2:0.9919582838201231, bias:-1.0809569067170495, loss:0.5722775714989511\n",
      "W1:1.375281908070765, W2:0.9932925201965586, bias:-1.0832557376861012, loss:0.5721098879694921\n",
      "W1:1.3783920388674573, W2:0.9946214850541907, bias:-1.0855496640946214, loss:0.5719425892063323\n",
      "W1:1.3815018788842572, W2:0.9959451870812532, bias:-1.0878387216019592, loss:0.5717756729550564\n",
      "W1:1.384611417822545, W2:0.9972636354306426, bias:-1.0901229450393277, loss:0.5716091369799373\n",
      "W1:1.387720645694149, W2:0.9985768396984784, bias:-1.092402368439451, loss:0.5714429790636868\n",
      "W1:1.3908295528097683, W2:0.9998848099035117, bias:-1.0946770250650726, loss:0.5712771970072149\n",
      "W1:1.3939381297678404, W2:1.0011875564673456, bias:-1.0969469474363684, loss:0.5711117886294\n",
      "W1:1.3970463674438416, W2:1.0024850901954392, bias:-1.099212167357308, loss:0.5709467517668644\n",
      "W1:1.400154256979998, W2:1.0037774222588625, bias:-1.1014727159410058, loss:0.5707820842737583\n",
      "W1:1.4032617897753954, W2:1.0050645641767728, bias:-1.103728623634098, loss:0.570617784021553\n",
      "W1:1.406368957476468, W2:1.0063465277995876, bias:-1.1059799202401879, loss:0.5704538488988363\n",
      "W1:1.4094757519678573, W2:1.0076233252928215, bias:-1.1082266349423924, loss:0.5702902768111181\n",
      "W1:1.4125821653636206, W2:1.0088949691215672, bias:-1.1104687963250253, loss:0.5701270656806394\n",
      "W1:1.4156881899987794, W2:1.010161472035591, bias:-1.112706432394452, loss:0.5699642134461878\n",
      "W1:1.418793818421194, W2:1.0114228470550204, bias:-1.1149395705991467, loss:0.5698017180629168\n",
      "W1:1.4218990433837486, W2:1.0126791074566026, bias:-1.1171682378489833, loss:0.5696395775021728\n",
      "W1:1.4250038578368411, W2:1.0139302667605077, bias:-1.1193924605337904, loss:0.5694777897513212\n",
      "W1:1.428108254921158, W2:1.015176338717659, bias:-1.1216122645411972, loss:0.5693163528135837\n",
      "W1:1.4312122279607287, W2:1.0164173372975687, bias:-1.1238276752738015, loss:0.5691552647078728\n",
      "W1:1.4343157704562486, W2:1.0176532766766573, bias:-1.126038717665681, loss:0.5689945234686349\n",
      "W1:1.4374188760786557, W2:1.0188841712270422, bias:-1.1282454161982776, loss:0.5688341271456948\n",
      "W1:1.440521538662956, W2:1.0201100355057735, bias:-1.130447794915677, loss:0.5686740738041036\n",
      "W1:1.4436237522022852, W2:1.0213308842444995, bias:-1.1326458774393084, loss:0.5685143615239896\n",
      "W1:1.4467255108421995, W2:1.0225467323395492, bias:-1.1348396869820843, loss:0.5683549884004138\n",
      "W1:1.4498268088751831, W2:1.0237575948424102, bias:-1.1370292463620049, loss:0.5681959525432261\n",
      "W1:1.4529276407353695, W2:1.0249634869505915, bias:-1.1392145780152465, loss:0.5680372520769257\n",
      "W1:1.456028000993462, W2:1.0261644239988514, bias:-1.1413957040087541, loss:0.5678788851405225\n",
      "W1:1.4591278843518511, W2:1.0273604214507817, bias:-1.1435726460523568, loss:0.5677208498874036\n",
      "W1:1.4622272856399177, W2:1.0285514948907288, bias:-1.1457454255104254, loss:0.5675631444851986\n",
      "W1:1.4653261998095166, W2:1.0297376600160433, bias:-1.14791406341309, loss:0.56740576711565\n",
      "W1:1.4684246219306327, W2:1.030918932629641, bias:-1.1500785804670335, loss:0.5672487159744848\n",
      "W1:1.471522547187203, W2:1.032095328632868, bias:-1.1522389970658782, loss:0.5670919892712867\n",
      "W1:1.474619970873099, W2:1.033266864018653, bias:-1.1543953333001817, loss:0.5669355852293732\n",
      "W1:1.4777168883882608, W2:1.0344335548649404, bias:-1.1565476089670559, loss:0.5667795020856711\n",
      "W1:1.480813295234982, W2:1.035595417328388, bias:-1.1586958435794248, loss:0.5666237380905966\n",
      "W1:1.4839091870143328, W2:1.0367524676383246, bias:-1.1608400563749346, loss:0.5664682915079364\n",
      "W1:1.487004559422722, W2:1.037904722090954, bias:-1.1629802663245297, loss:0.5663131606147281\n",
      "W1:1.4900994082485883, W2:1.0390521970437947, bias:-1.165116492140707, loss:0.5661583437011461\n",
      "W1:1.4931937293692197, W2:1.0401949089103504, bias:-1.1672487522854618, loss:0.566003839070386\n",
      "W1:1.4962875187476916, W2:1.041332874154998, bias:-1.1693770649779365, loss:0.5658496450385516\n",
      "W1:1.4993807724299233, W2:1.0424661092880878, bias:-1.1715014482017838, loss:0.5656957599345436\n",
      "W1:1.5024734865418448, W2:1.0435946308612443, bias:-1.1736219197122555, loss:0.5655421820999486\n",
      "W1:1.505565657286673, W2:1.0447184554628637, bias:-1.175738497043027, loss:0.565388909888931\n",
      "W1:1.5086572809422893, W2:1.0458375997137979, bias:-1.1778511975127683, loss:0.5652359416681242\n",
      "W1:1.5117483538587189, W2:1.0469520802632168, bias:-1.1799600382314719, loss:0.5650832758165248\n",
      "W1:1.5148388724557031, W2:1.0480619137846459, bias:-1.1820650361065443, loss:0.5649309107253876\n",
      "W1:1.5179288332203666, W2:1.0491671169721672, bias:-1.1841662078486734, loss:0.5647788447981208\n",
      "W1:1.5210182327049704, W2:1.0502677065367827, bias:-1.1862635699774788, loss:0.5646270764501838\n",
      "W1:1.5241070675247506, W2:1.051363699202928, bias:-1.188357138826953, loss:0.5644756041089845\n",
      "W1:1.52719533435584, W2:1.0524551117051355, bias:-1.1904469305507024, loss:0.5643244262137794\n",
      "W1:1.5302830299332664, W2:1.0535419607848384, bias:-1.1925329611269964, loss:0.5641735412155736\n",
      "W1:1.5333701510490279, W2:1.0546242631873104, bias:-1.1946152463636295, loss:0.5640229475770225\n",
      "W1:1.5364566945502396, W2:1.055702035658737, bias:-1.1966938019026059, loss:0.5638726437723336\n",
      "W1:1.5395426573373512, W2:1.0567752949434117, bias:-1.1987686432246536, loss:0.5637226282871702\n",
      "W1:1.542628036362431, W2:1.0578440577810533, bias:-1.2008397856535722, loss:0.5635728996185562\n",
      "W1:1.545712828627514, W2:1.0589083409042386, bias:-1.2029072443604225, loss:0.5634234562747807\n",
      "W1:1.5487970311830135, W2:1.0599681610359473, bias:-1.2049710343675648, loss:0.5632742967753047\n",
      "W1:1.551880641126191, W2:1.061023534887214, bias:-1.2070311705525492, loss:0.5631254196506681\n",
      "W1:1.5549636555996837, W2:1.0620744791548817, bias:-1.2090876676518652, loss:0.5629768234423983\n",
      "W1:1.558046071790088, W2:1.063121010519457, bias:-1.2111405402645568, loss:0.562828506702918\n",
      "W1:1.5611278869265943, W2:1.064163145643057, bias:-1.213189802855706, loss:0.5626804679954565\n",
      "W1:1.5642090982796752, W2:1.0652009011674495, bias:-1.2152354697597914, loss:0.5625327058939591\n",
      "W1:1.5672897031598205, W2:1.0662342937121807, bias:-1.2172775551839259, loss:0.5623852189829995\n",
      "W1:1.57036969891632, W2:1.0672633398727867, bias:-1.219316073210979, loss:0.5622380058576918\n",
      "W1:1.573449082936093, W2:1.0682880562190862, bias:-1.2213510378025867, loss:0.5620910651236036\n",
      "W1:1.5765278526425586, W2:1.0693084592935507, bias:-1.2233824628020553, loss:0.5619443953966703\n",
      "W1:1.579606005494551, W2:1.0703245656097502, bias:-1.2254103619371606, loss:0.5617979953031097\n",
      "W1:1.5826835389852734, W2:1.0713363916508694, bias:-1.2274347488228483, loss:0.5616518634793369\n",
      "W1:1.58576045064129, W2:1.072343953868294, bias:-1.2294556369638399, loss:0.5615059985718823\n",
      "W1:1.5888367380215567, W2:1.073347268680263, bias:-1.231473039757145, loss:0.5613603992373067\n",
      "W1:1.5919123987164878, W2:1.074346352470584, bias:-1.2334869704944869, loss:0.5612150641421206\n",
      "W1:1.594987430347056, W2:1.0753412215874107, bias:-1.2354974423646423, loss:0.5610699919627019\n",
      "W1:1.5980618305639251, W2:1.076331892342078, bias:-1.237504468455699, loss:0.5609251813852157\n",
      "W1:1.6011355970466177, W2:1.0773183810079938, bias:-1.2395080617572363, loss:0.5607806311055339\n",
      "W1:1.6042087275027093, W2:1.0783007038195869, bias:-1.2415082351624271, loss:0.5606363398291563\n",
      "W1:1.6072812196670552, W2:1.0792788769713029, bias:-1.2435050014700708, loss:0.5604923062711319\n",
      "W1:1.6103530713010443, W2:1.0802529166166541, bias:-1.245498373386553, loss:0.5603485291559805\n",
      "W1:1.6134242801918801, W2:1.0812228388673153, bias:-1.2474883635277396, loss:0.5602050072176171\n",
      "W1:1.6164948441518887, W2:1.0821886597922659, bias:-1.2494749844208055, loss:0.560061739199273\n",
      "W1:1.6195647610178514, W2:1.0831503954169768, bias:-1.2514582485060006, loss:0.5599187238534223\n",
      "W1:1.6226340286503618, W2:1.0841080617226395, bias:-1.2534381681383564, loss:0.5597759599417048\n",
      "W1:1.6257026449332062, W2:1.0850616746454362, bias:-1.2554147555893347, loss:0.5596334462348529\n",
      "W1:1.6287706077727666, W2:1.0860112500758488, bias:-1.2573880230484205, loss:0.5594911815126172\n",
      "W1:1.631837915097445, W2:1.0869568038580055, bias:-1.2593579826246613, loss:0.559349164563693\n",
      "W1:1.6349045648571083, W2:1.0878983517890644, bias:-1.2613246463481558, loss:0.5592073941856479\n",
      "W1:1.6379705550225545, W2:1.0888359096186304, bias:-1.263288026171492, loss:0.5590658691848502\n",
      "W1:1.641035883584996, W2:1.0897694930482071, bias:-1.2652481339711381, loss:0.5589245883763974\n",
      "W1:1.6441005485555635, W2:1.090699117730679, bias:-1.2672049815487876, loss:0.5587835505840446\n",
      "W1:1.6471645479648258, W2:1.0916247992698256, bias:-1.2691585806326602, loss:0.5586427546401357\n",
      "W1:1.6502278798623284, W2:1.0925465532198642, bias:-1.2711089428787605, loss:0.5585021993855332\n",
      "W1:1.6532905423161472, W2:1.0934643950850218, bias:-1.2730560798720951, loss:0.5583618836695496\n",
      "W1:1.6563525334124591, W2:1.0943783403191334, bias:-1.275000003127851, loss:0.5582218063498785\n",
      "W1:1.6594138512551269, W2:1.0952884043252666, bias:-1.276940724092536, loss:0.5580819662925274\n",
      "W1:1.6624744939652996, W2:1.0961946024553715, bias:-1.2788782541450818, loss:0.5579423623717504\n",
      "W1:1.6655344596810262, W2:1.0970969500099539, bias:-1.280812604597914, loss:0.5578029934699817\n",
      "W1:1.668593746556884, W2:1.0979954622377723, bias:-1.2827437866979854, loss:0.5576638584777693\n",
      "W1:1.6716523527636187, W2:1.0988901543355567, bias:-1.2846718116277798, loss:0.5575249562937105\n",
      "W1:1.6747102764877972, W2:1.0997810414477482, bias:-1.2865966905062818, loss:0.5573862858243851\n",
      "W1:1.6777675159314747, W2:1.10066813866626, bias:-1.2885184343899185, loss:0.5572478459842931\n",
      "W1:1.6808240693118695, W2:1.101551461030256, bias:-1.290437054273471, loss:0.5571096356957899\n",
      "W1:1.683879934861053, W2:1.10243102352595, bias:-1.2923525610909585, loss:0.5569716538890234\n",
      "W1:1.6869351108256472, W2:1.1033068410864224, bias:-1.2942649657164946, loss:0.5568338995018706\n",
      "W1:1.6899895954665352, W2:1.104178928591453, bias:-1.2961742789651185, loss:0.5566963714798764\n",
      "W1:1.6930433870585793, W2:1.1050473008673714, bias:-1.2980805115936016, loss:0.5565590687761915\n",
      "W1:1.696096483890351, W2:1.1059119726869233, bias:-1.2999836743012283, loss:0.5564219903515115\n",
      "W1:1.6991488842638678, W2:1.1067729587691508, bias:-1.301883777730556, loss:0.5562851351740157\n",
      "W1:1.7022005864943413, W2:1.1076302737792885, bias:-1.3037808324681506, loss:0.5561485022193075\n",
      "W1:1.7052515889099318, W2:1.1084839323286726, bias:-1.3056748490453016, loss:0.5560120904703552\n",
      "W1:1.7083018898515128, W2:1.109333948974664, bias:-1.307565837938717, loss:0.555875898917432\n",
      "W1:1.711351487672442, W2:1.110180338220584, bias:-1.3094538095711965, loss:0.5557399265580586\n",
      "W1:1.714400380738341, W2:1.1110231145156613, bias:-1.3113387743122873, loss:0.5556041723969435\n",
      "W1:1.7174485674268813, W2:1.1118622922549926, bias:-1.3132207424789202, loss:0.5554686354459272\n",
      "W1:1.7204960461275787, W2:1.1126978857795127, bias:-1.315099724336029, loss:0.5553333147239238\n",
      "W1:1.7235428152415933, W2:1.1135299093759763, bias:-1.3169757300971507, loss:0.5551982092568651\n",
      "W1:1.7265888731815364, W2:1.1143583772769499, bias:-1.3188487699250118, loss:0.5550633180776446\n",
      "W1:1.7296342183712843, W2:1.115183303660813, bias:-1.3207188539320962, loss:0.5549286402260609\n",
      "W1:1.732678849245797, W2:1.1160047026517699, bias:-1.322585992181199, loss:0.554794174748763\n",
      "W1:1.7357227642509436, W2:1.116822588319869, bias:-1.3244501946859657, loss:0.5546599206991965\n",
      "W1:1.7387659618433338, W2:1.1176369746810326, bias:-1.3263114714114155, loss:0.5545258771375474\n",
      "W1:1.7418084404901528, W2:1.1184478756970926, bias:-1.328169832274453, loss:0.5543920431306905\n",
      "W1:1.7448501986690037, W2:1.1192553052758365, bias:-1.330025287144365, loss:0.5542584177521342\n",
      "W1:1.7478912348677538, W2:1.1200592772710594, bias:-1.3318778458433045, loss:0.5541250000819684\n",
      "W1:1.7509315475843854, W2:1.1208598054826235, bias:-1.3337275181467638, loss:0.5539917892068126\n",
      "W1:1.7539711353268521, W2:1.1216569036565252, bias:-1.3355743137840337, loss:0.5538587842197624\n",
      "W1:1.7570099966129393, W2:1.1224505854849671, bias:-1.3374182424386527, loss:0.5537259842203394\n",
      "W1:1.7600481299701285, W2:1.123240864606439, bias:-1.3392593137488444, loss:0.5535933883144392\n",
      "W1:1.7630855339354663, W2:1.1240277546058017, bias:-1.3410975373079448, loss:0.5534609956142807\n",
      "W1:1.7661222070554377, W2:1.1248112690143797, bias:-1.342932922664818, loss:0.5533288052383568\n",
      "W1:1.7691581478858418, W2:1.1255914213100562, bias:-1.344765479324263, loss:0.553196816311383\n",
      "W1:1.772193354991673, W2:1.126368224917376, bias:-1.3465952167474118, loss:0.5530650279642487\n",
      "W1:1.7752278269470052, W2:1.1271416932076523, bias:-1.3484221443521156, loss:0.5529334393339688\n",
      "W1:1.7782615623348783, W2:1.1279118394990775, bias:-1.3502462715133243, loss:0.5528020495636337\n",
      "W1:1.7812945597471896, W2:1.1286786770568398, bias:-1.3520676075634566, loss:0.5526708578023626\n",
      "W1:1.7843268177845877, W2:1.1294422190932434, bias:-1.3538861617927616, loss:0.552539863205254\n",
      "W1:1.7873583350563693, W2:1.1302024787678326, bias:-1.3557019434496722, loss:0.55240906493334\n",
      "W1:1.7903891101803793, W2:1.130959469187521, bias:-1.357514961741152, loss:0.5522784621535379\n",
      "W1:1.7934191417829137, W2:1.1317132034067225, bias:-1.3593252258330326, loss:0.5521480540386049\n",
      "W1:1.796448428498625, W2:1.1324636944274873, bias:-1.3611327448503459, loss:0.5520178397670903\n",
      "W1:1.7994769689704306, W2:1.1332109551996405, bias:-1.362937527877647, loss:0.5518878185232907\n",
      "W1:1.8025047618494234, W2:1.1339549986209245, bias:-1.3647395839593326, loss:0.5517579894972046\n",
      "W1:1.8055318057947853, W2:1.134695837537144, bias:-1.3665389220999513, loss:0.5516283518844868\n",
      "W1:1.8085580994737027, W2:1.1354334847423138, bias:-1.3683355512645088, loss:0.5514989048864037\n",
      "W1:1.8115836415612845, W2:1.136167952978809, bias:-1.3701294803787656, loss:0.5513696477097899\n",
      "W1:1.8146084307404822, W2:1.1368992549375185, bias:-1.37192071832953, loss:0.5512405795670023\n",
      "W1:1.8176324657020122, W2:1.1376274032580014, bias:-1.3737092739649448, loss:0.5511116996758791\n",
      "W1:1.8206557451442802, W2:1.138352410528643, bias:-1.3754951560947684, loss:0.5509830072596944\n",
      "W1:1.823678267773307, W2:1.1390742892868169, bias:-1.3772783734906513, loss:0.5508545015471159\n",
      "W1:1.8267000323026572, W2:1.139793052019045, bias:-1.3790589348864066, loss:0.5507261817721628\n",
      "W1:1.829721037453369, W2:1.1405087111611631, bias:-1.3808368489782756, loss:0.5505980471741639\n",
      "W1:1.8327412819538864, W2:1.1412212790984864, bias:-1.3826121244251897, loss:0.5504700969977147\n",
      "W1:1.835760764539992, W2:1.141930768165976, bias:-1.3843847698490253, loss:0.5503423304926366\n",
      "W1:1.838779483954743, W2:1.142637190648409, bias:-1.386154793834857, loss:0.5502147469139362\n",
      "W1:1.8417974389484073, W2:1.1433405587805485, bias:-1.3879222049312048, loss:0.5500873455217633\n",
      "W1:1.844814628278402, W2:1.1440408847473162, bias:-1.3896870116502764, loss:0.5499601255813719\n",
      "W1:1.847831050709233, W2:1.144738180683965, bias:-1.3914492224682076, loss:0.5498330863630786\n",
      "W1:1.850846705012437, W2:1.1454324586762534, bias:-1.393208845825296, loss:0.5497062271422247\n",
      "W1:1.8538615899665232, W2:1.1461237307606222, bias:-1.394965890126234, loss:0.5495795471991347\n",
      "W1:1.856875704356917, W2:1.1468120089243696, bias:-1.396720363740335, loss:0.5494530458190786\n",
      "W1:1.8598890469759066, W2:1.1474973051058301, bias:-1.3984722750017577, loss:0.5493267222922323\n",
      "W1:1.8629016166225878, W2:1.1481796311945527, bias:-1.4002216322097272, loss:0.5492005759136406\n",
      "W1:1.8659134121028127, W2:1.1488589990314797, bias:-1.4019684436287512, loss:0.5490746059831767\n",
      "W1:1.868924432229138, W2:1.1495354204091275, bias:-1.4037127174888344, loss:0.548948811805506\n",
      "W1:1.8719346758207756, W2:1.1502089070717672, bias:-1.405454461985689, loss:0.5488231926900488\n",
      "W1:1.8749441417035422, W2:1.1508794707156058, bias:-1.4071936852809417, loss:0.5486977479509414\n",
      "W1:1.8779528287098126, W2:1.1515471229889687, bias:-1.4089303955023391, loss:0.5485724769070014\n",
      "W1:1.8809607356784726, W2:1.1522118754924817, bias:-1.4106646007439485, loss:0.5484473788816893\n",
      "W1:1.883967861454872, W2:1.1528737397792543, bias:-1.4123963090663572, loss:0.5483224532030734\n",
      "W1:1.886974204890781, W2:1.1535327273550628, bias:-1.4141255284968681, loss:0.5481976992037928\n",
      "W1:1.8899797648443446, W2:1.1541888496785342, bias:-1.4158522670296936, loss:0.5480731162210225\n",
      "W1:1.8929845401800405, W2:1.1548421181613298, bias:-1.4175765326261458, loss:0.5479487035964379\n",
      "W1:1.8959885297686356, W2:1.1554925441683304, bias:-1.4192983332148255, loss:0.5478244606761792\n",
      "W1:1.8989917324871457, W2:1.1561401390178194, bias:-1.4210176766918072, loss:0.5477003868108169\n",
      "W1:1.9019941472187936, W2:1.1567849139816688, bias:-1.4227345709208232, loss:0.5475764813553177\n",
      "W1:1.904995772852969, W2:1.1574268802855232, bias:-1.4244490237334444, loss:0.547452743669009\n",
      "W1:1.90799660828519, W2:1.158066049108984, bias:-1.4261610429292597, loss:0.5473291731155467\n",
      "W1:1.910996652417064, W2:1.1587024315857959, bias:-1.427870636276052, loss:0.5472057690628801\n",
      "W1:1.9139959041562498, W2:1.1593360388040301, bias:-1.4295778115099735, loss:0.5470825308832186\n",
      "W1:1.9169943624164199, W2:1.1599668818062703, bias:-1.4312825763357182, loss:0.5469594579529994\n",
      "W1:1.919992026117225, W2:1.1605949715897965, bias:-1.4329849384266917, loss:0.5468365496528542\n",
      "W1:1.922988894184257, W2:1.161220319106771, bias:-1.434684905425181, loss:0.5467138053675767\n",
      "W1:1.9259849655490144, W2:1.1618429352644213, bias:-1.4363824849425197, loss:0.5465912244860892\n",
      "W1:1.928980239148867, W2:1.162462830925226, bias:-1.4380776845592547, loss:0.5464688064014129\n",
      "W1:1.9319747139270218, W2:1.1630800169070983, bias:-1.4397705118253075, loss:0.5463465505106337\n",
      "W1:1.9349683888324896, W2:1.1636945039835696, bias:-1.4414609742601365, loss:0.5462244562148718\n",
      "W1:1.9379612628200513, W2:1.1643063028839742, bias:-1.4431490793528967, loss:0.5461025229192509\n",
      "W1:1.9409533348502261, W2:1.1649154242936322, bias:-1.4448348345625963, loss:0.5459807500328661\n",
      "W1:1.9439446038892387, W2:1.165521878854033, bias:-1.4465182473182547, loss:0.5458591369687535\n",
      "W1:1.946935068908988, W2:1.166125677163018, bias:-1.448199325019056, loss:0.5457376831438604\n",
      "W1:1.9499247288870158, W2:1.1667268297749636, bias:-1.449878075034502, loss:0.5456163879790137\n",
      "W1:1.9529135828064765, W2:1.167325347200963, bias:-1.4515545047045648, loss:0.5454952508988913\n",
      "W1:1.9559016296561063, W2:1.1679212399090084, bias:-1.4532286213398367, loss:0.5453742713319915\n",
      "W1:1.9588888684301933, W2:1.1685145183241723, bias:-1.4549004322216776, loss:0.5452534487106035\n",
      "W1:1.961875298128549, W2:1.1691051928287888, bias:-1.4565699446023643, loss:0.5451327824707787\n",
      "W1:1.9648609177564784, W2:1.169693273762634, bias:-1.4582371657052347, loss:0.5450122720523006\n",
      "W1:1.9678457263247515, W2:1.1702787714231067, bias:-1.4599021027248331, loss:0.5448919168986571\n",
      "W1:1.9708297228495755, W2:1.1708616960654081, bias:-1.4615647628270536, loss:0.5447717164570116\n",
      "W1:1.9738129063525665, W2:1.1714420579027207, bias:-1.4632251531492817, loss:0.5446516701781743\n",
      "W1:1.9767952758607223, W2:1.1720198671063877, bias:-1.464883280800535, loss:0.544531777516574\n",
      "W1:1.979776830406395, W2:1.1725951338060914, bias:-1.4665391528616025, loss:0.5444120379302316\n",
      "W1:1.982757569027264, W2:1.1731678680900304, bias:-1.4681927763851839, loss:0.5442924508807304\n",
      "W1:1.9857374907663103, W2:1.1737380800050978, bias:-1.469844158396025, loss:0.5441730158331901\n",
      "W1:1.9887165946717893, W2:1.1743057795570575, bias:-1.4714933058910549, loss:0.5440537322562395\n",
      "W1:1.9916948797972056, W2:1.1748709767107208, bias:-1.4731402258395205, loss:0.5439345996219885\n",
      "W1:1.9946723452012873, W2:1.1754336813901223, bias:-1.47478492518312, loss:0.543815617406003\n",
      "W1:1.9976489899479604, W2:1.1759939034786944, bias:-1.4764274108361353, loss:0.5436967850872768\n",
      "W1:2.0006248131063242, W2:1.1765516528194426, bias:-1.478067689685564, loss:0.5435781021482061\n",
      "W1:2.003599813750627, W2:1.1771069392151197, bias:-1.4797057685912498, loss:0.5434595680745633\n",
      "W1:2.0065739909602396, W2:1.1776597724283984, bias:-1.4813416543860114, loss:0.543341182355471\n",
      "W1:2.0095473438196345, W2:1.1782101621820453, bias:-1.4829753538757717, loss:0.5432229444833763\n",
      "W1:2.012519871418359, W2:1.1787581181590925, bias:-1.484606873839685, loss:0.5431048539540254\n",
      "W1:2.0154915728510128, W2:1.1793036500030096, bias:-1.486236221030263, loss:0.5429869102664383\n",
      "W1:2.018462447217224, W2:1.1798467673178745, bias:-1.4878634021735015, loss:0.5428691129228838\n",
      "W1:2.0214324936216275, W2:1.1803874796685445, bias:-1.4894884239690036, loss:0.5427514614288543\n",
      "W1:2.0244017111738395, W2:1.1809257965808257, bias:-1.4911112930901047, loss:0.542633955293042\n",
      "W1:2.0273700989884365, W2:1.1814617275416421, bias:-1.492732016183994, loss:0.5425165940273134\n",
      "W1:2.030337656184933, W2:1.1819952819992046, bias:-1.494350599871838, loss:0.5423993771466858\n",
      "W1:2.033304381887757, W2:1.1825264693631787, bias:-1.4959670507489002, loss:0.5422823041693027\n",
      "W1:2.0362702752262316, W2:1.1830552990048524, bias:-1.4975813753846616, loss:0.5421653746164105\n",
      "W1:2.039235335334549, W2:1.1835817802573023, bias:-1.4991935803229406, loss:0.5420485880123336\n",
      "W1:2.0421995613517523, W2:1.1841059224155597, bias:-1.500803672082011, loss:0.5419319438844535\n",
      "W1:2.0451629524217108, W2:1.1846277347367764, bias:-1.5024116571547197, loss:0.5418154417631823\n",
      "W1:2.048125507693101, W2:1.1851472264403893, bias:-1.504017542008604, loss:0.541699081181942\n",
      "W1:2.051087226319386, W2:1.1856644067082842, bias:-1.505621333086007, loss:0.5415828616771402\n",
      "W1:2.054048107458791, W2:1.18617928468496, bias:-1.5072230368041943, loss:0.5414667827881483\n",
      "W1:2.057008150274287, W2:1.1866918694776905, bias:-1.5088226595554668, loss:0.5413508440572786\n",
      "W1:2.0599673539335663, W2:1.1872021701566875, bias:-1.5104202077072757, loss:0.5412350450297614\n",
      "W1:2.0629257176090254, W2:1.1877101957552616, bias:-1.5120156876023356, loss:0.5411193852537236\n",
      "W1:2.0658832404777416, W2:1.1882159552699834, bias:-1.5136091055587362, loss:0.5410038642801663\n",
      "W1:2.0688399217214557, W2:1.1887194576608435, bias:-1.5152004678700541, loss:0.5408884816629433\n",
      "W1:2.07179576052655, W2:1.189220711851412, bias:-1.5167897808054644, loss:0.5407732369587389\n",
      "W1:2.0747507560840295, W2:1.1897197267289972, bias:-1.5183770506098502, loss:0.540658129727047\n",
      "W1:2.0777049075895024, W2:1.190216511144804, bias:-1.5199622835039122, loss:0.5405431595301492\n",
      "W1:2.0806582142431598, W2:1.1907110739140911, bias:-1.5215454856842776, loss:0.5404283259330945\n",
      "W1:2.0836106752497563, W2:1.1912034238163283, bias:-1.5231266633236087, loss:0.5403136285036771\n",
      "W1:2.0865622898185925, W2:1.1916935695953519, bias:-1.5247058225707095, loss:0.5401990668124173\n",
      "W1:2.0895130571634937, W2:1.1921815199595205, bias:-1.5262829695506335, loss:0.5400846404325388\n",
      "W1:2.0924629765027922, W2:1.1926672835818704, bias:-1.5278581103647892, loss:0.5399703489399506\n",
      "W1:2.095412047059309, W2:1.1931508691002686, bias:-1.5294312510910462, loss:0.539856191913224\n",
      "W1:2.0983602680603335, W2:1.193632285117567, bias:-1.5310023977838396, loss:0.5397421689335752\n",
      "W1:2.101307638737606, W2:1.1941115402017553, bias:-1.5325715564742748, loss:0.5396282795848438\n",
      "W1:2.1042541583273, W2:1.1945886428861126, bias:-1.5341387331702308, loss:0.5395145234534726\n",
      "W1:2.107199826070002, W2:1.1950636016693592, bias:-1.5357039338564633, loss:0.5394009001284892\n",
      "W1:2.110144641210695, W2:1.1955364250158074, bias:-1.5372671644947078, loss:0.5392874092014858\n",
      "W1:2.1130886029987392, W2:1.196007121355511, bias:-1.5388284310237803, loss:0.5391740502665995\n",
      "W1:2.116031710687856, W2:1.196475699084416, bias:-1.5403877393596799, loss:0.5390608229204944\n",
      "W1:2.1189739635361087, W2:1.1969421665645084, bias:-1.5419450953956884, loss:0.5389477267623413\n",
      "W1:2.121915360805884, W2:1.1974065321239624, bias:-1.543500505002471, loss:0.5388347613937987\n",
      "W1:2.1248559017638757, W2:1.197868804057288, bias:-1.5450539740281763, loss:0.538721926418996\n",
      "W1:2.1277955856810684, W2:1.1983289906254782, bias:-1.546605508298534, loss:0.5386092214445122\n",
      "W1:2.130734411832717, W2:1.1987871000561545, bias:-1.5481551136169545, loss:0.5384966460793598\n",
      "W1:2.1336723794983325, W2:1.1992431405437127, bias:-1.5497027957646266, loss:0.5383841999349648\n",
      "W1:2.136609487961663, W2:1.1996971202494677, bias:-1.5512485605006143, loss:0.5382718826251502\n",
      "W1:2.139545736510677, W2:1.2001490473017977, bias:-1.5527924135619542, loss:0.5381596937661166\n",
      "W1:2.142481124437546, W2:1.2005989297962876, bias:-1.5543343606637512, loss:0.5380476329764248\n",
      "W1:2.1454156510386304, W2:1.2010467757958716, bias:-1.5558744074992752, loss:0.5379356998769783\n",
      "W1:2.1483493156144586, W2:1.2014925933309764, bias:-1.557412559740055, loss:0.537823894091006\n",
      "W1:2.151282117469713, W2:1.2019363903996614, bias:-1.5589488230359743, loss:0.5377122152440439\n",
      "W1:2.1542140559132137, W2:1.2023781749677604, bias:-1.5604832030153648, loss:0.5376006629639186\n",
      "W1:2.1571451302579, W2:1.2028179549690223, bias:-1.5620157052851005, loss:0.5374892368807301\n",
      "W1:2.160075339820816, W2:1.2032557383052496, bias:-1.5635463354306909, loss:0.5373779366268341\n",
      "W1:2.1630046839230928, W2:1.2036915328464382, bias:-1.5650750990163733, loss:0.5372667618368255\n",
      "W1:2.165933161889934, W2:1.2041253464309152, bias:-1.5666020015852051, loss:0.5371557121475228\n",
      "W1:2.1688607730505973, W2:1.2045571868654772, bias:-1.5681270486591559, loss:0.5370447871979493\n",
      "W1:2.171787516738382, W2:1.204987061925527, bias:-1.5696502457391976, loss:0.5369339866293186\n",
      "W1:2.174713392290609, W2:1.2054149793552098, bias:-1.5711715983053969, loss:0.5368233100850166\n",
      "W1:2.1776383990486075, W2:1.2058409468675495, bias:-1.5726911118170037, loss:0.5367127572105872\n",
      "W1:2.180562536357699, W2:1.2062649721445837, bias:-1.5742087917125418, loss:0.5366023276537146\n",
      "W1:2.183485803567181, W2:1.206687062837498, bias:-1.5757246434098982, loss:0.5364920210642077\n",
      "W1:2.186408200030312, W2:1.2071072265667602, bias:-1.5772386723064116, loss:0.5363818370939847\n",
      "W1:2.1893297251042956, W2:1.2075254709222534, bias:-1.578750883778961, loss:0.5362717753970573\n",
      "W1:2.192250378150265, W2:1.207941803463409, bias:-1.5802612831840537, loss:0.5361618356295147\n",
      "W1:2.1951701585332675, W2:1.2083562317193384, bias:-1.5817698758579122, loss:0.536052017449509\n",
      "W1:2.1980890656222507, W2:1.2087687631889643, bias:-1.583276667116562, loss:0.5359423205172384\n",
      "W1:2.2010070987900456, W2:1.2091794053411524, bias:-1.5847816622559172, loss:0.535832744494934\n",
      "W1:2.203924257413352, W2:1.2095881656148404, bias:-1.5862848665518676, loss:0.5357232890468429\n",
      "W1:2.2068405408727245, W2:1.2099950514191686, bias:-1.587786285260364, loss:0.5356139538392134\n",
      "W1:2.209755948552555, W2:1.2104000701336084, bias:-1.5892859236175025, loss:0.5355047385402821\n",
      "W1:2.212670479841061, W2:1.2108032291080908, bias:-1.590783786839611, loss:0.535395642820257\n",
      "W1:2.2155841341302676, W2:1.2112045356631342, bias:-1.5922798801233327, loss:0.5352866663513033\n",
      "W1:2.2184969108159964, W2:1.211603997089972, bias:-1.5937742086457092, loss:0.5351778088075299\n",
      "W1:2.221408809297848, W2:1.212001620650678, bias:-1.5952667775642653, loss:0.5350690698649738\n",
      "W1:2.224319828979188, W2:1.212397413578294, bias:-1.5967575920170918, loss:0.5349604492015867\n",
      "W1:2.2272299692671327, W2:1.212791383076954, bias:-1.598246657122928, loss:0.5348519464972203\n",
      "W1:2.2301392295725355, W2:1.21318353632201, bias:-1.5997339779812434, loss:0.534743561433613\n",
      "W1:2.233047609309971, W2:1.213573880460155, bias:-1.6012195596723209, loss:0.5346352936943745\n",
      "W1:2.2359551078977216, W2:1.213962422609548, bias:-1.602703407257337, loss:0.5345271429649735\n",
      "W1:2.238861724757763, W2:1.214349169859936, bias:-1.604185525778443, loss:0.5344191089327236\n",
      "W1:2.241767459315751, W2:1.2147341292727774, bias:-1.6056659202588466, loss:0.5343111912867685\n",
      "W1:2.2446723110010054, W2:1.2151173078813629, bias:-1.607144595702891, loss:0.534203389718071\n",
      "W1:2.247576279246497, W2:1.2154987126909378, bias:-1.608621557096135, loss:0.5340957039193962\n",
      "W1:2.250479363488835, W2:1.2158783506788227, bias:-1.6100968094054333, loss:0.5339881335853017\n",
      "W1:2.2533815631682503, W2:1.2162562287945329, bias:-1.6115703575790141, loss:0.5338806784121218\n",
      "W1:2.256282877728584, W2:1.2166323539598987, bias:-1.6130422065465588, loss:0.5337733380979557\n",
      "W1:2.259183306617273, W2:1.2170067330691845, bias:-1.6145123612192802, loss:0.5336661123426544\n",
      "W1:2.2620828492853353, W2:1.217379372989207, bias:-1.61598082649, loss:0.5335590008478072\n",
      "W1:2.2649815051873574, W2:1.2177502805594536, bias:-1.6174476072332262, loss:0.5334520033167299\n",
      "W1:2.26787927378148, W2:1.218119462592199, bias:-1.618912708305231, loss:0.533345119454451\n",
      "W1:2.2707761545293854, W2:1.2184869258726232, bias:-1.6203761345441268, loss:0.5332383489677003\n",
      "W1:2.273672146896284, W2:1.2188526771589268, bias:-1.6218378907699427, loss:0.5331316915648954\n",
      "W1:2.2765672503508987, W2:1.2192167231824473, bias:-1.6232979817847006, loss:0.5330251469561298\n",
      "W1:2.279461464365455, W2:1.2195790706477743, bias:-1.6247564123724914, loss:0.5329187148531608\n",
      "W1:2.282354788415665, W2:1.2199397262328637, bias:-1.626213187299549, loss:0.5328123949693967\n",
      "W1:2.2852472219807147, W2:1.2202986965891525, bias:-1.6276683113143262, loss:0.5327061870198851\n",
      "W1:2.2881387645432527, W2:1.2206559883416717, bias:-1.6291217891475693, loss:0.5326000907213015\n",
      "W1:2.291029415589374, W2:1.22101160808916, bias:-1.6305736255123915, loss:0.5324941057919363\n",
      "W1:2.2939191746086096, W2:1.221365562404176, bias:-1.6320238251043473, loss:0.5323882319516833\n",
      "W1:2.2968080410939122, W2:1.22171785783321, bias:-1.6334723926015058, loss:0.5322824689220286\n",
      "W1:2.2996960145416434, W2:1.222068500896796, bias:-1.634919332664524, loss:0.5321768164260379\n",
      "W1:2.3025830944515606, W2:1.222417498089622, bias:-1.6363646499367193, loss:0.5320712741883461\n",
      "W1:2.3054692803268053, W2:1.2227648558806412, bias:-1.6378083490441422, loss:0.5319658419351455\n",
      "W1:2.3083545716738887, W2:1.223110580713181, bias:-1.639250434595648, loss:0.5318605193941736\n",
      "W1:2.3112389680026806, W2:1.2234546790050531, bias:-1.6406909111829688, loss:0.5317553062947031\n",
      "W1:2.3141224688263953, W2:1.223797157148662, bias:-1.6421297833807849, loss:0.5316502023675301\n",
      "W1:2.31700507366158, W2:1.2241380215111137, bias:-1.6435670557467954, loss:0.5315452073449627\n",
      "W1:2.3198867820281017, W2:1.224477278434323, bias:-1.6450027328217898, loss:0.5314403209608105\n",
      "W1:2.322767593449135, W2:1.224814934235121, bias:-1.6464368191297172, loss:0.5313355429503736\n",
      "W1:2.3256475074511496, W2:1.2251509952053623, bias:-1.6478693191777574, loss:0.5312308730504315\n",
      "W1:2.328526523563897, W2:1.2254854676120315, bias:-1.6493002374563899, loss:0.531126310999233\n",
      "W1:2.3314046413204004, W2:1.2258183576973478, bias:-1.6507295784394636, loss:0.5310218565364848\n",
      "W1:2.3342818602569397, W2:1.226149671678872, bias:-1.6521573465842656, loss:0.5309175094033406\n",
      "W1:2.3371581799130414, W2:1.2264794157496097, bias:-1.6535835463315904, loss:0.5308132693423921\n",
      "W1:2.3400335998314654, W2:1.2268075960781173, bias:-1.6550081821058078, loss:0.5307091360976574\n",
      "W1:2.3429081195581927, W2:1.2271342188086043, bias:-1.656431258314931, loss:0.5306051094145704\n",
      "W1:2.3457817386424136, W2:1.2274592900610377, bias:-1.6578527793506845, loss:0.530501189039972\n",
      "W1:2.348654456636516, W2:1.2277828159312447, bias:-1.6592727495885715, loss:0.530397374722098\n",
      "W1:2.351526273096073, W2:1.2281048024910146, bias:-1.6606911733879413, loss:0.5302936662105707\n",
      "W1:2.3543971875798317, W2:1.2284252557882014, bias:-1.6621080550920553, loss:0.530190063256388\n",
      "W1:2.3572671996496997, W2:1.2287441818468248, bias:-1.6635233990281544, loss:0.5300865656119131\n",
      "W1:2.3601363088707346, W2:1.2290615866671712, bias:-1.6649372095075243, loss:0.5299831730308665\n",
      "W1:2.363004514811132, W2:1.2293774762258944, bias:-1.6663494908255625, loss:0.5298798852683133\n",
      "W1:2.3658718170422137, W2:1.2296918564761157, bias:-1.6677602472618425, loss:0.5297767020806557\n",
      "W1:2.368738215138416, W2:1.2300047333475228, bias:-1.66916948308018, loss:0.5296736232256233\n",
      "W1:2.371603708677277, W2:1.23031611274647, bias:-1.6705772025286973, loss:0.5295706484622618\n",
      "W1:2.374468297239428, W2:1.2306260005560754, bias:-1.671983409839889, loss:0.5294677775509258\n",
      "W1:2.377331980408578, W2:1.2309344026363207, bias:-1.6733881092306842, loss:0.5293650102532679\n",
      "W1:2.3801947577715055, W2:1.2312413248241474, bias:-1.674791304902513, loss:0.5292623463322289\n",
      "W1:2.3830566289180446, W2:1.231546772933555, bias:-1.6761930010413684, loss:0.5291597855520305\n",
      "W1:2.3859175934410755, W2:1.2318507527556974, bias:-1.677593201817871, loss:0.5290573276781643\n",
      "W1:2.3887776509365124, W2:1.2321532700589795, bias:-1.678991911387331, loss:0.5289549724773833\n",
      "W1:2.3916368010032913, W2:1.2324543305891524, bias:-1.6803891338898123, loss:0.5288527197176932\n",
      "W1:2.3944950432433605, W2:1.23275394006941, bias:-1.6817848734501937, loss:0.5287505691683431\n",
      "W1:2.397352377261668, W2:1.2330521042004832, bias:-1.6831791341782325, loss:0.5286485205998155\n",
      "W1:2.40020880266615, W2:1.2333488286607344, bias:-1.6845719201686256, loss:0.5285465737838203\n",
      "W1:2.403064319067722, W2:1.233644119106252, bias:-1.6859632355010712, loss:0.5284447284932826\n",
      "W1:2.4059189260802656, W2:1.2339379811709439, bias:-1.687353084240331, loss:0.5283429845023365\n",
      "W1:2.4087726233206177, W2:1.2342304204666308, bias:-1.6887414704362906, loss:0.5282413415863151\n",
      "W1:2.41162541040856, W2:1.2345214425831388, bias:-1.6901283981240207, loss:0.5281397995217428\n",
      "W1:2.414477286966808, W2:1.234811053088392, bias:-1.6915138713238371, loss:0.528038358086326\n",
      "W1:2.4173282526210005, W2:1.2350992575285045, bias:-1.692897894041362, loss:0.5279370170589454\n",
      "W1:2.4201783069996874, W2:1.2353860614278716, bias:-1.6942804702675827, loss:0.5278357762196465\n",
      "W1:2.4230274497343207, W2:1.235671470289261, bias:-1.695661603978913, loss:0.527734635349633\n",
      "W1:2.425875680459242, W2:1.2359554895939042, bias:-1.697041299137251, loss:0.5276335942312568\n",
      "W1:2.4287229988116725, W2:1.2362381248015852, bias:-1.6984195596900395, loss:0.527532652648011\n",
      "W1:2.431569404431703, W2:1.2365193813507318, bias:-1.6997963895703243, loss:0.5274318103845209\n",
      "W1:2.434414896962281, W2:1.2367992646585042, bias:-1.7011717926968128, loss:0.5273310672265371\n",
      "W1:2.437259476049204, W2:1.237077780120884, bias:-1.7025457729739328, loss:0.527230422960926\n",
      "W1:2.4401031413411047, W2:1.2373549331127638, bias:-1.7039183342918904, loss:0.5271298773756633\n",
      "W1:2.4429458924894427, W2:1.2376307289880337, bias:-1.7052894805267278, loss:0.5270294302598248\n",
      "W1:2.445787729148494, W2:1.2379051730796704, bias:-1.706659215540381, loss:0.5269290814035799\n",
      "W1:2.4486286509753405, W2:1.2381782706998241, bias:-1.708027543180737, loss:0.5268288305981824\n",
      "W1:2.4514686576298583, W2:1.2384500271399053, bias:-1.709394467281691, loss:0.5267286776359645\n",
      "W1:2.454307748774709, W2:1.2387204476706712, bias:-1.7107599916632028, loss:0.5266286223103275\n",
      "W1:2.4571459240753284, W2:1.238989537542313, bias:-1.7121241201313542, loss:0.5265286644157353\n",
      "W1:2.459983183199917, W2:1.23925730198454, bias:-1.7134868564784043, loss:0.5264288037477063\n",
      "W1:2.4628195258194285, W2:1.2395237462066664, bias:-1.7148482044828461, loss:0.5263290401028061\n",
      "W1:2.4656549516075614, W2:1.239788875397695, bias:-1.7162081679094623, loss:0.5262293732786406\n",
      "W1:2.468489460240747, W2:1.2400526947264026, bias:-1.7175667505093803, loss:0.526129803073848\n",
      "W1:2.47132305139814, W2:1.2403152093414243, bias:-1.7189239560201284, loss:0.5260303292880913\n",
      "W1:2.474155724761609, W2:1.2405764243713364, bias:-1.7202797881656897, loss:0.525930951722052\n",
      "W1:2.476987480015726, W2:1.240836344924741, bias:-1.7216342506565578, loss:0.5258316701774217\n",
      "W1:2.4798183168477563, W2:1.241094976090348, bias:-1.722987347189791, loss:0.5257324844568964\n",
      "W1:2.482648234947648, W2:1.2413523229370582, bias:-1.724339081449066, loss:0.5256333943641681\n",
      "W1:2.485477234008023, W2:1.2416083905140456, bias:-1.7256894571047332, loss:0.5255343997039187\n",
      "W1:2.488305313724167, W2:1.2418631838508387, bias:-1.7270384778138692, loss:0.5254355002818124\n",
      "W1:2.491132473794019, W2:1.2421167079574031, bias:-1.7283861472203312, loss:0.5253366959044892\n",
      "W1:2.493958713918162, W2:1.2423689678242218, bias:-1.72973246895481, loss:0.5252379863795578\n",
      "W1:2.4967840337998135, W2:1.242619968422376, bias:-1.7310774466348826, loss:0.525139371515589\n",
      "W1:2.4996084331448145, W2:1.2428697147036256, bias:-1.732421083865066, loss:0.5250408511221085\n",
      "W1:2.5024319116616214, W2:1.2431182116004897, bias:-1.7337633842368692, loss:0.5249424250095909\n",
      "W1:2.505254469061295, W2:1.2433654640263252, bias:-1.7351043513288453, loss:0.5248440929894523\n",
      "W1:2.508076105057492, W2:1.2436114768754072, bias:-1.7364439887066445, loss:0.5247458548740446\n",
      "W1:2.510896819366455, W2:1.2438562550230072, bias:-1.7377822999230648, loss:0.5246477104766476\n",
      "W1:2.513716611707002, W2:1.244099803325472, bias:-1.7391192885181046, loss:0.5245496596114638\n",
      "W1:2.5165354818005183, W2:1.2443421266203014, bias:-1.7404549580190138, loss:0.5244517020936115\n",
      "W1:2.519353429370946, W2:1.244583229726227, bias:-1.741789311940345, loss:0.5243538377391177\n",
      "W1:2.5221704541447756, W2:1.2448231174432887, bias:-1.743122353784004, loss:0.5242560663649133\n",
      "W1:2.524986555851035, W2:1.2450617945529123, bias:-1.7444540870393013, loss:0.5241583877888253\n",
      "W1:2.527801734221282, W2:1.2452992658179864, bias:-1.7457845151830023, loss:0.524060801829571\n",
      "W1:2.530615988989594, W2:1.2455355359829388, bias:-1.7471136416793773, loss:0.523963308306752\n",
      "W1:2.5334293198925573, W2:1.245770609773812, bias:-1.7484414699802522, loss:0.5238659070408477\n",
      "W1:2.5362417266692607, W2:1.2460044918983395, bias:-1.7497680035250573, loss:0.5237685978532103\n",
      "W1:2.5390532090612847, W2:1.2462371870460212, bias:-1.751093245740878, loss:0.5236713805660564\n",
      "W1:2.5418637668126918, W2:1.2464686998881982, bias:-1.7524172000425038, loss:0.5235742550024636\n",
      "W1:2.544673399670018, W2:1.246699035078128, bias:-1.7537398698324766, loss:0.5234772209863626\n",
      "W1:2.547482107382264, W2:1.246928197251058, bias:-1.7550612585011414, loss:0.5233802783425324\n",
      "W1:2.5502898897008857, W2:1.2471561910243, bias:-1.756381369426693, loss:0.5232834268965934\n",
      "W1:2.553096746379785, W2:1.2473830209973045, bias:-1.7577002059752265, loss:0.5231866664750028\n",
      "W1:2.5559026771753013, W2:1.2476086917517326, bias:-1.759017771500784, loss:0.5230899969050475\n",
      "W1:2.558707681846201, W2:1.2478332078515306, bias:-1.7603340693454037, loss:0.5229934180148397\n",
      "W1:2.5615117601536705, W2:1.2480565738430016, bias:-1.7616491028391672, loss:0.5228969296333097\n",
      "W1:2.564314911861307, W2:1.2482787942548783, bias:-1.7629628753002475, loss:0.5228005315902013\n",
      "W1:2.5671171367351078, W2:1.248499873598395, bias:-1.7642753900349561, loss:0.5227042237160662\n",
      "W1:2.569918434543464, W2:1.2487198163673592, bias:-1.7655866503377904, loss:0.5226080058422571\n",
      "W1:2.572718805057149, W2:1.2489386270382232, bias:-1.766896659491481, loss:0.522511877800924\n",
      "W1:2.575518248049312, W2:1.249156310070155, bias:-1.7682054207670381, loss:0.5224158394250075\n",
      "W1:2.578316763295468, W2:1.249372869905109, bias:-1.7695129374237983, loss:0.5223198905482337\n",
      "W1:2.5811143505734897, W2:1.2495883109678962, bias:-1.770819212709471, loss:0.5222240310051088\n",
      "W1:2.5839110096635975, W2:1.249802637666255, bias:-1.7721242498601848, loss:0.5221282606309134\n",
      "W1:2.586706740348353, W2:1.2500158543909203, bias:-1.773428052100533, loss:0.5220325792616974\n",
      "W1:2.589501542412648, W2:1.2502279655156934, bias:-1.7747306226436195, loss:0.5219369867342754\n",
      "W1:2.592295415643698, W2:1.2504389753975107, bias:-1.776031964691105, loss:0.5218414828862196\n",
      "W1:2.595088359831033, W2:1.250648888376513, bias:-1.7773320814332516, loss:0.5217460675558564\n",
      "W1:2.5978803747664876, W2:1.250857708776114, bias:-1.778630976048969, loss:0.521650740582261\n",
      "W1:2.600671460244194, W2:1.251065440903068, bias:-1.7799286517058577, loss:0.5215555018052509\n",
      "W1:2.603461616060573, W2:1.2512720890475386, bias:-1.781225111560256, loss:0.5214603510653822\n",
      "W1:2.606250842014326, W2:1.2514776574831656, bias:-1.7825203587572829, loss:0.521365288203944\n",
      "W1:2.609039137906426, W2:1.2516821504671327, bias:-1.7838143964308835, loss:0.5212703130629535\n",
      "W1:2.6118265035401094, W2:1.2518855722402342, bias:-1.7851072277038724, loss:0.5211754254851505\n",
      "W1:2.6146129387208674, W2:1.2520879270269423, bias:-1.7863988556879786, loss:0.5210806253139935\n",
      "W1:2.617398443256439, W2:1.2522892190354729, bias:-1.7876892834838887, loss:0.5209859123936537\n",
      "W1:2.6201830169568003, W2:1.2524894524578518, bias:-1.7889785141812906, loss:0.520891286569011\n",
      "W1:2.6229666596341588, W2:1.252688631469981, bias:-1.790266550858917, loss:0.5207967476856482\n",
      "W1:2.6257493711029434, W2:1.2528867602317035, bias:-1.7915533965845891, loss:0.5207022955898473\n",
      "W1:2.628531151179797, W2:1.2530838428868691, bias:-1.7928390544152586, loss:0.5206079301285839\n",
      "W1:2.631311999683568, W2:1.2532798835633991, bias:-1.7941235273970517, loss:0.5205136511495227\n",
      "W1:2.6340919164353025, W2:1.2534748863733507, bias:-1.7954068185653111, loss:0.5204194585010131\n",
      "W1:2.636870901258236, W2:1.2536688554129816, bias:-1.7966889309446388, loss:0.5203253520320841\n",
      "W1:2.639648953977785, W2:1.253861794762814, bias:-1.797969867548938, loss:0.5202313315924398\n",
      "W1:2.6424260744215395, W2:1.2540537084876986, bias:-1.7992496313814557, loss:0.5201373970324552\n",
      "W1:2.645202262419255, W2:1.2542446006368781, bias:-1.8005282254348247, loss:0.520043548203171\n",
      "W1:2.6479775178028437, W2:1.25443447524405, bias:-1.8018056526911048, loss:0.5199497849562893\n",
      "W1:2.6507518404063672, W2:1.2546233363274295, bias:-1.8030819161218248, loss:0.5198561071441697\n",
      "W1:2.653525230066029, W2:1.2548111878898127, bias:-1.8043570186880236, loss:0.519762514619824\n",
      "W1:2.6562976866201655, W2:1.2549980339186384, bias:-1.805630963340292, loss:0.5196690072369122\n",
      "W1:2.6590692099092386, W2:1.2551838783860507, bias:-1.8069037530188132, loss:0.519575584849738\n",
      "W1:2.6618397997758283, W2:1.2553687252489603, bias:-1.8081753906534037, loss:0.5194822473132444\n",
      "W1:2.6646094560646247, W2:1.2555525784491064, bias:-1.8094458791635541, loss:0.5193889944830098\n",
      "W1:2.6673781786224198, W2:1.255735441913118, bias:-1.8107152214584699, loss:0.519295826215243\n",
      "W1:2.670145967298099, W2:1.2559173195525741, bias:-1.811983420437111, loss:0.5192027423667795\n",
      "W1:2.6729128219426364, W2:1.256098215264066, bias:-1.813250478988233, loss:0.5191097427950772\n",
      "W1:2.675678742409083, W2:1.256278132929256, bias:-1.8145163999904264, loss:0.5190168273582114\n",
      "W1:2.678443728552563, W2:1.256457076414939, bias:-1.815781186312156, loss:0.5189239959148724\n",
      "W1:2.681207780230262, W2:1.2566350495731011, bias:-1.817044840811802, loss:0.5188312483243593\n",
      "W1:2.683970897301423, W2:1.2568120562409806, bias:-1.818307366337698, loss:0.5187385844465777\n",
      "W1:2.686733079627337, W2:1.2569881002411263, bias:-1.8195687657281712, loss:0.5186460041420342\n",
      "W1:2.689494327071336, W2:1.257163185381457, bias:-1.8208290418115807, loss:0.5185535072718331\n",
      "W1:2.6922546394987856, W2:1.2573373154553211, bias:-1.8220881974063574, loss:0.5184610936976728\n",
      "W1:2.695014016777076, W2:1.2575104942415536, bias:-1.8233462353210426, loss:0.5183687632818403\n",
      "W1:2.697772458775617, W2:1.2576827255045357, bias:-1.8246031583543256, loss:0.5182765158872088\n",
      "W1:2.7005299653658286, W2:1.257854012994253, bias:-1.8258589692950837, loss:0.5181843513772335\n",
      "W1:2.7032865364211336, W2:1.2580243604463521, bias:-1.8271136709224194, loss:0.5180922696159466\n",
      "W1:2.706042171816952, W2:1.2581937715821994, bias:-1.8283672660056987, loss:0.518000270467955\n",
      "W1:2.708796871430691, W2:1.2583622501089375, bias:-1.8296197573045894, loss:0.5179083537984354\n",
      "W1:2.711550635141741, W2:1.2585297997195433, bias:-1.8308711475690986, loss:0.5178165194731306\n",
      "W1:2.714303462831464, W2:1.2586964240928837, bias:-1.8321214395396106, loss:0.517724767358346\n",
      "W1:2.7170553543831906, W2:1.2588621268937732, bias:-1.833370635946924, loss:0.5176330973209462\n",
      "W1:2.7198063096822094, W2:1.2590269117730293, bias:-1.8346187395122893, loss:0.5175415092283502\n",
      "W1:2.722556328615762, W2:1.259190782367529, bias:-1.8358657529474462, loss:0.517450002948529\n",
      "W1:2.7253054110730344, W2:1.2593537423002648, bias:-1.8371116789546602, loss:0.5173585783500005\n",
      "W1:2.7280535569451505, W2:1.2595157951803995, bias:-1.8383565202267593, loss:0.5172672353018274\n",
      "W1:2.730800766125164, W2:1.259676944603323, bias:-1.8396002794471715, loss:0.5171759736736125\n",
      "W1:2.7335470385080534, W2:1.2598371941507056, bias:-1.8408429592899607, loss:0.5170847933354951\n",
      "W1:2.736292373990712, W2:1.2599965473905543, bias:-1.842084562419863, loss:0.5169936941581482\n",
      "W1:2.739036772471943, W2:1.2601550078772665, bias:-1.8433250914923232, loss:0.5169026760127746\n",
      "W1:2.7417802338524515, W2:1.260312579151685, bias:-1.8445645491535305, loss:0.5168117387711029\n",
      "W1:2.744522758034838, W2:1.2604692647411513, bias:-1.8458029380404553, loss:0.5167208823053849\n",
      "W1:2.74726434492359, W2:1.2606250681595603, bias:-1.8470402607808833, loss:0.5166301064883915\n",
      "W1:2.7500049944250775, W2:1.260779992907414, bias:-1.8482765199934532, loss:0.5165394111934091\n",
      "W1:2.7527447064475434, W2:1.260934042471874, bias:-1.8495117182876901, loss:0.5164487962942372\n",
      "W1:2.755483480901098, W2:1.2610872203268157, bias:-1.8507458582640421, loss:0.5163582616651841\n",
      "W1:2.7582213176977115, W2:1.261239529932881, bias:-1.8519789425139153, loss:0.5162678071810638\n",
      "W1:2.760958216751208, W2:1.2613909747375298, bias:-1.853210973619708, loss:0.5161774327171923\n",
      "W1:2.7636941779772584, W2:1.2615415581750948, bias:-1.8544419541548465, loss:0.5160871381493853\n",
      "W1:2.7664292012933718, W2:1.2616912836668317, bias:-1.8556718866838193, loss:0.5159969233539541\n",
      "W1:2.769163286618891, W2:1.2618401546209725, bias:-1.8569007737622119, loss:0.5159067882077024\n",
      "W1:2.7718964338749847, W2:1.2619881744327766, bias:-1.8581286179367407, loss:0.515816732587923\n",
      "W1:2.7746286429846405, W2:1.2621353464845826, bias:-1.859355421745288, loss:0.5157267563723954\n",
      "W1:2.777359913872658, W2:1.2622816741458598, bias:-1.8605811877169354, loss:0.5156368594393818\n",
      "W1:2.780090246465644, W2:1.2624271607732593, bias:-1.8618059183719984, loss:0.5155470416676237\n",
      "W1:2.7828196406920025, W2:1.2625718097106648, bias:-1.8630296162220596, loss:0.5154573029363396\n",
      "W1:2.7855480964819312, W2:1.2627156242892432, bias:-1.8642522837700028, loss:0.5153676431252212\n",
      "W1:2.788275613767413, W2:1.2628586078274953, bias:-1.8654739235100464, loss:0.5152780621144313\n",
      "W1:2.791002192482209, W2:1.263000763631306, bias:-1.8666945379277768, loss:0.5151885597845994\n",
      "W1:2.793727832561854, W2:1.2631420949939944, bias:-1.8679141295001818, loss:0.515099136016819\n",
      "W1:2.7964525339436483, W2:1.2632826051963635, bias:-1.8691327006956835, loss:0.515009790692646\n",
      "W1:2.799176296566651, W2:1.2634222975067495, bias:-1.8703502539741712, loss:0.5149205236940928\n",
      "W1:2.8018991203716737, W2:1.2635611751810718, bias:-1.8715667917870347, loss:0.5148313349036289\n",
      "W1:2.804621005301275, W2:1.2636992414628818, bias:-1.8727823165771964, loss:0.5147422242041748\n",
      "W1:2.8073419512997533, W2:1.263836499583412, bias:-1.8739968307791444, loss:0.5146531914791012\n",
      "W1:2.810061958313139, W2:1.2639729527616246, bias:-1.8752103368189643, loss:0.5145642366122245\n",
      "W1:2.812781026289191, W2:1.2641086042042602, bias:-1.8764228371143723, loss:0.5144753594878054\n",
      "W1:2.8154991551773874, W2:1.2642434571058863, bias:-1.8776343340747466, loss:0.514386559990545\n",
      "W1:2.81821634492892, W2:1.2643775146489447, bias:-1.8788448301011595, loss:0.5142978380055815\n",
      "W1:2.8209325954966893, W2:1.2645107800038, bias:-1.8800543275864101, loss:0.5142091934184895\n",
      "W1:2.8236479068352964, W2:1.2646432563287877, bias:-1.8812628289150548, loss:0.5141206261152749\n",
      "W1:2.826362278901037, W2:1.2647749467702607, bias:-1.88247033646344, loss:0.5140321359823729\n",
      "W1:2.829075711651896, W2:1.2649058544626373, bias:-1.883676852599733, loss:0.5139437229066455\n",
      "W1:2.8317882050475403, W2:1.2650359825284478, bias:-1.8848823796839531, loss:0.5138553867753791\n",
      "W1:2.8344997590493124, W2:1.2651653340783822, bias:-1.8860869200680037, loss:0.5137671274762805\n",
      "W1:2.837210373620225, W2:1.2652939122113358, bias:-1.8872904760957026, loss:0.5136789448974753\n",
      "W1:2.8399200487249545, W2:1.2654217200144569, bias:-1.888493050102813, loss:0.5135908389275049\n",
      "W1:2.8426287843298343, W2:1.2655487605631917, bias:-1.889694644417075, loss:0.5135028094553233\n",
      "W1:2.8453365804028485, W2:1.2656750369213317, bias:-1.8908952613582357, loss:0.5134148563702959\n",
      "W1:2.848043436913627, W2:1.265800552141059, bias:-1.8920949032380796, loss:0.513326979562195\n",
      "W1:2.8507493538334385, W2:1.2659253092629916, bias:-1.8932935723604598, loss:0.5132391789211982\n",
      "W1:2.8534543311351834, W2:1.2660493113162299, bias:-1.8944912710213273, loss:0.513151454337886\n",
      "W1:2.85615836879339, W2:1.2661725613184007, bias:-1.8956880015087623, loss:0.5130638057032391\n",
      "W1:2.858861466784206, W2:1.2662950622757037, bias:-1.8968837661030031, loss:0.512976232908635\n",
      "W1:2.8615636250853944, W2:1.2664168171829553, bias:-1.8980785670764773, loss:0.5128887358458466\n",
      "W1:2.864264843676326, W2:1.266537829023634, bias:-1.8992724066938302, loss:0.5128013144070392\n",
      "W1:2.8669651225379744, W2:1.2666581007699247, bias:-1.9004652872119556, loss:0.5127139684847679\n",
      "W1:2.86966446165291, W2:1.266777635382763, bias:-1.9016572108800252, loss:0.5126266979719754\n",
      "W1:2.8723628610052927, W2:1.266896435811879, bias:-1.9028481799395174, loss:0.5125395027619889\n",
      "W1:2.8750603205808676, W2:1.2670145049958426, bias:-1.9040381966242472, loss:0.5124523827485183\n",
      "W1:2.877756840366958, W2:1.2671318458621053, bias:-1.9052272631603953, loss:0.5123653378256544\n",
      "W1:2.88045242035246, W2:1.2672484613270456, bias:-1.9064153817665368, loss:0.5122783678878646\n",
      "W1:2.8831470605278366, W2:1.2673643542960111, bias:-1.9076025546536706, loss:0.5121914728299918\n",
      "W1:2.8858407608851113, W2:1.2674795276633626, bias:-1.9087887840252482, loss:0.5121046525472526\n",
      "W1:2.8885335214178633, W2:1.2675939843125166, bias:-1.9099740720772023, loss:0.5120179069352332\n",
      "W1:2.89122534212122, W2:1.2677077271159882, bias:-1.911158420997975, loss:0.5119312358898883\n",
      "W1:2.8939162229918534, W2:1.2678207589354342, bias:-1.912341832968547, loss:0.5118446393075389\n",
      "W1:2.896606164027972, W2:1.2679330826216952, bias:-1.9135243101624657, loss:0.5117581170848692\n",
      "W1:2.899295165229317, W2:1.2680447010148381, bias:-1.9147058547458728, loss:0.5116716691189247\n",
      "W1:2.901983226597155, W2:1.2681556169441979, bias:-1.9158864688775334, loss:0.5115852953071099\n",
      "W1:2.9046703481342733, W2:1.2682658332284198, bias:-1.9170661547088634, loss:0.5114989955471866\n",
      "W1:2.9073565298449737, W2:1.2683753526755013, bias:-1.9182449143839575, loss:0.5114127697372707\n",
      "W1:2.9100417717350675, W2:1.2684841780828333, bias:-1.919422750039617, loss:0.5113266177758299\n",
      "W1:2.9127260738118683, W2:1.2685923122372422, bias:-1.9205996638053777, loss:0.5112405395616828\n",
      "W1:2.915409436084188, W2:1.26869975791503, bias:-1.9217756578035372, loss:0.5111545349939957\n",
      "W1:2.91809185856233, W2:1.268806517882017, bias:-1.922950734149182, loss:0.5110686039722805\n",
      "W1:2.920773341258085, W2:1.2689125948935807, bias:-1.9241248949502154, loss:0.5109827463963929\n",
      "W1:2.923453884184723, W2:1.2690179916946982, bias:-1.9252981423073845, loss:0.5108969621665291\n",
      "W1:2.9261334873569904, W2:1.269122711019986, bias:-1.9264704783143072, loss:0.5108112511832265\n",
      "W1:2.928812150791103, W2:1.2692267555937409, bias:-1.9276419050574993, loss:0.5107256133473577\n",
      "W1:2.93148987450474, W2:1.269330128129979, bias:-1.9288124246164011, loss:0.5106400485601313\n",
      "W1:2.9341666585170407, W2:1.2694328313324768, bias:-1.9299820390634046, loss:0.510554556723089\n",
      "W1:2.936842502848596, W2:1.2695348678948113, bias:-1.9311507504638799, loss:0.5104691377381032\n",
      "W1:2.939517407521445, W2:1.2696362405003987, bias:-1.9323185608762012, loss:0.5103837915073753\n",
      "W1:2.9421913725590687, W2:1.2697369518225345, bias:-1.9334854723517747, loss:0.5102985179334334\n",
      "W1:2.9448643979863847, W2:1.2698370045244332, bias:-1.9346514869350635, loss:0.5102133169191301\n",
      "W1:2.9475364838297424, W2:1.2699364012592667, bias:-1.9358166066636142, loss:0.5101281883676416\n",
      "W1:2.950207630116917, W2:1.2700351446702036, bias:-1.9369808335680834, loss:0.5100431321824644\n",
      "W1:2.952877836877104, W2:1.2701332373904488, bias:-1.938144169672263, loss:0.509958148267414\n",
      "W1:2.9555471041409134, W2:1.2702306820432812, bias:-1.9393066169931068, loss:0.5098732365266226\n",
      "W1:2.9582154319403657, W2:1.2703274812420924, bias:-1.9404681775407562, loss:0.5097883968645374\n",
      "W1:2.960882820308886, W2:1.2704236375904259, bias:-1.941628853318565, loss:0.5097036291859184\n",
      "W1:2.9635492692812972, W2:1.2705191536820146, bias:-1.9427886463231265, loss:0.5096189333958374\n",
      "W1:2.966214778893818, W2:1.2706140321008188, bias:-1.9439475585442973, loss:0.5095343093996741\n",
      "W1:2.9688793491840535, W2:1.2707082754210646, bias:-1.945105591965224, loss:0.5094497571031165\n",
      "W1:2.9715429801909936, W2:1.270801886207281, bias:-1.9462627485623678, loss:0.5093652764121577\n",
      "W1:2.9742056719550054, W2:1.270894867014338, bias:-1.9474190303055297, loss:0.5092808672330938\n",
      "W1:2.9768674245178293, W2:1.2709872203874844, bias:-1.9485744391578754, loss:0.5091965294725235\n",
      "W1:2.979528237922573, W2:1.2710789488623835, bias:-1.9497289770759612, loss:0.5091122630373441\n",
      "W1:2.9821881122137066, W2:1.2711700549651521, bias:-1.9508826460097572, loss:0.509028067834752\n",
      "W1:2.9848470474370576, W2:1.2712605412123963, bias:-1.952035447902674, loss:0.5089439437722391\n",
      "W1:2.987505043639805, W2:1.2713504101112487, bias:-1.9531873846915857, loss:0.5088598907575919\n",
      "W1:2.9901621008704757, W2:1.2714396641594048, bias:-1.9543384583068553, loss:0.5087759086988893\n",
      "W1:2.9928182191789374, W2:1.2715283058451599, bias:-1.9554886706723587, loss:0.508691997504501\n",
      "W1:2.995473398616395, W2:1.2716163376474447, bias:-1.9566380237055094, loss:0.5086081570830859\n",
      "W1:2.998127639235385, W2:1.2717037620358629, bias:-1.957786519317283, loss:0.5085243873435897\n",
      "W1:3.00078094108977, W2:1.2717905814707255, bias:-1.9589341594122405, loss:0.5084406881952449\n",
      "W1:3.0034333042347345, W2:1.271876798403088, bias:-1.960080945888553, loss:0.508357059547566\n",
      "W1:3.00608472872678, W2:1.2719624152747855, bias:-1.9612268806380257, loss:0.5082735013103509\n",
      "W1:3.0087352146237176, W2:1.2720474345184682, bias:-1.9623719655461211, loss:0.5081900133936773\n",
      "W1:3.0113847619846674, W2:1.2721318585576378, bias:-1.9635162024919837, loss:0.5081065957079018\n",
      "W1:3.014033370870049, W2:1.2722156898066814, bias:-1.9646595933484625, loss:0.5080232481636583\n",
      "W1:3.016681041341579, W2:1.2722989306709078, bias:-1.965802139982136, loss:0.5079399706718557\n",
      "W1:3.0193277734622663, W2:1.272381583546582, bias:-1.9669438442533345, loss:0.5078567631436762\n",
      "W1:3.021973567296405, W2:1.2724636508209597, bias:-1.9680847080161639, loss:0.507773625490575\n",
      "W1:3.0246184229095716, W2:1.2725451348723231, bias:-1.9692247331185289, loss:0.5076905576242772\n",
      "W1:3.02726234036862, W2:1.2726260380700145, bias:-1.9703639214021564, loss:0.5076075594567768\n",
      "W1:3.0299053197416743, W2:1.2727063627744708, bias:-1.9715022747026185, loss:0.5075246309003348\n",
      "W1:3.032547361098128, W2:1.2727861113372585, bias:-1.9726397948493555, loss:0.5074417718674783\n",
      "W1:3.0351884645086344, W2:1.272865286101107, bias:-1.9737764836656981, loss:0.5073589822709978\n",
      "W1:3.037828630045106, W2:1.272943889399943, bias:-1.9749123429688917, loss:0.5072762620239468\n",
      "W1:3.040467857780707, W2:1.2730219235589244, bias:-1.976047374570118, loss:0.5071936110396394\n",
      "W1:3.0431061477898487, W2:1.273099390894474, bias:-1.9771815802745176, loss:0.5071110292316496\n",
      "W1:3.0457435001481867, W2:1.2731762937143132, bias:-1.9783149618812133, loss:0.5070285165138082\n",
      "W1:3.048379914932614, W2:1.2732526343174946, bias:-1.9794475211833318, loss:0.506946072800203\n",
      "W1:3.051015392221257, W2:1.2733284149944366, bias:-1.9805792599680263, loss:0.5068636980051763\n",
      "W1:3.0536499320934705, W2:1.2734036380269556, bias:-1.9817101800164993, loss:0.5067813920433241\n",
      "W1:3.0562835346298343, W2:1.2734783056882995, bias:-1.982840283104024, loss:0.5066991548294932\n",
      "W1:3.0589161999121464, W2:1.2735524202431805, bias:-1.9839695709999663, loss:0.5066169862787816\n",
      "W1:3.0615479280234195, W2:1.2736259839478077, bias:-1.9850980454678078, loss:0.5065348863065361\n",
      "W1:3.0641787190478764, W2:1.27369899904992, bias:-1.9862257082651669, loss:0.5064528548283501\n",
      "W1:3.0668085730709453, W2:1.2737714677888181, bias:-1.9873525611438208, loss:0.5063708917600636\n",
      "W1:3.069437490179255, W2:1.2738433923953976, bias:-1.988478605849727, loss:0.5062889970177603\n",
      "W1:3.07206547046063, W2:1.2739147750921804, bias:-1.9896038441230457, loss:0.506207170517768\n",
      "W1:3.0746925140040857, W2:1.2739856180933473, bias:-1.9907282776981603, loss:0.5061254121766549\n",
      "W1:3.0773186208998253, W2:1.27405592360477, bias:-1.9918519083036998, loss:0.50604372191123\n",
      "W1:3.0799437912392333, W2:1.2741256938240424, bias:-1.99297473766256, loss:0.5059620996385412\n",
      "W1:3.082568025114872, W2:1.2741949309405132, bias:-1.9940967674919239, loss:0.5058805452758736\n",
      "W1:3.085191322620477, W2:1.2742636371353164, bias:-1.9952179995032844, loss:0.505799058740748\n",
      "W1:3.087813683850952, W2:1.2743318145814038, bias:-1.9963384354024643, loss:0.5057176399509202\n",
      "W1:3.0904351089023647, W2:1.274399465443575, bias:-1.9974580768896375, loss:0.5056362888243789\n",
      "W1:3.0930555978719427, W2:1.27446659187851, bias:-1.9985769256593506, loss:0.5055550052793455\n",
      "W1:3.095675150858068, W2:1.2745331960347994, bias:-1.9996949834005433, loss:0.505473789234271\n",
      "W1:3.0982937679602744, W2:1.2745992800529753, bias:-2.0008122517965687, loss:0.5053926406078361\n",
      "W1:3.10091144927924, W2:1.2746648460655423, bias:-2.0019287325252155, loss:0.5053115593189493\n",
      "W1:3.1035281949167857, W2:1.2747298961970086, bias:-2.0030444272587267, loss:0.5052305452867458\n",
      "W1:3.1061440049758686, W2:1.2747944325639156, bias:-2.004159337663822, loss:0.5051495984305862\n",
      "W1:3.10875887956058, W2:1.2748584572748691, bias:-2.005273465401716, loss:0.5050687186700543\n",
      "W1:3.111372818776138, W2:1.2749219724305694, bias:-2.006386812128142, loss:0.5049879059249578\n",
      "W1:3.1139858227288855, W2:1.274984980123841, bias:-2.0074993794933684, loss:0.5049071601153251\n",
      "W1:3.116597891526285, W2:1.2750474824396636, bias:-2.0086111691422217, loss:0.5048264811614046\n",
      "W1:3.119209025276914, W2:1.2751094814552009, bias:-2.0097221827141056, loss:0.5047458689836639\n",
      "W1:3.121819224090461, W2:1.2751709792398314, bias:-2.0108324218430216, loss:0.5046653235027883\n",
      "W1:3.1244284880777218, W2:1.2752319778551773, bias:-2.0119418881575872, loss:0.5045848446396792\n",
      "W1:3.1270368173505925, W2:1.2752924793551346, bias:-2.013050583281059, loss:0.5045044323154537\n",
      "W1:3.129644212022069, W2:1.2753524857859024, bias:-2.0141585088313487, loss:0.504424086451442\n",
      "W1:3.1322506722062404, W2:1.275411999186012, bias:-2.0152656664210467, loss:0.5043438069691879\n",
      "W1:3.134856198018285, W2:1.2754710215863563, bias:-2.0163720576574384, loss:0.5042635937904462\n",
      "W1:3.137460789574466, W2:1.275529555010219, bias:-2.017477684142526, loss:0.5041834468371816\n",
      "W1:3.1400644469921284, W2:1.2755876014733032, bias:-2.018582547473047, loss:0.5041033660315691\n",
      "W1:3.1426671703896925, W2:1.2756451629837604, bias:-2.0196866492404943, loss:0.5040233512959906\n",
      "W1:3.145268959886652, W2:1.2757022415422197, bias:-2.0207899910311347, loss:0.503943402553035\n",
      "W1:3.147869815603569, W2:1.2757588391418155, bias:-2.0218925744260283, loss:0.5038635197254974\n",
      "W1:3.1504697376620694, W2:1.2758149577682167, bias:-2.0229944010010485, loss:0.5037837027363762\n",
      "W1:3.1530687261848387, W2:1.2758705993996549, bias:-2.024095472326901, loss:0.5037039515088743\n",
      "W1:3.155666781295618, W2:1.2759257660069523, bias:-2.0251957899691417, loss:0.5036242659663959\n",
      "W1:3.158263903119201, W2:1.2759804595535507, bias:-2.026295355488197, loss:0.5035446460325462\n",
      "W1:3.1608600917814282, W2:1.2760346819955386, bias:-2.0273941704393814, loss:0.5034650916311313\n",
      "W1:3.163455347409183, W2:1.2760884352816797, bias:-2.0284922363729176, loss:0.5033856026861546\n",
      "W1:3.166049670130389, W2:1.2761417213534405, bias:-2.029589554833955, loss:0.5033061791218184\n",
      "W1:3.1686430600740043, W2:1.276194542145018, bias:-2.0306861273625874, loss:0.5032268208625208\n",
      "W1:3.171235517370018, W2:1.2762468995833673, bias:-2.0317819554938725, loss:0.5031475278328557\n",
      "W1:3.173827042149447, W2:1.2762987955882292, bias:-2.03287704075785, loss:0.5030682999576114\n",
      "W1:3.1764176345443302, W2:1.2763502320721574, bias:-2.033971384679561, loss:0.5029891371617691\n",
      "W1:3.179007294687726, W2:1.2764012109405458, bias:-2.035064988779064, loss:0.5029100393705025\n",
      "W1:3.1815960227137077, W2:1.2764517340916555, bias:-2.036157854571457, loss:0.5028310065091768\n",
      "W1:3.184183818757359, W2:1.2765018034166422, bias:-2.0372499835668916, loss:0.5027520385033468\n",
      "W1:3.1867706829547706, W2:1.2765514207995827, bias:-2.0383413772705943, loss:0.5026731352787566\n",
      "W1:3.1893566154430366, W2:1.276600588117502, bias:-2.0394320371828827, loss:0.5025942967613379\n",
      "W1:3.19194161636025, W2:1.2766493072403997, bias:-2.0405219647991846, loss:0.5025155228772101\n",
      "W1:3.1945256858454982, W2:1.2766975800312772, bias:-2.0416111616100556, loss:0.5024368135526779\n",
      "W1:3.1971088240388603, W2:1.2767454083461636, bias:-2.0426996291011963, loss:0.5023581687142316\n",
      "W1:3.199691031081402, W2:1.2767927940341421, bias:-2.0437873687534713, loss:0.5022795882885445\n",
      "W1:3.2022723071151726, W2:1.276839738937377, bias:-2.0448743820429263, loss:0.5022010722024733\n",
      "W1:3.2048526522832006, W2:1.2768862448911384, bias:-2.0459606704408055, loss:0.5021226203830571\n",
      "W1:3.2074320667294893, W2:1.27693231372383, bias:-2.04704623541357, loss:0.502044232757515\n",
      "W1:3.2100105505990144, W2:1.276977947257014, bias:-2.048131078422914, loss:0.5019659092532467\n",
      "W1:3.212588104037719, W2:1.2770231473054372, bias:-2.0492152009257842, loss:0.5018876497978307\n",
      "W1:3.21516472719251, W2:1.2770679156770566, bias:-2.050298604374395, loss:0.5018094543190231\n",
      "W1:3.2177404202112534, W2:1.2771122541730655, bias:-2.051381290216248, loss:0.5017313227447576\n",
      "W1:3.2203151832427728, W2:1.2771561645879186, bias:-2.0524632598941475, loss:0.5016532550031436\n",
      "W1:3.222889016436843, W2:1.2771996487093575, bias:-2.0535445148462177, loss:0.5015752510224658\n",
      "W1:3.2254619199441876, W2:1.2772427083184366, bias:-2.054625056505922, loss:0.501497310731183\n",
      "W1:3.2280338939164754, W2:1.2772853451895472, bias:-2.055704886302077, loss:0.501419434057927\n",
      "W1:3.2306049385063154, W2:1.2773275610904438, bias:-2.056784005658872, loss:0.5013416209315023\n",
      "W1:3.2331750538672543, W2:1.2773693577822685, bias:-2.0578624159958845, loss:0.5012638712808845\n",
      "W1:3.2357442401537724, W2:1.277410737019576, bias:-2.0589401187280973, loss:0.5011861850352195\n",
      "W1:3.238312497521279, W2:1.277451700550359, bias:-2.0600171152659157, loss:0.5011085621238236\n",
      "W1:3.2408798261261094, W2:1.2774922501160717, bias:-2.061093407015184, loss:0.5010310024761804\n",
      "W1:3.243446226125522, W2:1.277532387451656, bias:-2.0621689953772013, loss:0.5009535060219424\n",
      "W1:3.246011697677693, W2:1.277572114285565, bias:-2.06324388174874, loss:0.5008760726909283\n",
      "W1:3.248576240941714, W2:1.2776114323397874, bias:-2.0643180675220605, loss:0.5007987024131229\n",
      "W1:3.2511398560775873, W2:1.2776503433298723, bias:-2.0653915540849286, loss:0.5007213951186763\n",
      "W1:3.2537025432462228, W2:1.2776888489649536, bias:-2.0664643428206313, loss:0.5006441507379024\n",
      "W1:3.2562643026094342, W2:1.2777269509477736, bias:-2.0675364351079937, loss:0.5005669692012785\n",
      "W1:3.2588251343299355, W2:1.277764650974707, bias:-2.0686078323213954, loss:0.5004898504394452\n",
      "W1:3.2613850385713374, W2:1.2778019507357856, bias:-2.069678535830786, loss:0.5004127943832033\n",
      "W1:3.2639440154981436, W2:1.277838851914722, bias:-2.0707485470017013, loss:0.5003358009635153\n",
      "W1:3.2665020652757466, W2:1.2778753561889322, bias:-2.07181786719528, loss:0.5002588701115034\n",
      "W1:3.2690591880704254, W2:1.277911465229561, bias:-2.072886497768279, loss:0.5001820017584491\n",
      "W1:3.2716153840493405, W2:1.2779471807015041, bias:-2.0739544400730905, loss:0.5001051958357915\n",
      "W1:3.2741706533805317, W2:1.277982504263433, bias:-2.0750216954577563, loss:0.5000284522751278\n",
      "W1:3.2767249962329132, W2:1.2780174375678166, bias:-2.0760882652659847, loss:0.4999517710082117\n",
      "W1:3.279278412776271, W2:1.2780519822609466, bias:-2.077154150837166, loss:0.49987515196695215\n",
      "W1:3.2818309031812594, W2:1.278086139982959, bias:-2.0782193535063884, loss:0.4997985950834135\n",
      "W1:3.2843824676193965, W2:1.278119912367858, bias:-2.079283874604453, loss:0.4997221002898144\n",
      "W1:3.286933106263062, W2:1.2781533010435386, bias:-2.08034771545789, loss:0.49964566751852646\n",
      "W1:3.289482819285492, W2:1.2781863076318105, bias:-2.0814108773889735, loss:0.4995692967020739\n",
      "W1:3.2920316068607773, W2:1.2782189337484196, bias:-2.0824733617157385, loss:0.49949298777313317\n",
      "W1:3.2945794691638595, W2:1.2782511810030717, bias:-2.0835351697519946, loss:0.49941674066453123\n",
      "W1:3.297126406370526, W2:1.2782830509994547, bias:-2.084596302807342, loss:0.4993405553092455\n",
      "W1:3.2996724186574093, W2:1.2783145453352616, bias:-2.0856567621871873, loss:0.49926443164040274\n",
      "W1:3.3022175062019805, W2:1.278345665602212, bias:-2.086716549192758, loss:0.4991883695912786\n",
      "W1:3.304761669182548, W2:1.2783764133860764, bias:-2.087775665121117, loss:0.49911236909529627\n",
      "W1:3.3073049077782537, W2:1.2784067902666962, bias:-2.08883411126518, loss:0.4990364300860263\n",
      "W1:3.309847222169069, W2:1.2784367978180073, bias:-2.089891888913728, loss:0.498960552497186\n",
      "W1:3.3123886125357913, W2:1.2784664376080623, bias:-2.0909489993514243, loss:0.4988847362626377\n",
      "W1:3.3149290790600423, W2:1.278495711199052, bias:-2.092005443858828, loss:0.4988089813163892\n",
      "W1:3.317468621924262, W2:1.2785246201473273, bias:-2.093061223712409, loss:0.4987332875925919\n",
      "W1:3.3200072413117074, W2:1.2785531660034215, bias:-2.0941163401845646, loss:0.49865765502554155\n",
      "W1:3.322544937406448, W2:1.278581350312072, bias:-2.0951707945436313, loss:0.49858208354967554\n",
      "W1:3.325081710393364, W2:1.2786091746122414, bias:-2.096224588053902, loss:0.4985065730995743\n",
      "W1:3.3276175604581404, W2:1.2786366404371403, bias:-2.097277721975639, loss:0.4984311236099588\n",
      "W1:3.330152487787266, W2:1.2786637493142474, bias:-2.09833019756509, loss:0.4983557350156911\n",
      "W1:3.3326864925680293, W2:1.2786905027653321, bias:-2.0993820160745016, loss:0.49828040725177264\n",
      "W1:3.3352195749885145, W2:1.2787169023064753, bias:-2.100433178752134, loss:0.49820514025334467\n",
      "W1:3.337751735237599, W2:1.2787429494480904, bias:-2.101483686842276, loss:0.4981299339556862\n",
      "W1:3.340282973504951, W2:1.2787686456949454, bias:-2.1025335415852577, loss:0.49805478829421457\n",
      "W1:3.342813289981024, W2:1.278793992546183, bias:-2.1035827442174675, loss:0.49797970320448426\n",
      "W1:3.345342684857055, W2:1.2788189914953425, bias:-2.1046312959713633, loss:0.4979046786221856\n",
      "W1:3.347871158325061, W2:1.2788436440303799, bias:-2.105679198075489, loss:0.49782971448314517\n",
      "W1:3.3503987105778363, W2:1.2788679516336892, bias:-2.1067264517544886, loss:0.49775481072332456\n",
      "W1:3.352925341808948, W2:1.2788919157821232, bias:-2.107773058229118, loss:0.49767996727881963\n",
      "W1:3.355451052212734, W2:1.2789155379470143, bias:-2.108819018716261, loss:0.4976051840858599\n",
      "W1:3.3579758419842993, W2:1.2789388195941946, bias:-2.109864334428943, loss:0.49753046108080823\n",
      "W1:3.3604997113195125, W2:1.2789617621840168, bias:-2.1109090065763456, loss:0.4974557982001598\n",
      "W1:3.363022660415003, W2:1.278984367171375, bias:-2.111953036363818, loss:0.4973811953805416\n",
      "W1:3.3655446894681584, W2:1.279006636005724, bias:-2.112996424992893, loss:0.49730665255871115\n",
      "W1:3.36806579867712, W2:1.2790285701311008, bias:-2.1140391736613013, loss:0.4972321696715575\n",
      "W1:3.3705859882407805, W2:1.279050170986144, bias:-2.115081283562983, loss:0.49715774665609874\n",
      "W1:3.373105258358781, W2:1.279071440004114, bias:-2.116122755888102, loss:0.4970833834494828\n",
      "W1:3.375623609231507, W2:1.279092378612914, bias:-2.117163591823061, loss:0.49700907998898564\n",
      "W1:3.3781410410600867, W2:1.2791129882351084, bias:-2.118203792550513, loss:0.4969348362120111\n",
      "W1:3.3806575540463863, W2:1.2791332702879437, bias:-2.119243359249377, loss:0.49686065205609115\n",
      "W1:3.383173148393008, W2:1.279153226183368, bias:-2.1202822930948493, loss:0.4967865274588839\n",
      "W1:3.3856878243032864, W2:1.2791728573280514, bias:-2.121320595258418, loss:0.4967124623581736\n",
      "W1:3.3882015819812863, W2:1.2791921651234044, bias:-2.1223582669078764, loss:0.49663845669187007\n",
      "W1:3.390714421631798, W2:1.2792111509655986, bias:-2.123395309207336, loss:0.49656451039800786\n",
      "W1:3.393226343460335, W2:1.2792298162455857, bias:-2.12443172331724, loss:0.49649062341474615\n",
      "W1:3.3957373476731325, W2:1.279248162349117, bias:-2.125467510394375, loss:0.496416795680368\n",
      "W1:3.3982474344771414, W2:1.2792661906567628, bias:-2.126502671591888, loss:0.4963430271332788\n",
      "W1:3.400756604080028, W2:1.2792839025439318, bias:-2.127537208059295, loss:0.49626931771200716\n",
      "W1:3.403264856690169, W2:1.2793012993808897, bias:-2.128571120942496, loss:0.4961956673552036\n",
      "W1:3.40577219251665, W2:1.2793183825327792, bias:-2.1296044113837893, loss:0.4961220760016394\n",
      "W1:3.408278611769261, W2:1.2793351533596387, bias:-2.1306370805218817, loss:0.4960485435902074\n",
      "W1:3.410784114658495, W2:1.2793516132164209, bias:-2.1316691294919035, loss:0.4959750700599203\n",
      "W1:3.413288701395544, W2:1.2793677634530123, bias:-2.1327005594254205, loss:0.4959016553499105\n",
      "W1:3.415792372192296, W2:1.2793836054142516, bias:-2.133731371450447, loss:0.49582829939942935\n",
      "W1:3.4182951272613322, W2:1.2793991404399485, bias:-2.1347615666914583, loss:0.49575500214784685\n",
      "W1:3.4207969668159253, W2:1.2794143698649028, bias:-2.135791146269404, loss:0.4956817635346511\n",
      "W1:3.4232978910700336, W2:1.2794292950189223, bias:-2.1368201113017196, loss:0.49560858349944736\n",
      "W1:3.4257979002383014, W2:1.2794439172268421, bias:-2.13784846290234, loss:0.49553546198195786\n",
      "W1:3.428296994536054, W2:1.2794582378085428, bias:-2.1388762021817116, loss:0.4954623989220215\n",
      "W1:3.4307951741792952, W2:1.2794722580789686, bias:-2.1399033302468053, loss:0.49538939425959244\n",
      "W1:3.4332924393847053, W2:1.2794859793481457, bias:-2.1409298482011287, loss:0.4953164479347404\n",
      "W1:3.4357887903696365, W2:1.279499402921201, bias:-2.141955757144738, loss:0.4952435598876496\n",
      "W1:3.4382842273521113, W2:1.27951253009838, bias:-2.1429810581742506, loss:0.49517073005861856\n",
      "W1:3.440778750550819, W2:1.2795253621750646, bias:-2.144005752382858, loss:0.49509795838805976\n",
      "W1:3.443272360185114, W2:1.2795379004417915, bias:-2.145029840860338, loss:0.4950252448164981\n",
      "W1:3.4457650564750115, W2:1.2795501461842702, bias:-2.1460533246930655, loss:0.49495258928457186\n",
      "W1:3.4482568396411852, W2:1.2795621006834008, bias:-2.147076204964027, loss:0.49487999173303093\n",
      "W1:3.4507477099049644, W2:1.2795737652152919, bias:-2.14809848275283, loss:0.49480745210273686\n",
      "W1:3.453237667488332, W2:1.2795851410512782, bias:-2.1491201591357183, loss:0.49473497033466246\n",
      "W1:3.4557267126139193, W2:1.2795962294579382, bias:-2.150141235185581, loss:0.49466254636989093\n",
      "W1:3.458214845505007, W2:1.279607031697112, bias:-2.1511617119719655, loss:0.4945901801496156\n",
      "W1:3.4607020663855184, W2:1.279617549025919, bias:-2.1521815905610913, loss:0.4945178716151392\n",
      "W1:3.46318837548002, W2:1.2796277826967748, bias:-2.1532008720158586, loss:0.4944456207078742\n",
      "W1:3.465673773013716, W2:1.2796377339574092, bias:-2.1542195573958627, loss:0.4943734273693403\n",
      "W1:3.468158259212447, W2:1.2796474040508832, bias:-2.1552376477574047, loss:0.4943012915411668\n",
      "W1:3.470641834302687, W2:1.2796567942156067, bias:-2.156255144153504, loss:0.4942292131650898\n",
      "W1:3.4731244985115404, W2:1.2796659056853552, bias:-2.1572720476339087, loss:0.4941571921829524\n",
      "W1:3.47560625206674, W2:1.2796747396892874, bias:-2.1582883592451085, loss:0.4940852285367047\n",
      "W1:3.478087095196643, W2:1.2796832974519619, bias:-2.1593040800303465, loss:0.4940133221684027\n",
      "W1:3.4805670281302294, W2:1.2796915801933544, bias:-2.160319211029629, loss:0.4939414730202088\n",
      "W1:3.4830460510970984, W2:1.279699589128875, bias:-2.1613337532797394, loss:0.4938696810343892\n",
      "W1:3.485524164327467, W2:1.2797073254693845, bias:-2.1623477078142477, loss:0.49379794615331635\n",
      "W1:3.4880013680521653, W2:1.2797147904212114, bias:-2.163361075663523, loss:0.49372626831946603\n",
      "W1:3.4904776625026357, W2:1.279721985186169, bias:-2.1643738578547453, loss:0.49365464747541793\n",
      "W1:3.4929530479109294, W2:1.2797289109615715, bias:-2.1653860554119153, loss:0.493583083563856\n",
      "W1:3.4954275245097035, W2:1.2797355689402514, bias:-2.166397669355867, loss:0.4935115765275656\n",
      "W1:3.497901092532219, W2:1.2797419603105755, bias:-2.1674087007042795, loss:0.4934401263094362\n",
      "W1:3.5003737522123375, W2:1.2797480862564614, bias:-2.1684191504716863, loss:0.4933687328524577\n",
      "W1:3.5028455037845188, W2:1.2797539479573943, bias:-2.169429019669488, loss:0.49329739609972295\n",
      "W1:3.5053163474838183, W2:1.2797595465884428, bias:-2.170438309305963, loss:0.4932261159944247\n",
      "W1:3.5077862835458844, W2:1.279764883320276, bias:-2.171447020386279, loss:0.49315489247985766\n",
      "W1:3.5102553122069557, W2:1.2797699593191791, bias:-2.172455153912503, loss:0.49308372549941537\n",
      "W1:3.512723433703859, W2:1.2797747757470699, bias:-2.173462710883614, loss:0.4930126149965925\n",
      "W1:3.515190648274005, W2:1.2797793337615149, bias:-2.174469692295512, loss:0.49294156091498215\n",
      "W1:3.5176569561553888, W2:1.2797836345157452, bias:-2.1754760991410302, loss:0.4928705631982771\n",
      "W1:3.5201223575865837, W2:1.2797876791586724, bias:-2.1764819324099465, loss:0.4927996217902679\n",
      "W1:3.5225868528067408, W2:1.2797914688349052, bias:-2.1774871930889925, loss:0.49272873663484396\n",
      "W1:3.5250504420555866, W2:1.2797950046847648, bias:-2.178491882161866, loss:0.49265790767599177\n",
      "W1:3.527513125573419, W2:1.2797982878443004, bias:-2.179496000609241, loss:0.49258713485779526\n",
      "W1:3.5299749036011066, W2:1.2798013194453055, bias:-2.180499549408778, loss:0.49251641812443564\n",
      "W1:3.5324357763800838, W2:1.2798041006153333, bias:-2.1815025295351362, loss:0.49244575742018976\n",
      "W1:3.5348957441523505, W2:1.2798066324777129, bias:-2.1825049419599822, loss:0.49237515268943144\n",
      "W1:3.5373548071604684, W2:1.2798089161515638, bias:-2.1835067876520022, loss:0.4923046038766291\n",
      "W1:3.5398129656475583, W2:1.2798109527518124, bias:-2.1845080675769117, loss:0.4922341109263472\n",
      "W1:3.542270219857299, W2:1.279812743389207, bias:-2.185508782697467, loss:0.4921636737832447\n",
      "W1:3.5447265700339226, W2:1.2798142891703335, bias:-2.186508933973474, loss:0.4920932923920752\n",
      "W1:3.547182016422214, W2:1.27981559119763, bias:-2.1875085223618007, loss:0.4920229666976857\n",
      "W1:3.5496365592675074, W2:1.2798166505694035, bias:-2.188507548816386, loss:0.49195269664501756\n",
      "W1:3.552090198815684, W2:1.2798174683798433, bias:-2.1895060142882508, loss:0.49188248217910496\n",
      "W1:3.5545429353131697, W2:1.2798180457190378, bias:-2.1905039197255083, loss:0.4918123232450751\n",
      "W1:3.5569947690069323, W2:1.2798183836729884, bias:-2.1915012660733737, loss:0.49174221978814764\n",
      "W1:3.5594457001444795, W2:1.2798184833236255, bias:-2.192498054274176, loss:0.4916721717536341\n",
      "W1:3.5618957289738558, W2:1.279818345748823, bias:-2.193494285267366, loss:0.49160217908693843\n",
      "W1:3.564344855743641, W2:1.279817972022413, bias:-2.1944899599895287, loss:0.49153224173355453\n",
      "W1:3.5667930807029467, W2:1.2798173632142014, bias:-2.1954850793743916, loss:0.49146235963906865\n",
      "W1:3.5692404041014147, W2:1.2798165203899818, bias:-2.1964796443528365, loss:0.491392532749157\n",
      "W1:3.5716868261892145, W2:1.279815444611551, bias:-2.197473655852908, loss:0.49132276100958605\n",
      "W1:3.5741323472170405, W2:1.2798141369367237, bias:-2.1984671147998243, loss:0.4912530443662119\n",
      "W1:3.5765769674361096, W2:1.2798125984193467, bias:-2.1994600221159875, loss:0.4911833827649806\n",
      "W1:3.5790206870981587, W2:1.2798108301093134, bias:-2.200452378720993, loss:0.49111377615192686\n",
      "W1:3.5814635064554436, W2:1.2798088330525792, bias:-2.20144418553164, loss:0.4910442244731742\n",
      "W1:3.583905425760735, W2:1.279806608291175, bias:-2.202435443461939, loss:0.49097472767493483\n",
      "W1:3.586346445267316, W2:1.2798041568632224, bias:-2.203426153423127, loss:0.4909052857035087\n",
      "W1:3.5887865652289825, W2:1.2798014798029476, bias:-2.204416316323671, loss:0.49083589850528353\n",
      "W1:3.591225785900037, W2:1.2797985781406958, bias:-2.205405933069282, loss:0.4907665660267347\n",
      "W1:3.593664107535289, W2:1.2797954529029454, bias:-2.2063950045629235, loss:0.49069728821442365\n",
      "W1:3.5961015303900514, W2:1.2797921051123229, bias:-2.2073835317048207, loss:0.49062806501499945\n",
      "W1:3.5985380547201387, W2:1.2797885357876158, bias:-2.208371515392471, loss:0.49055889637519684\n",
      "W1:3.600973680781865, W2:1.2797847459437877, bias:-2.209358956520653, loss:0.4904897822418368\n",
      "W1:3.6034084088320397, W2:1.2797807365919922, bias:-2.2103458559814357, loss:0.49042072256182573\n",
      "W1:3.6058422391279685, W2:1.2797765087395865, bias:-2.21133221466419, loss:0.4903517172821554\n",
      "W1:3.608275171927448, W2:1.2797720633901455, bias:-2.2123180334555963, loss:0.4902827663499025\n",
      "W1:3.610707207488766, W2:1.2797674015434761, bias:-2.213303313239654, loss:0.49021386971222825\n",
      "W1:3.6131383460706963, W2:1.2797625241956305, bias:-2.214288054897692, loss:0.4901450273163782\n",
      "W1:3.6155685879324992, W2:1.2797574323389205, bias:-2.2152722593083776, loss:0.4900762391096814\n",
      "W1:3.6179979333339176, W2:1.2797521269619303, bias:-2.2162559273477256, loss:0.4900075050395514\n",
      "W1:3.6204263825351757, W2:1.2797466090495313, bias:-2.2172390598891085, loss:0.48993882505348446\n",
      "W1:3.6228539357969756, W2:1.279740879582895, bias:-2.218221657803265, loss:0.4898701990990595\n",
      "W1:3.625280593380496, W2:1.2797349395395072, bias:-2.2192037219583094, loss:0.48980162712393865\n",
      "W1:3.6277063555473896, W2:1.2797287898931806, bias:-2.220185253219741, loss:0.4897331090758658\n",
      "W1:3.6301312225597813, W2:1.2797224316140692, bias:-2.221166252450453, loss:0.48966464490266776\n",
      "W1:3.632555194680265, W2:1.2797158656686811, bias:-2.222146720510743, loss:0.4895962345522518\n",
      "W1:3.634978272171903, W2:1.2797090930198922, bias:-2.223126658258319, loss:0.4895278779726075\n",
      "W1:3.6374004552982218, W2:1.2797021146269596, bias:-2.224106066548313, loss:0.4894595751118047\n",
      "W1:3.639821744323211, W2:1.2796949314455344, bias:-2.225084946233286, loss:0.4893913259179951\n",
      "W1:3.642242139511321, W2:1.2796875444276752, bias:-2.2260632981632384, loss:0.4893231303394097\n",
      "W1:3.6446616411274615, W2:1.2796799545218616, bias:-2.2270411231856206, loss:0.48925498832435993\n",
      "W1:3.647080249436998, W2:1.2796721626730065, bias:-2.2280184221453396, loss:0.4891868998212377\n",
      "W1:3.64949796470575, W2:1.2796641698224704, bias:-2.2289951958847696, loss:0.4891188647785136\n",
      "W1:3.65191478719999, W2:1.2796559769080733, bias:-2.2299714452437596, loss:0.4890508831447379\n",
      "W1:3.6543307171864394, W2:1.2796475848641078, bias:-2.2309471710596434, loss:0.48898295486853977\n",
      "W1:3.656745754932268, W2:1.2796389946213527, bias:-2.2319223741672474, loss:0.48891507989862676\n",
      "W1:3.6591599007050912, W2:1.2796302071070853, bias:-2.232897055398901, loss:0.48884725818378505\n",
      "W1:3.6615731547729675, W2:1.2796212232450945, bias:-2.233871215584443, loss:0.4887794896728787\n",
      "W1:3.663985517404397, W2:1.2796120439556935, bias:-2.234844855551232, loss:0.48871177431484975\n",
      "W1:3.666396988868319, W2:1.2796026701557324, bias:-2.2358179761241552, loss:0.4886441120587174\n",
      "W1:3.66880756943411, W2:1.2795931027586107, bias:-2.2367905781256368, loss:0.48857650285357823\n",
      "W1:3.6712172593715806, W2:1.2795833426742909, bias:-2.2377626623756455, loss:0.48850894664860556\n",
      "W1:3.673626058950976, W2:1.2795733908093097, bias:-2.2387342296917048, loss:0.4884414433930493\n",
      "W1:3.6760339684429715, W2:1.2795632480667918, bias:-2.2397052808889004, loss:0.488373993036236\n",
      "W1:3.67844098811867, W2:1.2795529153464613, bias:-2.2406758167798895, loss:0.4883065955275683\n",
      "W1:3.6808471182496025, W2:1.2795423935446553, bias:-2.2416458381749083, loss:0.4882392508165239\n",
      "W1:3.6832523591077235, W2:1.2795316835543349, bias:-2.242615345881782, loss:0.4881719588526568\n",
      "W1:3.685656710965411, W2:1.279520786265099, bias:-2.243584340705932, loss:0.4881047195855958\n",
      "W1:3.688060174095462, W2:1.2795097025631958, bias:-2.2445528234503844, loss:0.48803753296504476\n",
      "W1:3.6904627487710937, W2:1.279498433331535, bias:-2.2455207949157794, loss:0.48797039894078237\n",
      "W1:3.692864435265938, W2:1.2794869794497, bias:-2.246488255900378, loss:0.48790331746266147\n",
      "W1:3.695265233854042, W2:1.2794753417939604, bias:-2.247455207200072, loss:0.48783628848060895\n",
      "W1:3.6976651448098643, W2:1.2794635212372845, bias:-2.2484216496083915, loss:0.48776931194462614\n",
      "W1:3.7000641684082747, W2:1.2794515186493503, bias:-2.249387583916513, loss:0.4877023878047876\n",
      "W1:3.70246230492455, W2:1.279439334896558, bias:-2.2503530109132672, loss:0.4876355160112411\n",
      "W1:3.7048595546343743, W2:1.2794269708420425, bias:-2.2513179313851492, loss:0.48756869651420787\n",
      "W1:3.7072559178138347, W2:1.2794144273456847, bias:-2.252282346116324, loss:0.48750192926398184\n",
      "W1:3.7096513947394216, W2:1.2794017052641233, bias:-2.2532462558886373, loss:0.48743521421092934\n",
      "W1:3.712045985688025, W2:1.2793888054507674, bias:-2.2542096614816205, loss:0.4873685513054891\n",
      "W1:3.7144396909369326, W2:1.2793757287558076, bias:-2.255172563672502, loss:0.487301940498172\n",
      "W1:3.7168325107638296, W2:1.2793624760262283, bias:-2.2561349632362133, loss:0.4872353817395609\n",
      "W1:3.719224445446794, W2:1.2793490481058185, bias:-2.2570968609453974, loss:0.4871688749803097\n",
      "W1:3.721615495264297, W2:1.2793354458351849, bias:-2.2580582575704167, loss:0.4871024201711441\n",
      "W1:3.7240056604951994, W2:1.279321670051762, bias:-2.259019153879361, loss:0.48703601726286094\n",
      "W1:3.726394941418751, W2:1.2793077215898248, bias:-2.2599795506380564, loss:0.48696966620632715\n",
      "W1:3.7287833383145874, W2:1.2792936012804998, bias:-2.260939448610072, loss:0.48690336695248126\n",
      "W1:3.7311708514627293, W2:1.279279309951777, bias:-2.261898848556727, loss:0.48683711945233127\n",
      "W1:3.733557481143579, W2:1.2792648484285203, bias:-2.2628577512371013, loss:0.4867709236569558\n",
      "W1:3.73594322763792, W2:1.2792502175324802, bias:-2.263816157408041, loss:0.48670477951750263\n",
      "W1:3.738328091226914, W2:1.2792354180823045, bias:-2.264774067824167, loss:0.4866386869851903\n",
      "W1:3.7407120721921006, W2:1.2792204508935494, bias:-2.265731483237882, loss:0.48657264601130573\n",
      "W1:3.7430951708153923, W2:1.2792053167786914, bias:-2.266688404399379, loss:0.48650665654720543\n",
      "W1:3.745477387379076, W2:1.2791900165471382, bias:-2.2676448320566505, loss:0.4864407185443145\n",
      "W1:3.7478587221658093, W2:1.2791745510052397, bias:-2.268600766955492, loss:0.48637483195412695\n",
      "W1:3.7502391754586184, W2:1.2791589209562997, bias:-2.269556209839514, loss:0.48630899672820527\n",
      "W1:3.752618747540897, W2:1.2791431272005864, bias:-2.2705111614501465, loss:0.48624321281818017\n",
      "W1:3.754997438696405, W2:1.279127170535344, bias:-2.271465622526649, loss:0.4861774801757502\n",
      "W1:3.757375249209264, W2:1.2791110517548034, bias:-2.272419593806116, loss:0.4861117987526811\n",
      "W1:3.7597521793639586, W2:1.2790947716501935, bias:-2.2733730760234856, loss:0.48604616850080756\n",
      "W1:3.7621282294453327, W2:1.2790783310097518, bias:-2.2743260699115475, loss:0.4859805893720305\n",
      "W1:3.7645033997385884, W2:1.2790617306187353, bias:-2.275278576200949, loss:0.4859150613183181\n",
      "W1:3.766877690529283, W2:1.2790449712594318, bias:-2.2762305956202034, loss:0.48584958429170527\n",
      "W1:3.7692511021033295, W2:1.2790280537111702, bias:-2.277182128895698, loss:0.48578415824429394\n",
      "W1:3.771623634746992, W2:1.2790109787503319, bias:-2.2781331767517, loss:0.4857187831282521\n",
      "W1:3.773995288746885, W2:1.278993747150361, bias:-2.2790837399103645, loss:0.48565345889581424\n",
      "W1:3.7763660643899724, W2:1.2789763596817751, bias:-2.2800338190917424, loss:0.4855881854992808\n",
      "W1:3.778735961963565, W2:1.2789588171121762, bias:-2.2809834150137873, loss:0.4855229628910177\n",
      "W1:3.7811049817553184, W2:1.2789411202062613, bias:-2.2819325283923626, loss:0.48545779102345654\n",
      "W1:3.7834731240532316, W2:1.2789232697258326, bias:-2.282881159941249, loss:0.4853926698490944\n",
      "W1:3.7858403891456445, W2:1.2789052664298086, bias:-2.283829310372151, loss:0.48532759932049335\n",
      "W1:3.788206777321238, W2:1.2788871110742341, bias:-2.284776980394706, loss:0.4852625793902805\n",
      "W1:3.7905722888690287, W2:1.2788688044122913, bias:-2.285724170716489, loss:0.4851976100111477\n",
      "W1:3.792936924078372, W2:1.2788503471943096, bias:-2.2866708820430217, loss:0.4851326911358507\n",
      "W1:3.7953006832389558, W2:1.278831740167776, bias:-2.287617115077779, loss:0.48506782271721066\n",
      "W1:3.797663566640801, W2:1.2788129840773461, bias:-2.288562870522195, loss:0.4850030047081119\n",
      "W1:3.800025574574259, W2:1.2787940796648538, bias:-2.2895081490756723, loss:0.484938237061503\n",
      "W1:3.802386707330011, W2:1.278775027669322, bias:-2.290452951435587, loss:0.48487351973039616\n",
      "W1:3.804746965199065, W2:1.2787558288269723, bias:-2.2913972782972967, loss:0.48480885266786705\n",
      "W1:3.807106348472754, W2:1.2787364838712356, bias:-2.2923411303541474, loss:0.4847442358270548\n",
      "W1:3.8094648574427357, W2:1.2787169935327625, bias:-2.2932845082974804, loss:0.48467966916116123\n",
      "W1:3.8118224924009896, W2:1.2786973585394326, bias:-2.294227412816639, loss:0.48461515262345145\n",
      "W1:3.8141792536398156, W2:1.2786775796163654, bias:-2.2951698445989757, loss:0.48455068616725305\n",
      "W1:3.8165351414518316, W2:1.2786576574859303, bias:-2.2961118043298594, loss:0.48448626974595616\n",
      "W1:3.8188901561299726, W2:1.2786375928677558, bias:-2.2970532926926817, loss:0.4844219033130136\n",
      "W1:3.8212442979674894, W2:1.2786173864787405, bias:-2.2979943103688636, loss:0.48435758682193913\n",
      "W1:3.8235975672579454, W2:1.2785970390330623, bias:-2.298934858037863, loss:0.48429332022631005\n",
      "W1:3.8259499642952166, W2:1.2785765512421885, bias:-2.2998749363771815, loss:0.484229103479764\n",
      "W1:3.828301489373488, W2:1.278555923814886, bias:-2.30081454606237, loss:0.48416493653600107\n",
      "W1:3.830652142787254, W2:1.278535157457231, bias:-2.301753687767037, loss:0.48410081934878185\n",
      "W1:3.8330019248313154, W2:1.278514252872618, bias:-2.3026923621628534, loss:0.48403675187192907\n",
      "W1:3.8353508358007775, W2:1.278493210761771, bias:-2.3036305699195623, loss:0.4839727340593258\n",
      "W1:3.8376988759910495, W2:1.2784720318227516, bias:-2.304568311704982, loss:0.483908765864916\n",
      "W1:3.840046045697842, W2:1.2784507167509702, bias:-2.305505588185015, loss:0.48384484724270427\n",
      "W1:3.8423923452171658, W2:1.2784292662391947, bias:-2.3064424000236543, loss:0.4837809781467556\n",
      "W1:3.8447377748453295, W2:1.2784076809775604, bias:-2.3073787478829892, loss:0.4837171585311957\n",
      "W1:3.847082334878939, W2:1.2783859616535793, bias:-2.3083146324232127, loss:0.4836533883502094\n",
      "W1:3.8494260256148953, W2:1.2783641089521505, bias:-2.3092500543026273, loss:0.48358966755804217\n",
      "W1:3.8517688473503924, W2:1.2783421235555683, bias:-2.3101850141776525, loss:0.48352599610899866\n",
      "W1:3.8541108003829163, W2:1.278320006143533, bias:-2.31111951270283, loss:0.4834623739574436\n",
      "W1:3.8564518850102427, W2:1.2782977573931595, bias:-2.3120535505308317, loss:0.4833988010578006\n",
      "W1:3.8587921015304363, W2:1.2782753779789873, bias:-2.312987128312465, loss:0.4833352773645529\n",
      "W1:3.8611314502418486, W2:1.2782528685729893, bias:-2.3139202466966786, loss:0.48327180283224214\n",
      "W1:3.8634699314431167, W2:1.2782302298445813, bias:-2.314852906330572, loss:0.4832083774154695\n",
      "W1:3.8658075454331606, W2:1.2782074624606319, bias:-2.3157851078593983, loss:0.48314500106889413\n",
      "W1:3.868144292511183, W2:1.2781845670854703, bias:-2.316716851926572, loss:0.48308167374723393\n",
      "W1:3.870480172976667, W2:1.2781615443808976, bias:-2.3176481391736754, loss:0.48301839540526553\n",
      "W1:3.8728151871293743, W2:1.278138395006194, bias:-2.3185789702404658, loss:0.48295516599782257\n",
      "W1:3.8751493352693442, W2:1.278115119618129, bias:-2.31950934576488, loss:0.4828919854797983\n",
      "W1:3.8774826176968915, W2:1.2780917188709706, bias:-2.3204392663830413, loss:0.4828288538061422\n",
      "W1:3.879815034712605, W2:1.278068193416494, bias:-2.3213687327292662, loss:0.4827657709318624\n",
      "W1:3.882146586617347, W2:1.2780445439039907, bias:-2.3222977454360705, loss:0.48270273681202414\n",
      "W1:3.8844772737122493, W2:1.2780207709802776, bias:-2.323226305134175, loss:0.4826397514017499\n",
      "W1:3.8868070962987145, W2:1.2779968752897062, bias:-2.3241544124525113, loss:0.48257681465621954\n",
      "W1:3.889136054678412, W2:1.277972857474171, bias:-2.3250820680182303, loss:0.4825139265306697\n",
      "W1:3.891464149153279, W2:1.2779487181731195, bias:-2.3260092724567047, loss:0.4824510869803937\n",
      "W1:3.893791380025516, W2:1.2779244580235594, bias:-2.3269360263915386, loss:0.48238829596074206\n",
      "W1:3.8961177475975877, W2:1.277900077660069, bias:-2.327862330444571, loss:0.4823255534271215\n",
      "W1:3.89844325217222, W2:1.2778755777148056, bias:-2.3287881852358834, loss:0.4822628593349949\n",
      "W1:3.900767894052399, W2:1.2778509588175138, bias:-2.329713591383805, loss:0.48220021363988136\n",
      "W1:3.9030916735413697, W2:1.2778262215955345, bias:-2.3306385495049193, loss:0.48213761629735646\n",
      "W1:3.9054145909426343, W2:1.2778013666738144, bias:-2.33156306021407, loss:0.4820750672630511\n",
      "W1:3.907736646559951, W2:1.2777763946749132, bias:-2.332487124124366, loss:0.48201256649265206\n",
      "W1:3.910057840697331, W2:1.2777513062190138, bias:-2.333410741847189, loss:0.4819501139419019\n",
      "W1:3.912378173659039, W2:1.2777261019239299, bias:-2.3343339139921992, loss:0.48188770956659854\n",
      "W1:3.9146976457495906, W2:1.2777007824051148, bias:-2.3352566411673386, loss:0.4818253533225947\n",
      "W1:3.917016257273751, W2:1.2776753482756704, bias:-2.3361789239788404, loss:0.48176304516579854\n",
      "W1:3.9193340085365342, W2:1.2776498001463554, bias:-2.3371007630312333, loss:0.4817007850521737\n",
      "W1:3.9216508998431996, W2:1.2776241386255935, bias:-2.338022158927347, loss:0.4816385729377375\n",
      "W1:3.923966931499253, W2:1.2775983643194828, bias:-2.338943112268318, loss:0.4815764087785628\n",
      "W1:3.9262821038104425, W2:1.277572477831803, bias:-2.3398636236535975, loss:0.48151429253077677\n",
      "W1:3.92859641708276, W2:1.277546479764025, bias:-2.340783693680953, loss:0.4814522241505603\n",
      "W1:3.9309098716224375, W2:1.2775203707153184, bias:-2.341703322946479, loss:0.48139020359414997\n",
      "W1:3.933222467735946, W2:1.2774941512825606, bias:-2.3426225120445996, loss:0.48132823081783477\n",
      "W1:3.9355342057299953, W2:1.2774678220603444, bias:-2.343541261568074, loss:0.4812663057779584\n",
      "W1:3.93784508591153, W2:1.2774413836409866, bias:-2.3444595721080037, loss:0.4812044284309186\n",
      "W1:3.9401551085877315, W2:1.2774148366145364, bias:-2.3453774442538378, loss:0.481142598733166\n",
      "W1:3.9424642740660136, W2:1.2773881815687835, bias:-2.346294878593379, loss:0.48108081664120556\n",
      "W1:3.9447725826540223, W2:1.2773614190892664, bias:-2.347211875712787, loss:0.4810190821115948\n",
      "W1:3.947080034659635, W2:1.27733454975928, bias:-2.3481284361965877, loss:0.48095739510094476\n",
      "W1:3.9493866303909573, W2:1.277307574159885, bias:-2.349044560627676, loss:0.48089575556592\n",
      "W1:3.9516923701563234, W2:1.2772804928699142, bias:-2.349960249587322, loss:0.4808341634632373\n",
      "W1:3.9539972542642934, W2:1.2772533064659823, bias:-2.3508755036551783, loss:0.4807726187496666\n",
      "W1:3.956301283023653, W2:1.2772260155224933, bias:-2.351790323409282, loss:0.48071112138203015\n",
      "W1:3.9586044567434104, W2:1.277198620611648, bias:-2.3527047094260634, loss:0.48064967131720343\n",
      "W1:3.9609067757327976, W2:1.2771711223034525, bias:-2.353618662280351, loss:0.48058826851211367\n",
      "W1:3.9632082403012663, W2:1.2771435211657267, bias:-2.354532182545375, loss:0.48052691292374045\n",
      "W1:3.9655088507584875, W2:1.2771158177641109, bias:-2.3554452707927758, loss:0.48046560450911574\n",
      "W1:3.967808607414351, W2:1.2770880126620747, bias:-2.356357927592606, loss:0.48040434322532294\n",
      "W1:3.9701075105789627, W2:1.2770601064209248, bias:-2.3572701535133387, loss:0.4803431290294977\n",
      "W1:3.9724055605626436, W2:1.2770320995998126, bias:-2.3581819491218714, loss:0.4802819618788273\n",
      "W1:3.974702757675929, W2:1.2770039927557417, bias:-2.359093314983532, loss:0.48022084173055096\n",
      "W1:3.9769991022295663, W2:1.2769757864435765, bias:-2.360004251662083, loss:0.4801597685419586\n",
      "W1:3.979294594534515, W2:1.276947481216049, bias:-2.360914759719729, loss:0.4800987422703919\n",
      "W1:3.981589234901943, W2:1.2769190776237678, bias:-2.36182483971712, loss:0.48003776287324346\n",
      "W1:3.983883023643227, W2:1.2768905762152245, bias:-2.362734492213358, loss:0.47997683030795757\n",
      "W1:3.986175961069952, W2:1.276861977536802, bias:-2.3636437177660015, loss:0.4799159445320287\n",
      "W1:3.9884680474939076, W2:1.276833282132782, bias:-2.3645525169310706, loss:0.47985510550300264\n",
      "W1:3.990759283227088, W2:1.2768044905453526, bias:-2.3654608902630527, loss:0.4797943131784754\n",
      "W1:3.993049668581691, W2:1.2767756033146165, bias:-2.3663688383149086, loss:0.47973356751609386\n",
      "W1:3.995339203870115, W2:1.2767466209785974, bias:-2.3672763616380754, loss:0.47967286847355517\n",
      "W1:3.9976278894049595, W2:1.2767175440732483, bias:-2.368183460782474, loss:0.479612216008607\n",
      "W1:3.9999157254990236, W2:1.2766883731324594, bias:-2.3690901362965135, loss:0.47955161007904684\n",
      "W1:4.002202712465303, W2:1.2766591086880643, bias:-2.3699963887270954, loss:0.4794910506427227\n",
      "W1:4.004488850616991, W2:1.2766297512698486, bias:-2.37090221861962, loss:0.4794305376575317\n",
      "W1:4.006774140267476, W2:1.276600301405557, bias:-2.3718076265179904, loss:0.479370071081422\n",
      "W1:4.009058581730339, W2:1.2765707596209, bias:-2.3727126129646194, loss:0.47930965087239025\n",
      "W1:4.0113421753193546, W2:1.2765411264395627, bias:-2.373617178500432, loss:0.4792492769884834\n",
      "W1:4.013624921348488, W2:1.276511402383211, bias:-2.374521323664873, loss:0.4791889493877976\n",
      "W1:4.015906820131896, W2:1.2764815879714988, bias:-2.3754250489959094, loss:0.4791286680284781\n",
      "W1:4.018187871983922, W2:1.2764516837220765, bias:-2.376328355030039, loss:0.47906843286871953\n",
      "W1:4.020468077219097, W2:1.2764216901505971, bias:-2.3772312423022917, loss:0.4790082438667657\n",
      "W1:4.022747436152139, W2:1.2763916077707238, bias:-2.378133711346236, loss:0.47894810098090973\n",
      "W1:4.025025949097951, W2:1.2763614370941372, bias:-2.3790357626939853, loss:0.4788880041694927\n",
      "W1:4.0273036163716185, W2:1.2763311786305427, bias:-2.3799373968762003, loss:0.4788279533909051\n",
      "W1:4.029580438288411, W2:1.2763008328876775, bias:-2.3808386144220957, loss:0.47876794860358574\n",
      "W1:4.031856415163777, W2:1.2762704003713174, bias:-2.3817394158594447, loss:0.4787079897660222\n",
      "W1:4.034131547313345, W2:1.2762398815852845, bias:-2.382639801714584, loss:0.47864807683675004\n",
      "W1:4.036405835052925, W2:1.276209277031454, bias:-2.383539772512418, loss:0.4785882097743537\n",
      "W1:4.0386792786985, W2:1.2761785872097609, bias:-2.3844393287764243, loss:0.4785283885374652\n",
      "W1:4.040951878566232, W2:1.2761478126182078, bias:-2.3853384710286587, loss:0.4784686130847647\n",
      "W1:4.043223634972456, W2:1.2761169537528712, bias:-2.38623719978976, loss:0.4784088833749808\n",
      "W1:4.045494548233682, W2:1.276086011107909, bias:-2.387135515578953, loss:0.4783491993668891\n",
      "W1:4.047764618666592, W2:1.2760549851755671, bias:-2.3880334189140564, loss:0.4782895610193137\n",
      "W1:4.050033846588038, W2:1.2760238764461862, bias:-2.3889309103114855, loss:0.47822996829112585\n",
      "W1:4.052302232315044, W2:1.275992685408209, bias:-2.3898279902862574, loss:0.4781704211412444\n",
      "W1:4.0545697761648, W2:1.2759614125481873, bias:-2.3907246593519957, loss:0.4781109195286355\n",
      "W1:4.056836478454665, W2:1.2759300583507882, bias:-2.3916209180209353, loss:0.4780514634123129\n",
      "W1:4.059102339502164, W2:1.2758986232988014, bias:-2.392516766803927, loss:0.4779920527513371\n",
      "W1:4.061367359624988, W2:1.275867107873146, bias:-2.393412206210443, loss:0.47793268750481593\n",
      "W1:4.0636315391409905, W2:1.275835512552877, bias:-2.39430723674858, loss:0.4778733676319041\n",
      "W1:4.065894878368188, W2:1.2758038378151924, bias:-2.3952018589250645, loss:0.47781409309180295\n",
      "W1:4.068157377624758, W2:1.2757720841354392, bias:-2.3960960732452588, loss:0.47775486384376115\n",
      "W1:4.07041903722904, W2:1.2757402519871213, bias:-2.3969898802131633, loss:0.4776956798470732\n",
      "W1:4.072679857499529, W2:1.275708341841905, bias:-2.397883280331423, loss:0.4776365410610808\n",
      "W1:4.074939838754882, W2:1.2756763541696265, bias:-2.3987762741013317, loss:0.4775774474451718\n",
      "W1:4.07719898131391, W2:1.275644289438298, bias:-2.3996688620228346, loss:0.4775183989587802\n",
      "W1:4.079457285495581, W2:1.2756121481141143, bias:-2.4005610445945362, loss:0.47745939556138683\n",
      "W1:4.081714751619015, W2:1.27557993066146, bias:-2.4014528223137024, loss:0.47740043721251796\n",
      "W1:4.083971380003488, W2:1.2755476375429151, bias:-2.4023441956762652, loss:0.47734152387174583\n",
      "W1:4.086227170968428, W2:1.2755152692192626, bias:-2.403235165176829, loss:0.4772826554986896\n",
      "W1:4.088482124833411, W2:1.2754828261494942, bias:-2.404125731308673, loss:0.4772238320530129\n",
      "W1:4.090736241918166, W2:1.275450308790817, bias:-2.405015894563756, loss:0.4771650534944263\n",
      "W1:4.0929895225425685, W2:1.27541771759866, bias:-2.405905655432723, loss:0.477106319782685\n",
      "W1:4.095241967026642, W2:1.275385053026681, bias:-2.406795014404906, loss:0.47704763087758995\n",
      "W1:4.097493575690555, W2:1.2753523155267719, bias:-2.407683971968332, loss:0.47698898673898815\n",
      "W1:4.099744348854624, W2:1.2753195055490658, bias:-2.4085725286097253, loss:0.4769303873267713\n",
      "W1:4.101994286839307, W2:1.275286623541944, bias:-2.409460684814512, loss:0.47687183260087596\n",
      "W1:4.104243389965207, W2:1.275253669952041, bias:-2.4103484410668248, loss:0.4768133225212851\n",
      "W1:4.106491658553066, W2:1.2752206452242516, bias:-2.4112357978495083, loss:0.47675485704802534\n",
      "W1:4.108739092923769, W2:1.2751875498017375, bias:-2.4121227556441216, loss:0.47669643614116897\n",
      "W1:4.110985693398339, W2:1.2751543841259327, bias:-2.4130093149309437, loss:0.476638059760833\n",
      "W1:4.113231460297938, W2:1.2751211486365504, bias:-2.4138954761889773, loss:0.4765797278671792\n",
      "W1:4.115476393943867, W2:1.275087843771589, bias:-2.4147812398959534, loss:0.47652144042041367\n",
      "W1:4.11772049465756, W2:1.2750544699673383, bias:-2.415666606528336, loss:0.47646319738078735\n",
      "W1:4.119963762760587, W2:1.2750210276583858, bias:-2.4165515765613255, loss:0.4764049987085956\n",
      "W1:4.122206198574654, W2:1.2749875172776228, bias:-2.417436150468863, loss:0.476346844364178\n",
      "W1:4.124447802421598, W2:1.2749539392562508, bias:-2.418320328723636, loss:0.4762887343079185\n",
      "W1:4.126688574623388, W2:1.274920294023787, bias:-2.4192041117970797, loss:0.47623066850024554\n",
      "W1:4.128928515502123, W2:1.2748865820080706, bias:-2.420087500159385, loss:0.4761726469016309\n",
      "W1:4.131167625380034, W2:1.2748528036352698, bias:-2.4209704942794996, loss:0.4761146694725905\n",
      "W1:4.133405904579478, W2:1.2748189593298862, bias:-2.421853094625133, loss:0.4760567361736852\n",
      "W1:4.135643353422941, W2:1.2747850495147621, bias:-2.422735301662762, loss:0.47599884696551836\n",
      "W1:4.137879972233034, W2:1.2747510746110862, bias:-2.4236171158576334, loss:0.4759410018087377\n",
      "W1:4.140115761332496, W2:1.2747170350383994, bias:-2.4244985376737676, loss:0.47588320066403417\n",
      "W1:4.1423507210441874, W2:1.2746829312146009, bias:-2.4253795675739647, loss:0.4758254434921426\n",
      "W1:4.144584851691093, W2:1.2746487635559538, bias:-2.4262602060198075, loss:0.47576773025384167\n",
      "W1:4.1468181535963184, W2:1.2746145324770919, bias:-2.427140453471665, loss:0.4757100609099522\n",
      "W1:4.149050627083093, W2:1.2745802383910247, bias:-2.428020310388698, loss:0.47565243542133956\n",
      "W1:4.151282272474764, W2:1.2745458817091437, bias:-2.428899777228861, loss:0.4755948537489113\n",
      "W1:4.153513090094798, W2:1.274511462841228, bias:-2.429778854448909, loss:0.47553731585361875\n",
      "W1:4.155743080266779, W2:1.2744769821954505, bias:-2.4306575425043992, loss:0.475479821696456\n",
      "W1:4.15797224331441, W2:1.2744424401783834, bias:-2.4315358418496955, loss:0.4754223712384597\n",
      "W1:4.160200579561509, W2:1.2744078371950043, bias:-2.4324137529379737, loss:0.47536496444071025\n",
      "W1:4.162428089332006, W2:1.2743731736487014, bias:-2.433291276221224, loss:0.47530760126432975\n",
      "W1:4.164654772949948, W2:1.27433844994128, bias:-2.4341684121502567, loss:0.47525028167048367\n",
      "W1:4.166880630739494, W2:1.2743036664729677, bias:-2.4350451611747035, loss:0.47519300562037975\n",
      "W1:4.169105663024915, W2:1.2742688236424204, bias:-2.435921523743024, loss:0.4751357730752684\n",
      "W1:4.171329870130592, W2:1.2742339218467278, bias:-2.4367975003025086, loss:0.4750785839964422\n",
      "W1:4.173553252381015, W2:1.274198961481419, bias:-2.437673091299282, loss:0.4750214383452361\n",
      "W1:4.175775810100785, W2:1.2741639429404688, bias:-2.438548297178308, loss:0.4749643360830276\n",
      "W1:4.17799754361461, W2:1.2741288666163018, bias:-2.4394231183833934, loss:0.4749072771712361\n",
      "W1:4.180218453247302, W2:1.2740937328998, bias:-2.44029755535719, loss:0.474850261571323\n",
      "W1:4.1824385393237815, W2:1.2740585421803072, bias:-2.4411716085412016, loss:0.47479328924479186\n",
      "W1:4.184657802169073, W2:1.2740232948456343, bias:-2.442045278375785, loss:0.474736360153188\n",
      "W1:4.186876242108305, W2:1.2739879912820657, bias:-2.442918565300155, loss:0.4746794742580986\n",
      "W1:4.189093859466707, W2:1.2739526318743646, bias:-2.4437914697523886, loss:0.47462263152115286\n",
      "W1:4.191310654569613, W2:1.2739172170057778, bias:-2.4446639921694286, loss:0.47456583190402135\n",
      "W1:4.193526627742456, W2:1.2738817470580424, bias:-2.4455361329870873, loss:0.4745090753684162\n",
      "W1:4.195741779310768, W2:1.27384622241139, bias:-2.446407892640049, loss:0.47445236187609124\n",
      "W1:4.197956109600181, W2:1.2738106434445533, bias:-2.447279271561876, loss:0.4743956913888417\n",
      "W1:4.2001696189364255, W2:1.2737750105347707, bias:-2.4481502701850113, loss:0.47433906386850405\n",
      "W1:4.202382307645327, W2:1.273739324057792, bias:-2.4490208889407814, loss:0.47428247927695605\n",
      "W1:4.204594176052808, W2:1.2737035843878841, bias:-2.4498911282594023, loss:0.47422593757611664\n",
      "W1:4.206805224484885, W2:1.2736677918978354, bias:-2.4507609885699804, loss:0.4741694387279461\n",
      "W1:4.20901545326767, W2:1.2736319469589623, bias:-2.451630470300519, loss:0.47411298269444535\n",
      "W1:4.211224862727366, W2:1.2735960499411139, bias:-2.4524995738779203, loss:0.4740565694376565\n",
      "W1:4.2134334531902695, W2:1.2735601012126772, bias:-2.4533682997279884, loss:0.4740001989196626\n",
      "W1:4.215641224982768, W2:1.2735241011405831, bias:-2.4542366482754354, loss:0.47394387110258757\n",
      "W1:4.217848178431338, W2:1.2734880500903112, bias:-2.455104619943883, loss:0.4738875859485954\n",
      "W1:4.220054313862547, W2:1.2734519484258944, bias:-2.455972215155866, loss:0.47383134341989164\n",
      "W1:4.222259631603048, W2:1.2734157965099255, bias:-2.4568394343328386, loss:0.473775143478722\n",
      "W1:4.224464131979585, W2:1.2733795947035615, bias:-2.4577062778951744, loss:0.4737189860873727\n",
      "W1:4.226667815318985, W2:1.2733433433665289, bias:-2.4585727462621723, loss:0.4736628712081703\n",
      "W1:4.228870681948162, W2:1.273307042857129, bias:-2.459438839852059, loss:0.4736067988034817\n",
      "W1:4.231072732194114, W2:1.273270693532243, bias:-2.4603045590819943, loss:0.47355076883571434\n",
      "W1:4.233273966383924, W2:1.2732342957473377, bias:-2.461169904368072, loss:0.47349478126731576\n",
      "W1:4.235474384844755, W2:1.2731978498564693, bias:-2.4620348761253252, loss:0.47343883606077347\n",
      "W1:4.237673987903855, W2:1.27316135621229, bias:-2.46289947476773, loss:0.4733829331786151\n",
      "W1:4.23987277588855, W2:1.2731248151660521, bias:-2.4637637007082085, loss:0.4733270725834084\n",
      "W1:4.242070749126247, W2:1.2730882270676132, bias:-2.464627554358631, loss:0.4732712542377609\n",
      "W1:4.244267907944433, W2:1.2730515922654417, bias:-2.4654910361298223, loss:0.4732154781043199\n",
      "W1:4.2464642526706715, W2:1.2730149111066216, bias:-2.4663541464315633, loss:0.47315974414577266\n",
      "W1:4.248659783632604, W2:1.272978183936857, bias:-2.467216885672594, loss:0.4731040523248459\n",
      "W1:4.250854501157948, W2:1.2729414111004782, bias:-2.4680792542606187, loss:0.4730484026043063\n",
      "W1:4.253048405574497, W2:1.2729045929404457, bias:-2.4689412526023085, loss:0.47299279494695995\n",
      "W1:4.255241497210119, W2:1.2728677297983553, bias:-2.469802881103304, loss:0.4729372293156521\n",
      "W1:4.257433776392755, W2:1.2728308220144435, bias:-2.470664140168221, loss:0.4728817056732678\n",
      "W1:4.25962524345042, W2:1.2727938699275925, bias:-2.4715250302006506, loss:0.4728262239827314\n",
      "W1:4.261815898711198, W2:1.2727568738753339, bias:-2.4723855516031654, loss:0.47277078420700636\n",
      "W1:4.264005742503248, W2:1.272719834193855, bias:-2.4732457047773218, loss:0.47271538630909526\n",
      "W1:4.266194775154796, W2:1.2726827512180032, bias:-2.4741054901236637, loss:0.47266003025204006\n",
      "W1:4.268382996994139, W2:1.2726456252812905, bias:-2.474964908041725, loss:0.4726047159989221\n",
      "W1:4.2705704083496405, W2:1.2726084567158988, bias:-2.4758239589300333, loss:0.47254944351286093\n",
      "W1:4.272757009549734, W2:1.2725712458526846, bias:-2.4766826431861144, loss:0.47249421275701525\n",
      "W1:4.274942800922918, W2:1.2725339930211832, bias:-2.4775409612064947, loss:0.4724390236945831\n",
      "W1:4.2771277827977565, W2:1.2724966985496147, bias:-2.478398913386704, loss:0.47238387628880063\n",
      "W1:4.27931195550288, W2:1.2724593627648875, bias:-2.4792565001212794, loss:0.47232877050294353\n",
      "W1:4.281495319366982, W2:1.272421985992604, bias:-2.4801137218037685, loss:0.47227370630032545\n",
      "W1:4.283677874718819, W2:1.2723845685570652, bias:-2.480970578826733, loss:0.4722186836442987\n",
      "W1:4.2858596218872105, W2:1.2723471107812743, bias:-2.481827071581751, loss:0.47216370249825457\n",
      "W1:4.288040561201037, W2:1.2723096129869431, bias:-2.4826832004594226, loss:0.4721087628256224\n",
      "W1:4.290220692989241, W2:1.2722720754944956, bias:-2.483538965849369, loss:0.47205386458987\n",
      "W1:4.292400017580822, W2:1.2722344986230727, bias:-2.4843943681402405, loss:0.4719990077545035\n",
      "W1:4.294578535304841, W2:1.2721968826905372, bias:-2.485249407719716, loss:0.47194419228306733\n",
      "W1:4.296756246490417, W2:1.2721592280134784, bias:-2.4861040849745075, loss:0.4718894181391443\n",
      "W1:4.298933151466725, W2:1.2721215349072164, bias:-2.4869584002903644, loss:0.4718346852863551\n",
      "W1:4.301109250562999, W2:1.272083803685807, bias:-2.487812354052075, loss:0.4717799936883582\n",
      "W1:4.303284544108526, W2:1.2720460346620466, bias:-2.488665946643471, loss:0.4717253433088508\n",
      "W1:4.3054590324326485, W2:1.2720082281474756, bias:-2.489519178447429, loss:0.47167073411156757\n",
      "W1:4.307632715864764, W2:1.271970384452384, bias:-2.4903720498458757, loss:0.4716161660602809\n",
      "W1:4.309805594734322, W2:1.2719325038858158, bias:-2.4912245612197896, loss:0.47156163911880156\n",
      "W1:4.311977669370826, W2:1.2718945867555733, bias:-2.492076712949205, loss:0.47150715325097753\n",
      "W1:4.31414894010383, W2:1.2718566333682215, bias:-2.4929285054132135, loss:0.4714527084206948\n",
      "W1:4.316319407262939, W2:1.2718186440290928, bias:-2.4937799389899697, loss:0.4713983045918766\n",
      "W1:4.318489071177808, W2:1.2717806190422916, bias:-2.494631014056692, loss:0.47134394172848426\n",
      "W1:4.3206579321781415, W2:1.2717425587106979, bias:-2.495481730989667, loss:0.47128961979451617\n",
      "W1:4.322825990593691, W2:1.271704463335973, bias:-2.4963320901642514, loss:0.4712353387540081\n",
      "W1:4.324993246754258, W2:1.2716663332185634, bias:-2.497182091954877, loss:0.4711810985710338\n",
      "W1:4.327159700989689, W2:1.2716281686577044, bias:-2.4980317367350517, loss:0.47112689920970346\n",
      "W1:4.329325353629877, W2:1.271589969951426, bias:-2.498881024877364, loss:0.47107274063416543\n",
      "W1:4.331490205004761, W2:1.2715517373965555, bias:-2.4997299567534847, loss:0.4710186228086044\n",
      "W1:4.333654255444323, W2:1.2715134712887237, bias:-2.500578532734171, loss:0.4709645456972428\n",
      "W1:4.335817505278589, W2:1.271475171922368, bias:-2.501426753189269, loss:0.4709105092643398\n",
      "W1:4.337979954837629, W2:1.2714368395907372, bias:-2.502274618487718, loss:0.47085651347419155\n",
      "W1:4.340141604451555, W2:1.2713984745858955, bias:-2.5031221289975503, loss:0.47080255829113143\n",
      "W1:4.342302454450518, W2:1.271360077198727, bias:-2.5039692850858977, loss:0.4707486436795296\n",
      "W1:4.344462505164713, W2:1.2713216477189402, bias:-2.504816087118993, loss:0.47069476960379286\n",
      "W1:4.346621756924372, W2:1.2712831864350718, bias:-2.505662535462172, loss:0.47064093602836493\n",
      "W1:4.348780210059768, W2:1.271244693634491, bias:-2.5065086304798783, loss:0.4705871429177264\n",
      "W1:4.350937864901211, W2:1.2712061696034045, bias:-2.507354372535665, loss:0.470533390236394\n",
      "W1:4.353094721779049, W2:1.2711676146268593, bias:-2.508199761992198, loss:0.4704796779489216\n",
      "W1:4.355250781023668, W2:1.2711290289887487, bias:-2.509044799211259, loss:0.4704260060198992\n",
      "W1:4.357406042965486, W2:1.2710904129718146, bias:-2.5098894845537476, loss:0.47037237441395346\n",
      "W1:4.359560507934961, W2:1.271051766857653, bias:-2.510733818379686, loss:0.4703187830957475\n",
      "W1:4.361714176262582, W2:1.2710130909267179, bias:-2.51157780104822, loss:0.4702652320299809\n",
      "W1:4.363867048278875, W2:1.2709743854583249, bias:-2.5124214329176233, loss:0.4702117211813892\n",
      "W1:4.366019124314396, W2:1.270935650730656, bias:-2.5132647143452984, loss:0.47015825051474436\n",
      "W1:4.368170404699734, W2:1.270896887020763, bias:-2.514107645687782, loss:0.47010481999485465\n",
      "W1:4.370320889765512, W2:1.2708580946045729, bias:-2.514950227300746, loss:0.470051429586564\n",
      "W1:4.372470579842381, W2:1.27081927375689, bias:-2.5157924595390018, loss:0.46999807925475334\n",
      "W1:4.374619475261022, W2:1.2707804247514018, bias:-2.5166343427565008, loss:0.4699447689643385\n",
      "W1:4.3767675763521465, W2:1.2707415478606823, bias:-2.5174758773063397, loss:0.46989149868027197\n",
      "W1:4.3789148834464955, W2:1.2707026433561954, bias:-2.518317063540762, loss:0.4698382683675421\n",
      "W1:4.381061396874836, W2:1.2706637115083004, bias:-2.5191579018111616, loss:0.46978507799117286\n",
      "W1:4.383207116967962, W2:1.2706247525862548, bias:-2.519998392468084, loss:0.4697319275162241\n",
      "W1:4.385352044056696, W2:1.2705857668582188, bias:-2.5208385358612313, loss:0.46967881690779145\n",
      "W1:4.387496178471886, W2:1.270546754591259, bias:-2.5216783323394627, loss:0.46962574613100616\n",
      "W1:4.389639520544402, W2:1.2705077160513527, bias:-2.522517782250799, loss:0.46957271515103527\n",
      "W1:4.391782070605141, W2:1.2704686515033916, bias:-2.5233568859424245, loss:0.469519723933081\n",
      "W1:4.393923828985024, W2:1.270429561211186, bias:-2.5241956437606903, loss:0.4694667724423816\n",
      "W1:4.396064796014993, W2:1.2703904454374684, bias:-2.525034056051116, loss:0.4694138606442106\n",
      "W1:4.398204972026013, W2:1.2703513044438977, bias:-2.525872123158394, loss:0.46936098850387664\n",
      "W1:4.400344357349072, W2:1.2703121384910627, bias:-2.5267098454263897, loss:0.469308155986724\n",
      "W1:4.402482952315175, W2:1.2702729478384867, bias:-2.5275472231981477, loss:0.46925536305813237\n",
      "W1:4.404620757255351, W2:1.2702337327446305, bias:-2.5283842568158916, loss:0.46920260968351624\n",
      "W1:4.406757772500647, W2:1.2701944934668967, bias:-2.529220946621028, loss:0.4691498958283258\n",
      "W1:4.408893998382128, W2:1.270155230261634, bias:-2.5300572929541474, loss:0.46909722145804605\n",
      "W1:4.411029435230877, W2:1.2701159433841398, bias:-2.5308932961550306, loss:0.4690445865381974\n",
      "W1:4.413164083377996, W2:1.2700766330886655, bias:-2.5317289565626475, loss:0.46899199103433475\n",
      "W1:4.415297943154601, W2:1.2700372996284188, bias:-2.5325642745151615, loss:0.46893943491204876\n",
      "W1:4.417431014891827, W2:1.269997943255569, bias:-2.5333992503499325, loss:0.4688869181369642\n",
      "W1:4.4195632989208224, W2:1.2699585642212496, bias:-2.534233884403518, loss:0.4688344406747413\n",
      "W1:4.42169479557275, W2:1.2699191627755624, bias:-2.535068177011677, loss:0.46878200249107477\n",
      "W1:4.423825505178788, W2:1.2698797391675816, bias:-2.535902128509373, loss:0.4687296035516944\n",
      "W1:4.425955428070125, W2:1.269840293645357, bias:-2.536735739230775, loss:0.46867724382236453\n",
      "W1:4.428084564577967, W2:1.2698008264559182, bias:-2.537569009509261, loss:0.46862492326888405\n",
      "W1:4.430212915033527, W2:1.269761337845278, bias:-2.5384019396774202, loss:0.46857264185708675\n",
      "W1:4.432340479768032, W2:1.269721828058436, bias:-2.539234530067057, loss:0.4685203995528411\n",
      "W1:4.434467259112719, W2:1.2696822973393829, bias:-2.5400667810091915, loss:0.468468196322049\n",
      "W1:4.4365932533988355, W2:1.2696427459311033, bias:-2.540898692834063, loss:0.4684160321306484\n",
      "W1:4.438718462957637, W2:1.2696031740755798, bias:-2.541730265871133, loss:0.4683639069446109\n",
      "W1:4.440842888120389, W2:1.2695635820137967, bias:-2.5425615004490867, loss:0.4683118207299424\n",
      "W1:4.4429665292183635, W2:1.2695239699857437, bias:-2.543392396895837, loss:0.46825977345268277\n",
      "W1:4.445089386582841, W2:1.269484338230419, bias:-2.5442229555385247, loss:0.46820776507890716\n",
      "W1:4.447211460545108, W2:1.2694446869858338, bias:-2.5450531767035236, loss:0.46815579557472414\n",
      "W1:4.449332751436458, W2:1.269405016489015, bias:-2.545883060716441, loss:0.4681038649062766\n",
      "W1:4.451453259588189, W2:1.2693653269760088, bias:-2.546712607902122, loss:0.46805197303974194\n",
      "W1:4.453572985331604, W2:1.269325618681885, bias:-2.5475418185846497, loss:0.46800011994133084\n",
      "W1:4.455691928998011, W2:1.2692858918407406, bias:-2.5483706930873495, loss:0.46794830557728884\n",
      "W1:4.45781009091872, W2:1.269246146685702, bias:-2.5491992317327914, loss:0.4678965299138948\n",
      "W1:4.459927471425044, W2:1.2692063834489293, bias:-2.550027434842791, loss:0.46784479291746217\n",
      "W1:4.462044070848299, W2:1.269166602361621, bias:-2.5508553027384147, loss:0.4677930945543375\n",
      "W1:4.464159889519803, W2:1.2691268036540155, bias:-2.5516828357399786, loss:0.4677414347909018\n",
      "W1:4.466274927770874, W2:1.2690869875553958, bias:-2.5525100341670535, loss:0.4676898135935695\n",
      "W1:4.46838918593283, W2:1.2690471542940929, bias:-2.5533368983384666, loss:0.4676382309287888\n",
      "W1:4.4705026643369905, W2:1.2690073040974883, bias:-2.554163428572304, loss:0.46758668676304177\n",
      "W1:4.472615363314673, W2:1.268967437192019, bias:-2.5549896251859128, loss:0.467535181062844\n",
      "W1:4.4747272831971925, W2:1.2689275538031797, bias:-2.5558154884959037, loss:0.46748371379474457\n",
      "W1:4.476838424315863, W2:1.2688876541555267, bias:-2.5566410188181536, loss:0.4674322849253261\n",
      "W1:4.478948787001997, W2:1.2688477384726813, bias:-2.5574662164678075, loss:0.46738089442120506\n",
      "W1:4.481058371586901, W2:1.2688078069773332, bias:-2.558291081759281, loss:0.46732954224903095\n",
      "W1:4.483167178401879, W2:1.268767859891244, bias:-2.559115615006263, loss:0.46727822837548677\n",
      "W1:4.48527520777823, W2:1.2687278974352498, bias:-2.5599398165217178, loss:0.4672269527672889\n",
      "W1:4.4873824600472485, W2:1.2686879198292662, bias:-2.560763686617887, loss:0.4671757153911869\n",
      "W1:4.489488935540224, W2:1.2686479272922897, bias:-2.5615872256062935, loss:0.4671245162139641\n",
      "W1:4.491594634588436, W2:1.2686079200424025, bias:-2.562410433797741, loss:0.46707335520243626\n",
      "W1:4.493699557523162, W2:1.2685678982967752, bias:-2.5632333115023194, loss:0.467022232323453\n",
      "W1:4.4958037046756685, W2:1.2685278622716702, bias:-2.5640558590294042, loss:0.4669711475438964\n",
      "W1:4.497907076377214, W2:1.2684878121824452, bias:-2.5648780766876618, loss:0.4669201008306824\n",
      "W1:4.5000096729590515, W2:1.268447748243556, bias:-2.565699964785049, loss:0.4668690921507592\n",
      "W1:4.502111494752421, W2:1.2684076706685607, bias:-2.566521523628818, loss:0.4668181214711085\n",
      "W1:4.504212542088553, W2:1.268367579670122, bias:-2.5673427535255153, loss:0.46676718875874473\n",
      "W1:4.506312815298669, W2:1.2683274754600105, bias:-2.5681636547809874, loss:0.46671629398071507\n",
      "W1:4.508412314713979, W2:1.2682873582491088, bias:-2.568984227700381, loss:0.46666543710409986\n",
      "W1:4.5105110406656825, W2:1.268247228247414, bias:-2.569804472588146, loss:0.46661461809601207\n",
      "W1:4.512608993484964, W2:1.2682070856640413, bias:-2.570624389748037, loss:0.46656383692359726\n",
      "W1:4.514706173502997, W2:1.268166930707227, bias:-2.5714439794831168, loss:0.46651309355403403\n",
      "W1:4.516802581050942, W2:1.2681267635843319, bias:-2.5722632420957576, loss:0.4664623879545333\n",
      "W1:4.518898216459944, W2:1.2680865845018439, bias:-2.5730821778876436, loss:0.46641172009233944\n",
      "W1:4.520993080061135, W2:1.268046393665382, bias:-2.573900787159773, loss:0.4663610899347281\n",
      "W1:4.523087172185631, W2:1.2680061912796994, bias:-2.5747190702124607, loss:0.466310497449008\n",
      "W1:4.525180493164534, W2:1.2679659775486856, bias:-2.57553702734534, loss:0.46625994260252096\n",
      "W1:4.527273043328926, W2:1.2679257526753707, bias:-2.576354658857365, loss:0.46620942536264065\n",
      "W1:4.529364823009877, W2:1.2678855168619283, bias:-2.5771719650468126, loss:0.466158945696773\n",
      "W1:4.531455832538437, W2:1.267845270309678, bias:-2.5779889462112853, loss:0.46610850357235667\n",
      "W1:4.533546072245637, W2:1.2678050132190894, bias:-2.578805602647712, loss:0.46605809895686257\n",
      "W1:4.535635542462494, W2:1.2677647457897847, bias:-2.5796219346523523, loss:0.46600773181779365\n",
      "W1:4.5377242435200005, W2:1.2677244682205415, bias:-2.5804379425207964, loss:0.46595740212268544\n",
      "W1:4.539812175749134, W2:1.2676841807092964, bias:-2.5812536265479684, loss:0.4659071098391054\n",
      "W1:4.54189933948085, W2:1.2676438834531483, bias:-2.5820689870281286, loss:0.4658568549346531\n",
      "W1:4.543985735046082, W2:1.2676035766483602, bias:-2.5828840242548754, loss:0.4658066373769604\n",
      "W1:4.546071362775745, W2:1.267563260490364, bias:-2.583698738521147, loss:0.46575645713369096\n",
      "W1:4.548156223000731, W2:1.2675229351737618, bias:-2.584513130119224, loss:0.4657063141725407\n",
      "W1:4.5502403160519105, W2:1.2674826008923306, bias:-2.5853271993407323, loss:0.4656562084612376\n",
      "W1:4.55232364226013, W2:1.2674422578390236, bias:-2.5861409464766427, loss:0.4656061399675409\n",
      "W1:4.554406201956214, W2:1.267401906205975, bias:-2.5869543718172756, loss:0.46555610865924263\n",
      "W1:4.556487995470961, W2:1.2673615461845011, bias:-2.5877674756523015, loss:0.4655061145041662\n",
      "W1:4.558569023135149, W2:1.2673211779651048, bias:-2.5885802582707442, loss:0.4654561574701665\n",
      "W1:4.560649285279527, W2:1.267280801737478, bias:-2.5893927199609825, loss:0.46540623752513094\n",
      "W1:4.562728782234821, W2:1.267240417690504, bias:-2.590204861010751, loss:0.4653563546369781\n",
      "W1:4.56480751433173, W2:1.2672000260122622, bias:-2.5910166817071447, loss:0.46530650877365837\n",
      "W1:4.566885481900928, W2:1.2671596268900285, bias:-2.5918281823366183, loss:0.4652566999031537\n",
      "W1:4.56896268527306, W2:1.2671192205102804, bias:-2.59263936318499, loss:0.4652069279934779\n",
      "W1:4.5710391247787445, W2:1.2670788070586987, bias:-2.593450224537444, loss:0.46515719301267605\n",
      "W1:4.573114800748573, W2:1.267038386720171, bias:-2.5942607666785302, loss:0.4651074949288249\n",
      "W1:4.575189713513105, W2:1.2669979596787946, bias:-2.5950709898921684, loss:0.4650578337100326\n",
      "W1:4.577263863402877, W2:1.2669575261178787, bias:-2.5958808944616494, loss:0.4650082093244387\n",
      "W1:4.579337250748389, W2:1.2669170862199481, bias:-2.596690480669637, loss:0.464958621740214\n",
      "W1:4.581409875880116, W2:1.2668766401667455, bias:-2.597499748798171, loss:0.46490907092556105\n",
      "W1:4.5834817391285, W2:1.266836188139235, bias:-2.5983086991286672, loss:0.4648595568487135\n",
      "W1:4.5855528408239525, W2:1.2667957303176038, bias:-2.599117331941921, loss:0.4648100794779362\n",
      "W1:4.587623181296854, W2:1.2667552668812665, bias:-2.5999256475181096, loss:0.4647606387815253\n",
      "W1:4.5896927608775515, W2:1.2667147980088667, bias:-2.6007336461367925, loss:0.46471123472780795\n",
      "W1:4.591761579896361, W2:1.2666743238782805, bias:-2.601541328076914, loss:0.4646618672851428\n",
      "W1:4.593829638683565, W2:1.2666338446666192, bias:-2.6023486936168068, loss:0.4646125364219193\n",
      "W1:4.595896937569412, W2:1.2665933605502318, bias:-2.6031557430341907, loss:0.464563242106558\n",
      "W1:4.597963476884117, W2:1.2665528717047079, bias:-2.603962476606178, loss:0.464513984307511\n",
      "W1:4.60002925695786, W2:1.2665123783048808, bias:-2.6047688946092737, loss:0.46446476299326034\n",
      "W1:4.602094278120785, W2:1.26647188052483, bias:-2.6055749973193767, loss:0.4644155781323198\n",
      "W1:4.604158540703003, W2:1.266431378537884, bias:-2.6063807850117833, loss:0.4643664296932343\n",
      "W1:4.606222045034587, W2:1.2663908725166226, bias:-2.6071862579611884, loss:0.46431731764457873\n",
      "W1:4.608284791445574, W2:1.2663503626328807, bias:-2.6079914164416875, loss:0.46426824195495925\n",
      "W1:4.610346780265964, W2:1.26630984905775, bias:-2.6087962607267787, loss:0.4642192025930132\n",
      "W1:4.6124080118257185, W2:1.266269331961582, bias:-2.609600791089364, loss:0.46417019952740796\n",
      "W1:4.614468486454763, W2:1.2662288115139912, bias:-2.610405007801752, loss:0.4641212327268423\n",
      "W1:4.616528204482984, W2:1.266188287883857, bias:-2.61120891113566, loss:0.4640723021600454\n",
      "W1:4.6185871662402285, W2:1.2661477612393273, bias:-2.6120125013622135, loss:0.46402340779577655\n",
      "W1:4.6206453720563045, W2:1.26610723174782, bias:-2.612815778751952, loss:0.46397454960282647\n",
      "W1:4.62270282226098, W2:1.2660666995760268, bias:-2.613618743574828, loss:0.4639257275500162\n",
      "W1:4.624759517183982, W2:1.2660261648899154, bias:-2.61442139610021, loss:0.4638769416061967\n",
      "W1:4.626815457154999, W2:1.265985627854732, bias:-2.615223736596883, loss:0.4638281917402502\n",
      "W1:4.628870642503676, W2:1.265945088635004, bias:-2.616025765333052, loss:0.46377947792108914\n",
      "W1:4.6309250735596175, W2:1.2659045473945432, bias:-2.6168274825763436, loss:0.4637308001176564\n",
      "W1:4.632978750652385, W2:1.265864004296447, bias:-2.6176288885938064, loss:0.4636821582989245\n",
      "W1:4.635031674111498, W2:1.265823459503103, bias:-2.618429983651915, loss:0.46363355243389753\n",
      "W1:4.637083844266432, W2:1.26578291317619, bias:-2.6192307680165694, loss:0.46358498249160895\n",
      "W1:4.639135261446621, W2:1.265742365476681, bias:-2.6200312419530993, loss:0.463536448441123\n",
      "W1:4.641185925981453, W2:1.2657018165648466, bias:-2.620831405726264, loss:0.4634879502515338\n",
      "W1:4.6432358382002725, W2:1.265661266600256, bias:-2.6216312596002553, loss:0.4634394878919656\n",
      "W1:4.645284998432379, W2:1.2656207157417811, bias:-2.6224308038386988, loss:0.4633910613315731\n",
      "W1:4.647333407007026, W2:1.2655801641475986, bias:-2.6232300387046554, loss:0.4633426705395412\n",
      "W1:4.649381064253422, W2:1.2655396119751916, bias:-2.624028964460624, loss:0.4632943154850841\n",
      "W1:4.6514279705007295, W2:1.2654990593813535, bias:-2.624827581368542, loss:0.46324599613744705\n",
      "W1:4.653474126078064, W2:1.26545850652219, bias:-2.6256258896897893, loss:0.46319771246590435\n",
      "W1:4.655519531314492, W2:1.265417953553121, bias:-2.626423889685187, loss:0.4631494644397611\n",
      "W1:4.657564186539036, W2:1.265377400628884, bias:-2.6272215816150015, loss:0.4631012520283521\n",
      "W1:4.659608092080667, W2:1.2653368479035367, bias:-2.6280189657389457, loss:0.46305307520104133\n",
      "W1:4.661651248268311, W2:1.2652962955304587, bias:-2.62881604231618, loss:0.46300493392722336\n",
      "W1:4.663693655430842, W2:1.2652557436623542, bias:-2.629612811605315, loss:0.46295682817632255\n",
      "W1:4.665735313897088, W2:1.2652151924512547, bias:-2.6304092738644123, loss:0.4629087579177925\n",
      "W1:4.667776223995823, W2:1.2651746420485215, bias:-2.6312054293509877, loss:0.4628607231211175\n",
      "W1:4.669816386055774, W2:1.2651340926048482, bias:-2.6320012783220115, loss:0.4628127237558104\n",
      "W1:4.671855800405617, W2:1.2650935442702629, bias:-2.6327968210339106, loss:0.46276475979141424\n",
      "W1:4.673894467373977, W2:1.2650529971941304, bias:-2.63359205774257, loss:0.46271683119750223\n",
      "W1:4.675932387289426, W2:1.2650124515251555, bias:-2.6343869887033358, loss:0.462668937943676\n",
      "W1:4.677969560480485, W2:1.2649719074113848, bias:-2.635181614171015, loss:0.46262107999956803\n",
      "W1:4.680005987275624, W2:1.2649313650002088, bias:-2.6359759343998785, loss:0.46257325733483917\n",
      "W1:4.682041668003259, W2:1.2648908244383652, bias:-2.6367699496436625, loss:0.46252546991918064\n",
      "W1:4.684076602991753, W2:1.2648502858719408, bias:-2.63756366015557, loss:0.46247771772231255\n",
      "W1:4.686110792569415, W2:1.2648097494463737, bias:-2.6383570661882723, loss:0.4624300007139846\n",
      "W1:4.688144237064501, W2:1.2647692153064558, bias:-2.6391501679939116, loss:0.462382318863976\n",
      "W1:4.690176936805212, W2:1.2647286835963356, bias:-2.6399429658241016, loss:0.46233467214209545\n",
      "W1:4.692208892119695, W2:1.26468815445952, bias:-2.640735459929929, loss:0.4622870605181801\n",
      "W1:4.694240103336042, W2:1.2646476280388774, bias:-2.6415276505619567, loss:0.4622394839620975\n",
      "W1:4.6962705707822865, W2:1.2646071044766387, bias:-2.642319537970224, loss:0.4621919424437436\n",
      "W1:4.69830029478641, W2:1.264566583914401, bias:-2.6431111224042487, loss:0.46214443593304416\n",
      "W1:4.700329275676335, W2:1.264526066493129, bias:-2.643902404113029, loss:0.46209696439995374\n",
      "W1:4.702357513779928, W2:1.2644855523531584, bias:-2.6446933833450443, loss:0.46204952781445613\n",
      "W1:4.704385009424998, W2:1.264445041634197, bias:-2.645484060348258, loss:0.462002126146564\n",
      "W1:4.706411762939297, W2:1.2644045344753276, bias:-2.646274435370118, loss:0.46195475936631963\n",
      "W1:4.708437774650518, W2:1.2643640310150106, bias:-2.6470645086575595, loss:0.4619074274437937\n",
      "W1:4.710463044886296, W2:1.2643235313910854, bias:-2.6478542804570053, loss:0.4618601303490866\n",
      "W1:4.712487573974209, W2:1.264283035740774, bias:-2.6486437510143683, loss:0.4618128680523271\n",
      "W1:4.7145113622417725, W2:1.2642425442006815, bias:-2.649432920575053, loss:0.4617656405236729\n",
      "W1:4.716534410016444, W2:1.2642020569068, bias:-2.650221789383957, loss:0.4617184477333108\n",
      "W1:4.7185567176256225, W2:1.2641615739945102, bias:-2.6510103576854718, loss:0.4616712896514566\n",
      "W1:4.720578285396644, W2:1.2641210955985835, bias:-2.651798625723486, loss:0.4616241662483549\n",
      "W1:4.722599113656785, W2:1.2640806218531844, bias:-2.6525865937413853, loss:0.4615770774942786\n",
      "W1:4.724619202733261, W2:1.2640401528918725, bias:-2.6533742619820555, loss:0.4615300233595301\n",
      "W1:4.726638552953225, W2:1.2639996888476057, bias:-2.654161630687882, loss:0.4614830038144395\n",
      "W1:4.72865716464377, W2:1.2639592298527411, bias:-2.6549487001007543, loss:0.461436018829367\n",
      "W1:4.730675038131925, W2:1.2639187760390378, bias:-2.655735470462065, loss:0.4613890683747001\n",
      "W1:4.732692173744656, W2:1.263878327537659, bias:-2.656521942012712, loss:0.46134215242085586\n",
      "W1:4.734708571808868, W2:1.2638378844791749, bias:-2.6573081149931013, loss:0.46129527093827954\n",
      "W1:4.736724232651401, W2:1.2637974469935636, bias:-2.6580939896431466, loss:0.46124842389744475\n",
      "W1:4.738739156599031, W2:1.2637570152102142, bias:-2.6588795662022724, loss:0.4612016112688541\n",
      "W1:4.740753343978472, W2:1.263716589257929, bias:-2.6596648449094142, loss:0.4611548330230383\n",
      "W1:4.742766795116371, W2:1.263676169264925, bias:-2.660449826003022, loss:0.4611080891305568\n",
      "W1:4.744779510339309, W2:1.2636357553588364, bias:-2.661234509721059, loss:0.46106137956199755\n",
      "W1:4.7467914899738055, W2:1.2635953476667172, bias:-2.6620188963010056, loss:0.4610147042879763\n",
      "W1:4.748802734346312, W2:1.2635549463150424, bias:-2.66280298597986, loss:0.4609680632791378\n",
      "W1:4.750813243783215, W2:1.263514551429711, bias:-2.6635867789941394, loss:0.4609214565061548\n",
      "W1:4.752823018610833, W2:1.2634741631360478, bias:-2.664370275579882, loss:0.4608748839397285\n",
      "W1:4.7548320591554205, W2:1.2634337815588055, bias:-2.6651534759726476, loss:0.46082834555058805\n",
      "W1:4.7568403657431615, W2:1.2633934068221666, bias:-2.665936380407521, loss:0.46078184130949124\n",
      "W1:4.758847938700175, W2:1.2633530390497458, bias:-2.6667189891191105, loss:0.46073537118722374\n",
      "W1:4.760854778352511, W2:1.263312678364592, bias:-2.667501302341553, loss:0.4606889351545998\n",
      "W1:4.762860885026151, W2:1.2632723248891906, bias:-2.6682833203085115, loss:0.46064253318246134\n",
      "W1:4.76486625904701, W2:1.2632319787454653, bias:-2.6690650432531804, loss:0.46059616524167857\n",
      "W1:4.7668709007409324, W2:1.26319164005478, bias:-2.6698464714082846, loss:0.4605498313031495\n",
      "W1:4.768874810433693, W2:1.2631513089379414, bias:-2.6706276050060813, loss:0.4605035313378007\n",
      "W1:4.770877988450998, W2:1.2631109855152005, bias:-2.671408444278362, loss:0.46045726531658654\n",
      "W1:4.772880435118483, W2:1.2630706699062553, bias:-2.6721889894564534, loss:0.4604110332104888\n",
      "W1:4.774882150761715, W2:1.2630303622302521, bias:-2.672969240771219, loss:0.46036483499051833\n",
      "W1:4.776883135706187, W2:1.2629900626057882, bias:-2.6737491984530615, loss:0.4603186706277127\n",
      "W1:4.778883390277325, W2:1.2629497711509134, bias:-2.674528862731922, loss:0.46027254009313817\n",
      "W1:4.780882914800479, W2:1.2629094879831324, bias:-2.6753082338372836, loss:0.4602264433578884\n",
      "W1:4.782881709600932, W2:1.2628692132194066, bias:-2.6760873119981716, loss:0.46018038039308495\n",
      "W1:4.784879775003893, W2:1.262828946976156, bias:-2.676866097443156, loss:0.46013435116987744\n",
      "W1:4.786877111334498, W2:1.262788689369262, bias:-2.6776445904003516, loss:0.4600883556594427\n",
      "W1:4.78887371891781, W2:1.2627484405140679, bias:-2.67842279109742, loss:0.4600423938329859\n",
      "W1:4.790869598078822, W2:1.2627082005253822, bias:-2.679200699761571, loss:0.4599964656617393\n",
      "W1:4.792864749142449, W2:1.2626679695174805, bias:-2.679978316619565, loss:0.4599505711169632\n",
      "W1:4.7948591724335365, W2:1.2626277476041063, bias:-2.680755641897712, loss:0.45990471016994544\n",
      "W1:4.796852868276853, W2:1.2625875348984745, bias:-2.681532675821875, loss:0.45985888279200143\n",
      "W1:4.798845836997096, W2:1.2625473315132723, bias:-2.682309418617471, loss:0.459813088954474\n",
      "W1:4.800838078918884, W2:1.2625071375606614, bias:-2.683085870509472, loss:0.4597673286287339\n",
      "W1:4.802829594366763, W2:1.26246695315228, bias:-2.683862031722406, loss:0.4597216017861786\n",
      "W1:4.804820383665203, W2:1.2624267783992453, bias:-2.68463790248036, loss:0.459675908398234\n",
      "W1:4.8068104471385995, W2:1.2623866134121542, bias:-2.6854134830069794, loss:0.45963024843635253\n",
      "W1:4.808799785111271, W2:1.2623464583010864, bias:-2.6861887735254704, loss:0.45958462187201493\n",
      "W1:4.810788397907459, W2:1.2623063131756054, bias:-2.6869637742586012, loss:0.4595390286767286\n",
      "W1:4.81277628585133, W2:1.2622661781447613, bias:-2.6877384854287034, loss:0.4594934688220285\n",
      "W1:4.814763449266971, W2:1.262226053317092, bias:-2.6885129072576732, loss:0.45944794227947683\n",
      "W1:4.8167498884783955, W2:1.2621859388006251, bias:-2.689287039966973, loss:0.45940244902066335\n",
      "W1:4.818735603809536, W2:1.2621458347028807, bias:-2.690060883777632, loss:0.4593569890172046\n",
      "W1:4.820720595584249, W2:1.262105741130872, bias:-2.6908344389102483, loss:0.4593115622407451\n",
      "W1:4.822704864126312, W2:1.2620656581911083, bias:-2.6916077055849907, loss:0.4592661686629555\n",
      "W1:4.824688409759423, W2:1.2620255859895957, bias:-2.6923806840215985, loss:0.4592208082555345\n",
      "W1:4.826671232807203, W2:1.26198552463184, bias:-2.6931533744393836, loss:0.4591754809902076\n",
      "W1:4.828653333593193, W2:1.2619454742228486, bias:-2.693925777057232, loss:0.4591301868387271\n",
      "W1:4.830634712440855, W2:1.2619054348671312, bias:-2.6946978920936053, loss:0.45908492577287324\n",
      "W1:4.832615369673569, W2:1.261865406668703, bias:-2.695469719766541, loss:0.4590396977644517\n",
      "W1:4.834595305614638, W2:1.2618253897310852, bias:-2.6962412602936556, loss:0.45899450278529713\n",
      "W1:4.836574520587282, W2:1.2617853841573081, bias:-2.6970125138921435, loss:0.45894934080726996\n",
      "W1:4.838553014914643, W2:1.2617453900499125, bias:-2.6977834807787797, loss:0.4589042118022574\n",
      "W1:4.8405307889197795, W2:1.2617054075109506, bias:-2.6985541611699215, loss:0.4588591157421746\n",
      "W1:4.84250784292567, W2:1.2616654366419895, bias:-2.699324555281509, loss:0.4588140525989626\n",
      "W1:4.844484177255211, W2:1.2616254775441116, bias:-2.700094663329067, loss:0.45876902234458966\n",
      "W1:4.846459792231217, W2:1.2615855303179169, bias:-2.7008644855277044, loss:0.45872402495105097\n",
      "W1:4.84843468817642, W2:1.2615455950635248, bias:-2.7016340220921187, loss:0.4586790603903685\n",
      "W1:4.8504088654134705, W2:1.2615056718805762, bias:-2.702403273236594, loss:0.4586341286345906\n",
      "W1:4.852382324264935, W2:1.2614657608682345, bias:-2.703172239175005, loss:0.45858922965579313\n",
      "W1:4.854355065053299, W2:1.2614258621251881, bias:-2.7039409201208158, loss:0.4585443634260779\n",
      "W1:4.856327088100961, W2:1.261385975749652, bias:-2.7047093162870834, loss:0.45849952991757364\n",
      "W1:4.858298393730238, W2:1.261346101839369, bias:-2.7054774278864575, loss:0.4584547291024359\n",
      "W1:4.8602689822633645, W2:1.2613062404916122, bias:-2.7062452551311815, loss:0.45840996095284653\n",
      "W1:4.862238854022489, W2:1.2612663918031866, bias:-2.7070127982330954, loss:0.4583652254410148\n",
      "W1:4.8642080093296745, W2:1.2612265558704305, bias:-2.7077800574036353, loss:0.4583205225391749\n",
      "W1:4.866176448506901, W2:1.2611867327892174, bias:-2.708547032853836, loss:0.45827585221958916\n",
      "W1:4.868144171876063, W2:1.2611469226549579, bias:-2.7093137247943306, loss:0.4582312144545458\n",
      "W1:4.870111179758968, W2:1.2611071255626012, bias:-2.7100801334353535, loss:0.45818660921635956\n",
      "W1:4.87207747247734, W2:1.2610673416066367, bias:-2.71084625898674, loss:0.4581420364773713\n",
      "W1:4.874043050352816, W2:1.2610275708810963, bias:-2.7116121016579298, loss:0.4580974962099487\n",
      "W1:4.876007913706946, W2:1.2609878134795556, bias:-2.712377661657965, loss:0.45805298838648584\n",
      "W1:4.877972062861196, W2:1.2609480694951354, bias:-2.7131429391954938, loss:0.4580085129794028\n",
      "W1:4.879935498136942, W2:1.2609083390205043, bias:-2.7139079344787707, loss:0.45796406996114636\n",
      "W1:4.881898219855475, W2:1.260868622147879, bias:-2.714672647715658, loss:0.45791965930418943\n",
      "W1:4.883860228337998, W2:1.2608289189690276, bias:-2.7154370791136277, loss:0.457875280981031\n",
      "W1:4.8858215239056255, W2:1.2607892295752698, bias:-2.716201228879761, loss:0.4578309349641969\n",
      "W1:4.887782106879387, W2:1.2607495540574798, bias:-2.7169650972207497, loss:0.4577866212262384\n",
      "W1:4.88974197758022, W2:1.260709892506087, bias:-2.7177286843429, loss:0.45774233973973344\n",
      "W1:4.891701136328977, W2:1.2606702450110778, bias:-2.71849199045213, loss:0.45769809047728616\n",
      "W1:4.893659583446418, W2:1.260630611661998, bias:-2.7192550157539737, loss:0.4576538734115264\n",
      "W1:4.895617319253217, W2:1.260590992547954, bias:-2.72001776045358, loss:0.45760968851511064\n",
      "W1:4.897574344069959, W2:1.2605513877576136, bias:-2.7207802247557162, loss:0.45756553576072084\n",
      "W1:4.899530658217137, W2:1.260511797379209, bias:-2.721542408864767, loss:0.45752141512106576\n",
      "W1:4.9014862620151565, W2:1.2604722215005382, bias:-2.7223043129847366, loss:0.45747732656887946\n",
      "W1:4.903441155784332, W2:1.2604326602089653, bias:-2.7230659373192503, loss:0.4574332700769222\n",
      "W1:4.905395339844887, W2:1.260393113591424, bias:-2.723827282071555, loss:0.45738924561798044\n",
      "W1:4.907348814516956, W2:1.2603535817344176, bias:-2.72458834744452, loss:0.4573452531648661\n",
      "W1:4.909301580120581, W2:1.2603140647240216, bias:-2.725349133640639, loss:0.45730129269041764\n",
      "W1:4.911253636975714, W2:1.2602745626458849, bias:-2.726109640862031, loss:0.4572573641674989\n",
      "W1:4.913204985402215, W2:1.2602350755852316, bias:-2.726869869310441, loss:0.45721346756899983\n",
      "W1:4.915155625719854, W2:1.2601956036268624, bias:-2.727629819187242, loss:0.4571696028678357\n",
      "W1:4.9171055582483065, W2:1.2601561468551563, bias:-2.7283894906934347, loss:0.4571257700369481\n",
      "W1:4.919054783307158, W2:1.2601167053540718, bias:-2.7291488840296503, loss:0.4570819690493046\n",
      "W1:4.9210033012159, W2:1.2600772792071495, bias:-2.7299079993961497, loss:0.4570381998778977\n",
      "W1:4.922951112293934, W2:1.2600378684975122, bias:-2.7306668369928273, loss:0.4569944624957463\n",
      "W1:4.924898216860565, W2:1.2599984733078677, bias:-2.731425397019209, loss:0.45695075687589437\n",
      "W1:4.926844615235008, W2:1.2599590937205098, bias:-2.7321836796744554, loss:0.45690708299141214\n",
      "W1:4.928790307736382, W2:1.25991972981732, bias:-2.732941685157363, loss:0.4568634408153951\n",
      "W1:4.930735294683716, W2:1.2598803816797683, bias:-2.7336994136663635, loss:0.4568198303209646\n",
      "W1:4.932679576395941, W2:1.2598410493889165, bias:-2.7344568653995265, loss:0.4567762514812671\n",
      "W1:4.934623153191897, W2:1.2598017330254179, bias:-2.73521404055456, loss:0.45673270426947515\n",
      "W1:4.936566025390327, W2:1.2597624326695198, bias:-2.7359709393288125, loss:0.4566891886587863\n",
      "W1:4.938508193309882, W2:1.259723148401065, bias:-2.7367275619192717, loss:0.45664570462242404\n",
      "W1:4.9404496572691174, W2:1.2596838802994925, bias:-2.7374839085225675, loss:0.45660225213363703\n",
      "W1:4.942390417586491, W2:1.2596446284438403, bias:-2.738239979334973, loss:0.4565588311656995\n",
      "W1:4.94433047458037, W2:1.2596053929127455, bias:-2.7389957745524054, loss:0.4565154416919109\n",
      "W1:4.9462698285690205, W2:1.2595661737844475, bias:-2.7397512943704263, loss:0.45647208368559616\n",
      "W1:4.948208479870618, W2:1.2595269711367874, bias:-2.7405065389842433, loss:0.45642875712010594\n",
      "W1:4.950146428803239, W2:1.2594877850472115, bias:-2.7412615085887118, loss:0.4563854619688154\n",
      "W1:4.9520836756848645, W2:1.2594486155927715, bias:-2.742016203378334, loss:0.45634219820512567\n",
      "W1:4.954020220833379, W2:1.2594094628501264, bias:-2.7427706235472633, loss:0.45629896580246304\n",
      "W1:4.9559560645665695, W2:1.259370326895544, bias:-2.7435247692893014, loss:0.45625576473427887\n",
      "W1:4.9578912072021275, W2:1.2593312078049024, bias:-2.7442786407979027, loss:0.4562125949740498\n",
      "W1:4.959825649057646, W2:1.2592921056536912, bias:-2.7450322382661727, loss:0.4561694564952774\n",
      "W1:4.961759390450621, W2:1.2592530205170132, bias:-2.7457855618868714, loss:0.4561263492714891\n",
      "W1:4.963692431698451, W2:1.2592139524695858, bias:-2.746538611852413, loss:0.45608327327623654\n",
      "W1:4.965624773118437, W2:1.2591749015857423, bias:-2.747291388354866, loss:0.45604022848309744\n",
      "W1:4.96755641502778, W2:1.2591358679394338, bias:-2.7480438915859575, loss:0.45599721486567385\n",
      "W1:4.969487357743586, W2:1.25909685160423, bias:-2.74879612173707, loss:0.45595423239759314\n",
      "W1:4.971417601582859, W2:1.2590578526533212, bias:-2.749548078999246, loss:0.4559112810525077\n",
      "W1:4.973347146862506, W2:1.2590188711595192, bias:-2.7502997635631865, loss:0.4558683608040951\n",
      "W1:4.975275993899333, W2:1.2589799071952592, bias:-2.7510511756192537, loss:0.4558254716260573\n",
      "W1:4.977204143010051, W2:1.2589409608326012, bias:-2.7518023153574713, loss:0.4557826134921222\n",
      "W1:4.979131594511268, W2:1.2589020321432307, bias:-2.7525531829675254, loss:0.45573978637604157\n",
      "W1:4.981058348719493, W2:1.258863121198461, bias:-2.7533037786387657, loss:0.4556969902515927\n",
      "W1:4.982984405951136, W2:1.2588242280692343, bias:-2.7540541025602066, loss:0.45565422509257764\n",
      "W1:4.984909766522506, W2:1.2587853528261228, bias:-2.754804154920528, loss:0.4556114908728231\n",
      "W1:4.986834430749812, W2:1.2587464955393306, bias:-2.7555539359080763, loss:0.45556878756618074\n",
      "W1:4.9887583989491615, W2:1.2587076562786945, bias:-2.7563034457108655, loss:0.455526115146527\n",
      "W1:4.990681671436563, W2:1.2586688351136859, bias:-2.7570526845165784, loss:0.45548347358776337\n",
      "W1:4.992604248527923, W2:1.2586300321134118, bias:-2.7578016525125673, loss:0.4554408628638154\n",
      "W1:4.994526130539046, W2:1.2585912473466168, bias:-2.7585503498858546, loss:0.455398282948634\n",
      "W1:4.996447317785637, W2:1.2585524808816833, bias:-2.7592987768231345, loss:0.45535573381619443\n",
      "W1:4.998367810583298, W2:1.258513732786634, bias:-2.7600469335107736, loss:0.4553132154404965\n",
      "W1:5.000287609247531, W2:1.2584750031291332, bias:-2.7607948201348123, loss:0.4552707277955653\n",
      "W1:5.002206714093733, W2:1.258436291976487, bias:-2.7615424368809647, loss:0.45522827085544965\n",
      "W1:5.0041251254372, W2:1.2583975993956462, bias:-2.7622897839346208, loss:0.45518584459422357\n",
      "W1:5.006042843593128, W2:1.2583589254532062, bias:-2.7630368614808467, loss:0.4551434489859854\n",
      "W1:5.007959868876607, W2:1.2583202702154095, bias:-2.7637836697043863, loss:0.45510108400485794\n",
      "W1:5.009876201602625, W2:1.2582816337481462, bias:-2.764530208789661, loss:0.4550587496249886\n",
      "W1:5.011791842086067, W2:1.258243016116956, bias:-2.7652764789207716, loss:0.4550164458205495\n",
      "W1:5.0137067906417165, W2:1.2582044173870293, bias:-2.7660224802814994, loss:0.4549741725657366\n",
      "W1:5.0156210475842515, W2:1.2581658376232079, bias:-2.7667682130553066, loss:0.4549319298347711\n",
      "W1:5.017534613228247, W2:1.2581272768899874, bias:-2.7675136774253373, loss:0.4548897176018978\n",
      "W1:5.019447487888175, W2:1.2580887352515175, bias:-2.7682588735744185, loss:0.4548475358413866\n",
      "W1:5.0213596718784, W2:1.2580502127716042, bias:-2.7690038016850615, loss:0.4548053845275309\n",
      "W1:5.023271165513187, W2:1.2580117095137104, bias:-2.7697484619394626, loss:0.45476326363464925\n",
      "W1:5.025181969106694, W2:1.2579732255409577, bias:-2.7704928545195027, loss:0.4547211731370843\n",
      "W1:5.027092082972975, W2:1.2579347609161269, bias:-2.7712369796067504, loss:0.45467911300920244\n",
      "W1:5.029001507425977, W2:1.2578963157016603, bias:-2.771980837382462, loss:0.4546370832253949\n",
      "W1:5.030910242779545, W2:1.2578578899596626, bias:-2.772724428027582, loss:0.454595083760077\n",
      "W1:5.032818289347419, W2:1.257819483751902, bias:-2.773467751722744, loss:0.45455311458768777\n",
      "W1:5.03472564744323, W2:1.2577810971398111, bias:-2.7742108086482733, loss:0.4545111756826916\n",
      "W1:5.036632317380505, W2:1.2577427301844895, bias:-2.7749535989841845, loss:0.45446926701957574\n",
      "W1:5.038538299472668, W2:1.2577043829467034, bias:-2.7756961229101855, loss:0.454427388572852\n",
      "W1:5.040443594033033, W2:1.2576660554868881, bias:-2.776438380605678, loss:0.45438554031705664\n",
      "W1:5.042348201374811, W2:1.257627747865149, bias:-2.777180372249756, loss:0.4543437222267494\n",
      "W1:5.044252121811103, W2:1.2575894601412623, bias:-2.777922098021209, loss:0.45430193427651494\n",
      "W1:5.046155355654907, W2:1.2575511923746767, bias:-2.7786635580985237, loss:0.4542601764409609\n",
      "W1:5.048057903219113, W2:1.2575129446245148, bias:-2.7794047526598815, loss:0.45421844869471956\n",
      "W1:5.049959764816503, W2:1.257474716949574, bias:-2.7801456818831625, loss:0.45417675101244687\n",
      "W1:5.051860940759753, W2:1.2574365094083275, bias:-2.7808863459459445, loss:0.45413508336882297\n",
      "W1:5.0537614313614325, W2:1.2573983220589264, bias:-2.7816267450255054, loss:0.4540934457385519\n",
      "W1:5.055661236934002, W2:1.2573601549592002, bias:-2.782366879298823, loss:0.4540518380963615\n",
      "W1:5.057560357789814, W2:1.2573220081666578, bias:-2.783106748942576, loss:0.45401026041700326\n",
      "W1:5.059458794241115, W2:1.25728388173849, bias:-2.7838463541331455, loss:0.45396871267525307\n",
      "W1:5.061356546600042, W2:1.257245775731569, bias:-2.784585695046615, loss:0.4539271948459102\n",
      "W1:5.063253615178624, W2:1.2572076902024507, bias:-2.7853247718587713, loss:0.45388570690379754\n",
      "W1:5.065150000288782, W2:1.257169625207376, bias:-2.7860635847451065, loss:0.4538442488237626\n",
      "W1:5.067045702242329, W2:1.2571315808022716, bias:-2.786802133880818, loss:0.4538028205806757\n",
      "W1:5.068940721350969, W2:1.2570935570427508, bias:-2.78754041944081, loss:0.4537614221494314\n",
      "W1:5.070835057926295, W2:1.2570555539841155, bias:-2.7882784415996924, loss:0.45372005350494776\n",
      "W1:5.072728712279793, W2:1.2570175716813567, bias:-2.7890162005317842, loss:0.4536787146221666\n",
      "W1:5.074621684722841, W2:1.2569796101891566, bias:-2.789753696411113, loss:0.4536374054760533\n",
      "W1:5.076513975566704, W2:1.2569416695618887, bias:-2.790490929411416, loss:0.45359612604159716\n",
      "W1:5.078405585122541, W2:1.2569037498536195, bias:-2.791227899706141, loss:0.45355487629381075\n",
      "W1:5.0802965137013985, W2:1.2568658511181097, bias:-2.791964607468447, loss:0.4535136562077302\n",
      "W1:5.0821867616142145, W2:1.2568279734088155, bias:-2.792701052871206, loss:0.45347246575841543\n",
      "W1:5.0840763291718165, W2:1.2567901167788893, bias:-2.7934372360870023, loss:0.45343130492094963\n",
      "W1:5.085965216684922, W2:1.2567522812811813, bias:-2.794173157288134, loss:0.45339017367043977\n",
      "W1:5.0878534244641385, W2:1.2567144669682402, bias:-2.794908816646614, loss:0.4533490719820162\n",
      "W1:5.089740952819961, W2:1.256676673892315, bias:-2.795644214334171, loss:0.4533079998308326\n",
      "W1:5.091627802062774, W2:1.2566389021053554, bias:-2.7963793505222503, loss:0.45326695719206617\n",
      "W1:5.093513972502855, W2:1.2566011516590136, bias:-2.7971142253820136, loss:0.45322594404091754\n",
      "W1:5.095399464450365, W2:1.256563422604645, bias:-2.7978488390843417, loss:0.45318496035261063\n",
      "W1:5.097284278215357, W2:1.2565257149933093, bias:-2.798583191799833, loss:0.45314400610239275\n",
      "W1:5.099168414107771, W2:1.2564880288757723, bias:-2.7993172836988065, loss:0.45310308126553467\n",
      "W1:5.101051872437437, W2:1.256450364302506, bias:-2.800051114951301, loss:0.4530621858173305\n",
      "W1:5.1029346535140725, W2:1.2564127213236904, bias:-2.8007846857270775, loss:0.4530213197330971\n",
      "W1:5.104816757647282, W2:1.2563750999892147, bias:-2.8015179961956176, loss:0.45298048298817517\n",
      "W1:5.106698185146561, W2:1.256337500348678, bias:-2.802251046526127, loss:0.4529396755579288\n",
      "W1:5.108578936321289, W2:1.2562999224513907, bias:-2.8029838368875346, loss:0.45289889741774464\n",
      "W1:5.110459011480734, W2:1.2562623663463752, bias:-2.803716367448494, loss:0.4528581485430328\n",
      "W1:5.112338410934054, W2:1.2562248320823677, bias:-2.8044486383773837, loss:0.4528174289092268\n",
      "W1:5.114217134990292, W2:1.2561873197078186, bias:-2.8051806498423084, loss:0.452776738491783\n",
      "W1:5.116095183958378, W2:1.2561498292708941, bias:-2.8059124020111, loss:0.45273607726618115\n",
      "W1:5.11797255814713, W2:1.2561123608194769, bias:-2.8066438950513177, loss:0.4526954452079235\n",
      "W1:5.119849257865254, W2:1.2560749144011674, bias:-2.807375129130249, loss:0.4526548422925364\n",
      "W1:5.121725283421339, W2:1.2560374900632851, bias:-2.808106104414911, loss:0.45261426849556835\n",
      "W1:5.123600635123865, W2:1.2560000878528692, bias:-2.8088368210720502, loss:0.45257372379259125\n",
      "W1:5.125475313281195, W2:1.2559627078166802, bias:-2.8095672792681445, loss:0.45253320815919973\n",
      "W1:5.12734931820158, W2:1.2559253500012002, bias:-2.8102974791694026, loss:0.4524927215710118\n",
      "W1:5.129222650193157, W2:1.2558880144526348, bias:-2.8110274209417665, loss:0.45245226400366806\n",
      "W1:5.131095309563946, W2:1.2558507012169136, bias:-2.8117571047509102, loss:0.4524118354328324\n",
      "W1:5.132967296621858, W2:1.2558134103396916, bias:-2.8124865307622424, loss:0.4523714358341913\n",
      "W1:5.1348386116746845, W2:1.2557761418663502, bias:-2.8132156991409056, loss:0.45233106518345406\n",
      "W1:5.136709255030107, W2:1.2557388958419977, bias:-2.813944610051778, loss:0.4522907234563533\n",
      "W1:5.13857922699569, W2:1.2557016723114713, bias:-2.8146732636594747, loss:0.4522504106286441\n",
      "W1:5.140448527878882, W2:1.2556644713193374, bias:-2.815401660128346, loss:0.4522101266761042\n",
      "W1:5.14231715798702, W2:1.255627292909893, bias:-2.8161297996224812, loss:0.45216987157453475\n",
      "W1:5.144185117627323, W2:1.2555901371271667, bias:-2.8168576823057077, loss:0.4521296452997589\n",
      "W1:5.146052407106896, W2:1.2555530040149194, bias:-2.8175853083415916, loss:0.45208944782762295\n",
      "W1:5.147919026732728, W2:1.2555158936166457, bias:-2.8183126778934393, loss:0.4520492791339963\n",
      "W1:5.149784976811693, W2:1.255478805975575, bias:-2.8190397911242973, loss:0.45200913919477004\n",
      "W1:5.15165025765055, W2:1.255441741134672, bias:-2.819766648196954, loss:0.4519690279858592\n",
      "W1:5.1535148695559405, W2:1.2554046991366385, bias:-2.8204932492739396, loss:0.4519289454832002\n",
      "W1:5.1553788128343925, W2:1.2553676800239137, bias:-2.8212195945175274, loss:0.45188889166275287\n",
      "W1:5.157242087792316, W2:1.2553306838386755, bias:-2.821945684089734, loss:0.4518488665004995\n",
      "W1:5.159104694736004, W2:1.2552937106228412, bias:-2.8226715181523203, loss:0.4518088699724448\n",
      "W1:5.160966633971636, W2:1.2552567604180696, bias:-2.8233970968667923, loss:0.45176890205461606\n",
      "W1:5.1628279058052735, W2:1.2552198332657603, bias:-2.8241224203944015, loss:0.45172896272306357\n",
      "W1:5.164688510542861, W2:1.255182929207056, bias:-2.8248474888961463, loss:0.45168905195385917\n",
      "W1:5.166548448490227, W2:1.2551460482828432, bias:-2.8255723025327724, loss:0.45164916972309826\n",
      "W1:5.168407719953083, W2:1.2551091905337528, bias:-2.8262968614647725, loss:0.45160931600689785\n",
      "W1:5.170266325237024, W2:1.2550723560001618, bias:-2.8270211658523885, loss:0.4515694907813981\n",
      "W1:5.172124264647525, W2:1.2550355447221933, bias:-2.827745215855612, loss:0.4515296940227607\n",
      "W1:5.173981538489949, W2:1.2549987567397183, bias:-2.8284690116341835, loss:0.4514899257071707\n",
      "W1:5.175838147069537, W2:1.2549619920923565, bias:-2.8291925533475957, loss:0.451450185810835\n",
      "W1:5.177694090691414, W2:1.2549252508194768, bias:-2.829915841155092, loss:0.451410474309983\n",
      "W1:5.179549369660588, W2:1.254888532960199, bias:-2.830638875215668, loss:0.4513707911808661\n",
      "W1:5.181403984281949, W2:1.2548518385533944, bias:-2.831361655688072, loss:0.4513311363997584\n",
      "W1:5.1832579348602685, W2:1.2548151676376866, bias:-2.8320841827308065, loss:0.45129150994295647\n",
      "W1:5.1851112217002004, W2:1.2547785202514525, bias:-2.8328064565021274, loss:0.4512519117867782\n",
      "W1:5.1869638451062805, W2:1.2547418964328236, bias:-2.833528477160046, loss:0.4512123419075648\n",
      "W1:5.188815805382926, W2:1.2547052962196867, bias:-2.8342502448623295, loss:0.4511728002816792\n",
      "W1:5.1906671028344356, W2:1.254668719649685, bias:-2.8349717597665007, loss:0.45113328688550625\n",
      "W1:5.19251773776499, W2:1.2546321667602187, bias:-2.8356930220298406, loss:0.45109380169545366\n",
      "W1:5.194367710478652, W2:1.2545956375884462, bias:-2.8364140318093867, loss:0.45105434468795075\n",
      "W1:5.196217021279363, W2:1.254559132171285, bias:-2.8371347892619356, loss:0.4510149158394489\n",
      "W1:5.198065670470949, W2:1.2545226505454128, bias:-2.8378552945440427, loss:0.45097551512642214\n",
      "W1:5.199913658357113, W2:1.2544861927472681, bias:-2.8385755478120234, loss:0.4509361425253661\n",
      "W1:5.201760985241443, W2:1.2544497588130514, bias:-2.839295549221953, loss:0.4508967980127986\n",
      "W1:5.203607651427405, W2:1.254413348778726, bias:-2.840015298929669, loss:0.45085748156525934\n",
      "W1:5.205453657218346, W2:1.2543769626800187, bias:-2.8407347970907697, loss:0.4508181931593104\n",
      "W1:5.207299002917495, W2:1.2543406005524216, bias:-2.8414540438606157, loss:0.45077893277153563\n",
      "W1:5.209143688827958, W2:1.2543042624311918, bias:-2.842173039394331, loss:0.45073970037854116\n",
      "W1:5.210987715252727, W2:1.2542679483513528, bias:-2.842891783846804, loss:0.4507004959569539\n",
      "W1:5.212831082494668, W2:1.2542316583476962, bias:-2.8436102773726875, loss:0.4506613194834243\n",
      "W1:5.214673790856532, W2:1.2541953924547813, bias:-2.844328520126398, loss:0.4506221709346238\n",
      "W1:5.216515840640946, W2:1.2541591507069365, bias:-2.845046512262119, loss:0.4505830502872458\n",
      "W1:5.21835723215042, W2:1.2541229331382608, bias:-2.8457642539338, loss:0.4505439575180057\n",
      "W1:5.220197965687341, W2:1.254086739782624, bias:-2.8464817452951583, loss:0.4505048926036407\n",
      "W1:5.222038041553978, W2:1.254050570673668, bias:-2.8471989864996776, loss:0.4504658555209094\n",
      "W1:5.223877460052479, W2:1.254014425844807, bias:-2.8479159777006107, loss:0.45042684624659307\n",
      "W1:5.225716221484868, W2:1.2539783053292288, bias:-2.84863271905098, loss:0.45038786475749415\n",
      "W1:5.2275543261530535, W2:1.2539422091598966, bias:-2.8493492107035765, loss:0.45034891103043684\n",
      "W1:5.229391774358819, W2:1.2539061373695484, bias:-2.850065452810962, loss:0.450309985042267\n",
      "W1:5.231228566403831, W2:1.2538700899906985, bias:-2.8507814455254694, loss:0.45027108676985234\n",
      "W1:5.233064702589631, W2:1.2538340670556385, bias:-2.851497188999203, loss:0.45023221619008263\n",
      "W1:5.234900183217641, W2:1.253798068596438, bias:-2.8522126833840393, loss:0.45019337327986847\n",
      "W1:5.236735008589162, W2:1.2537620946449457, bias:-2.852927928831628, loss:0.45015455801614285\n",
      "W1:5.238569179005373, W2:1.25372614523279, bias:-2.853642925493391, loss:0.4501157703758599\n",
      "W1:5.240402694767333, W2:1.2536902203913796, bias:-2.8543576735205263, loss:0.4500770103359954\n",
      "W1:5.242235556175977, W2:1.2536543201519053, bias:-2.855072173064005, loss:0.4500382778735472\n",
      "W1:5.2440677635321205, W2:1.2536184445453398, bias:-2.855786424274575, loss:0.4499995729655339\n",
      "W1:5.245899317136455, W2:1.2535825936024394, bias:-2.856500427302759, loss:0.44996089558899616\n",
      "W1:5.247730217289553, W2:1.2535467673537442, bias:-2.857214182298857, loss:0.44992224572099604\n",
      "W1:5.2495604642918625, W2:1.2535109658295789, bias:-2.8579276894129455, loss:0.44988362333861737\n",
      "W1:5.251390058443709, W2:1.2534751890600546, bias:-2.85864094879488, loss:0.44984502841896457\n",
      "W1:5.2532190000452985, W2:1.2534394370750686, bias:-2.859353960594293, loss:0.4498064609391644\n",
      "W1:5.255047289396713, W2:1.2534037099043056, bias:-2.860066724960598, loss:0.4497679208763646\n",
      "W1:5.256874926797912, W2:1.2533680075772389, bias:-2.8607792420429865, loss:0.4497294082077344\n",
      "W1:5.258701912548733, W2:1.2533323301231305, bias:-2.861491511990431, loss:0.44969092291046436\n",
      "W1:5.26052824694889, W2:1.2532966775710326, bias:-2.862203534951685, loss:0.4496524649617666\n",
      "W1:5.262353930297975, W2:1.2532610499497878, bias:-2.862915311075283, loss:0.4496140343388743\n",
      "W1:5.264178962895457, W2:1.2532254472880306, bias:-2.8636268405095424, loss:0.4495756310190423\n",
      "W1:5.266003345040683, W2:1.2531898696141879, bias:-2.864338123402563, loss:0.44953725497954616\n",
      "W1:5.267827077032876, W2:1.2531543169564796, bias:-2.8650491599022274, loss:0.44949890619768323\n",
      "W1:5.269650159171135, W2:1.2531187893429196, bias:-2.8657599501562028, loss:0.4494605846507717\n",
      "W1:5.2714725917544385, W2:1.2530832868013169, bias:-2.866470494311941, loss:0.44942229031615155\n",
      "W1:5.273294375081639, W2:1.2530478093592756, bias:-2.867180792516678, loss:0.4493840231711834\n",
      "W1:5.275115509451466, W2:1.2530123570441971, bias:-2.867890844917437, loss:0.4493457831932494\n",
      "W1:5.276935995162528, W2:1.2529769298832791, bias:-2.868600651661026, loss:0.44930757035975266\n",
      "W1:5.278755832513308, W2:1.252941527903518, bias:-2.8693102128940406, loss:0.44926938464811744\n",
      "W1:5.280575021802165, W2:1.2529061511317088, bias:-2.8700195287628643, loss:0.44923122603578935\n",
      "W1:5.282393563327335, W2:1.252870799594446, bias:-2.8707285994136673, loss:0.4491930945002345\n",
      "W1:5.284211457386929, W2:1.2528354733181246, bias:-2.87143742499241, loss:0.44915499001894116\n",
      "W1:5.286028704278936, W2:1.252800172328941, bias:-2.872146005644841, loss:0.44911691256941727\n",
      "W1:5.287845304301219, W2:1.2527648966528937, bias:-2.872854341516499, loss:0.44907886212919323\n",
      "W1:5.2896612577515185, W2:1.2527296463157833, bias:-2.873562432752713, loss:0.44904083867581934\n",
      "W1:5.29147656492745, W2:1.2526944213432147, bias:-2.874270279498603, loss:0.44900284218686737\n",
      "W1:5.293291226126505, W2:1.2526592217605967, bias:-2.8749778818990808, loss:0.44896487263993\n",
      "W1:5.295105241646051, W2:1.2526240475931432, bias:-2.8756852400988495, loss:0.44892693001262085\n",
      "W1:5.29691861178333, W2:1.2525888988658742, bias:-2.8763923542424057, loss:0.44888901428257433\n",
      "W1:5.298731336835459, W2:1.2525537756036162, bias:-2.8770992244740388, loss:0.4488511254274463\n",
      "W1:5.300543417099433, W2:1.2525186778310033, bias:-2.877805850937832, loss:0.4488132634249127\n",
      "W1:5.30235485287212, W2:1.2524836055724775, bias:-2.878512233777663, loss:0.44877542825267086\n",
      "W1:5.304165644450263, W2:1.2524485588522898, bias:-2.8792183731372045, loss:0.44873761988843874\n",
      "W1:5.305975792130482, W2:1.252413537694501, bias:-2.879924269159924, loss:0.4486998383099555\n",
      "W1:5.30778529620927, W2:1.2523785421229823, bias:-2.880629921989086, loss:0.44866208349498043\n",
      "W1:5.309594156982997, W2:1.252343572161416, bias:-2.8813353317677506, loss:0.44862435542129425\n",
      "W1:5.311402374747906, W2:1.2523086278332964, bias:-2.882040498638776, loss:0.4485866540666981\n",
      "W1:5.3132099498001155, W2:1.2522737091619307, bias:-2.8827454227448173, loss:0.4485489794090139\n",
      "W1:5.31501688243562, W2:1.2522388161704392, bias:-2.883450104228328, loss:0.44851133142608446\n",
      "W1:5.3168231729502855, W2:1.2522039488817565, bias:-2.884154543231561, loss:0.44847371009577286\n",
      "W1:5.318628821639855, W2:1.2521691073186323, bias:-2.884858739896568, loss:0.4484361153959634\n",
      "W1:5.3204338287999455, W2:1.2521342915036315, bias:-2.8855626943652, loss:0.44839854730456064\n",
      "W1:5.322238194726048, W2:1.252099501459136, bias:-2.88626640677911, loss:0.44836100579948995\n",
      "W1:5.324041919713529, W2:1.2520647372073443, bias:-2.8869698772797503, loss:0.4483234908586974\n",
      "W1:5.325845004057627, W2:1.2520299987702732, bias:-2.8876731060083753, loss:0.44828600246014944\n",
      "W1:5.327647448053457, W2:1.2519952861697574, bias:-2.888376093106042, loss:0.4482485405818331\n",
      "W1:5.329449251996007, W2:1.2519605994274516, bias:-2.8890788387136093, loss:0.4482111052017563\n",
      "W1:5.331250416180137, W2:1.25192593856483, bias:-2.8897813429717396, loss:0.4481736962979467\n",
      "W1:5.333050940900585, W2:1.251891303603188, bias:-2.8904836060208985, loss:0.44813631384845376\n",
      "W1:5.33485082645196, W2:1.251856694563642, bias:-2.891185628001356, loss:0.44809895783134623\n",
      "W1:5.3366500731287445, W2:1.2518221114671308, bias:-2.891887409053187, loss:0.44806162822471374\n",
      "W1:5.338448681225297, W2:1.251787554334416, bias:-2.8925889493162713, loss:0.4480243250066666\n",
      "W1:5.340246651035849, W2:1.2517530231860827, bias:-2.8932902489302945, loss:0.4479870481553354\n",
      "W1:5.342043982854503, W2:1.2517185180425405, bias:-2.8939913080347486, loss:0.44794979764887094\n",
      "W1:5.343840676975239, W2:1.2516840389240238, bias:-2.894692126768932, loss:0.4479125734654447\n",
      "W1:5.345636733691906, W2:1.2516495858505927, bias:-2.895392705271951, loss:0.4478753755832484\n",
      "W1:5.3474321532982305, W2:1.2516151588421338, bias:-2.8960930436827192, loss:0.44783820398049395\n",
      "W1:5.349226936087809, W2:1.2515807579183607, bias:-2.896793142139959, loss:0.4478010586354138\n",
      "W1:5.351021082354113, W2:1.2515463830988145, bias:-2.8974930007822017, loss:0.447763939526261\n",
      "W1:5.352814592390487, W2:1.2515120344028652, bias:-2.8981926197477876, loss:0.4477268466313079\n",
      "W1:5.354607466490148, W2:1.2514777118497118, bias:-2.8988919991748667, loss:0.4476897799288482\n",
      "W1:5.3563997049461864, W2:1.2514434154583831, bias:-2.8995911392013998, loss:0.44765273939719524\n",
      "W1:5.358191308051564, W2:1.2514091452477385, bias:-2.9002900399651583, loss:0.44761572501468305\n",
      "W1:5.359982276099119, W2:1.2513749012364683, bias:-2.9009887016037257, loss:0.44757873675966503\n",
      "W1:5.361772609381559, W2:1.251340683443095, bias:-2.9016871242544964, loss:0.44754177461051553\n",
      "W1:5.363562308191465, W2:1.2513064918859733, bias:-2.902385308054678, loss:0.447504838545629\n",
      "W1:5.3653513728212925, W2:1.2512723265832917, bias:-2.9030832531412907, loss:0.4474679285434199\n",
      "W1:5.367139803563367, W2:1.251238187553072, bias:-2.903780959651168, loss:0.44743104458232247\n",
      "W1:5.368927600709887, W2:1.2512040748131708, bias:-2.904478427720958, loss:0.44739418664079156\n",
      "W1:5.370714764552927, W2:1.25116998838128, bias:-2.9051756574871215, loss:0.4473573546973019\n",
      "W1:5.372501295384429, W2:1.2511359282749273, bias:-2.905872649085936, loss:0.4473205487303483\n",
      "W1:5.374287193496209, W2:1.251101894511477, bias:-2.906569402653494, loss:0.4472837687184455\n",
      "W1:5.376072459179958, W2:1.2510678871081302, bias:-2.9072659183257032, loss:0.44724701464012856\n",
      "W1:5.3778570927272344, W2:1.2510339060819267, bias:-2.907962196238288, loss:0.4472102864739522\n",
      "W1:5.379641094429473, W2:1.250999951449744, bias:-2.9086582365267906, loss:0.44717358419849146\n",
      "W1:5.381424464577978, W2:1.2509660232282995, bias:-2.9093540393265687, loss:0.4471369077923409\n",
      "W1:5.383207203463927, W2:1.2509321214341498, bias:-2.9100496047727993, loss:0.44710025723411545\n",
      "W1:5.3849893113783684, W2:1.2508982460836922, bias:-2.910744933000477, loss:0.44706363250244985\n",
      "W1:5.386770788612224, W2:1.250864397193165, bias:-2.9114400241444156, loss:0.4470270335759986\n",
      "W1:5.388551635456286, W2:1.2508305747786486, bias:-2.912134878339248, loss:0.4469904604334362\n",
      "W1:5.39033185220122, W2:1.2507967788560652, bias:-2.912829495719427, loss:0.44695391305345705\n",
      "W1:5.392111439137561, W2:1.2507630094411808, bias:-2.9135238764192257, loss:0.44691739141477543\n",
      "W1:5.393890396555718, W2:1.2507292665496044, bias:-2.9142180205727373, loss:0.44688089549612525\n",
      "W1:5.3956687247459705, W2:1.2506955501967896, bias:-2.9149119283138765, loss:0.44684442527626034\n",
      "W1:5.397446423998469, W2:1.250661860398035, bias:-2.9156055997763803, loss:0.44680798073395417\n",
      "W1:5.399223494603237, W2:1.2506281971684847, bias:-2.916299035093807, loss:0.4467715618480004\n",
      "W1:5.400999936850168, W2:1.2505945605231288, bias:-2.9169922343995376, loss:0.4467351685972121\n",
      "W1:5.4027757510290275, W2:1.2505609504768043, bias:-2.9176851978267764, loss:0.4466988009604218\n",
      "W1:5.404550937429452, W2:1.2505273670441956, bias:-2.9183779255085516, loss:0.4466624589164825\n",
      "W1:5.4063254963409495, W2:1.2504938102398353, bias:-2.9190704175777142, loss:0.4466261424442662\n",
      "W1:5.408099428052901, W2:1.2504602800781046, bias:-2.919762674166941, loss:0.44658985152266484\n",
      "W1:5.409872732854555, W2:1.250426776573234, bias:-2.9204546954087327, loss:0.44655358613059004\n",
      "W1:5.411645411035034, W2:1.250393299739304, bias:-2.921146481435416, loss:0.4465173462469727\n",
      "W1:5.41341746288333, W2:1.250359849590245, bias:-2.921838032379144, loss:0.4464811318507641\n",
      "W1:5.415188888688307, W2:1.2503264261398395, bias:-2.9225293483718944, loss:0.44644494292093434\n",
      "W1:5.416959688738698, W2:1.250293029401721, bias:-2.9232204295454727, loss:0.44640877943647334\n",
      "W1:5.418729863323112, W2:1.2502596593893756, bias:-2.9239112760315114, loss:0.4463726413763908\n",
      "W1:5.420499412730021, W2:1.2502263161161422, bias:-2.924601887961471, loss:0.44633652871971546\n",
      "W1:5.422268337247775, W2:1.2501929995952135, bias:-2.9252922654666396, loss:0.44630044144549624\n",
      "W1:5.424036637164591, W2:1.250159709839636, bias:-2.9259824086781343, loss:0.44626437953280096\n",
      "W1:5.425804312768559, W2:1.250126446862311, bias:-2.9266723177269007, loss:0.4462283429607172\n",
      "W1:5.427571364347636, W2:1.2500932106759952, bias:-2.9273619927437142, loss:0.44619233170835143\n",
      "W1:5.429337792189653, W2:1.2500600012933012, bias:-2.92805143385918, loss:0.4461563457548309\n",
      "W1:5.43110359658231, W2:1.250026818726698, bias:-2.928740641203734, loss:0.446120385079301\n",
      "W1:5.4328687778131775, W2:1.249993662988512, bias:-2.929429614907642, loss:0.4460844496609269\n",
      "W1:5.434633336169697, W2:1.2499605340909268, bias:-2.9301183551010017, loss:0.4460485394788931\n",
      "W1:5.436397271939181, W2:1.2499274320459846, bias:-2.9308068619137426, loss:0.4460126545124037\n",
      "W1:5.438160585408811, W2:1.2498943568655863, bias:-2.931495135475626, loss:0.445976794740682\n",
      "W1:5.439923276865639, W2:1.2498613085614922, bias:-2.932183175916245, loss:0.4459409601429705\n",
      "W1:5.441685346596588, W2:1.2498282871453226, bias:-2.932870983365027, loss:0.445905150698531\n",
      "W1:5.443446794888452, W2:1.2497952926285585, bias:-2.9335585579512324, loss:0.44586936638664515\n",
      "W1:5.445207622027892, W2:1.2497623250225418, bias:-2.9342458998039547, loss:0.44583360718661263\n",
      "W1:5.446967828301443, W2:1.2497293843384762, bias:-2.9349330090521226, loss:0.44579787307775354\n",
      "W1:5.448727413995508, W2:1.2496964705874278, bias:-2.935619885824499, loss:0.44576216403940694\n",
      "W1:5.450486379396359, W2:1.2496635837803254, bias:-2.9363065302496816, loss:0.4457264800509304\n",
      "W1:5.452244724790142, W2:1.2496307239279612, bias:-2.9369929424561043, loss:0.4456908210917016\n",
      "W1:5.454002450462868, W2:1.2495978910409915, bias:-2.937679122572037, loss:0.44565518714111685\n",
      "W1:5.455759556700421, W2:1.249565085129937, bias:-2.9383650707255855, loss:0.4456195781785919\n",
      "W1:5.457516043788555, W2:1.2495323062051833, bias:-2.9390507870446925, loss:0.4455839941835609\n",
      "W1:5.459271912012892, W2:1.2494995542769822, bias:-2.939736271657138, loss:0.4455484351354786\n",
      "W1:5.461027161658925, W2:1.2494668293554514, bias:-2.94042152469054, loss:0.4455129010138169\n",
      "W1:5.462781793012017, W2:1.2494341314505752, bias:-2.941106546272354, loss:0.44547739179806845\n",
      "W1:5.464535806357399, W2:1.2494014605722055, bias:-2.9417913365298745, loss:0.4454419074677441\n",
      "W1:5.4662892019801745, W2:1.2493688167300616, bias:-2.942475895590235, loss:0.4454064480023739\n",
      "W1:5.468041980165315, W2:1.2493361999337316, bias:-2.9431602235804073, loss:0.4453710133815067\n",
      "W1:5.469794141197661, W2:1.2493036101926722, bias:-2.943844320627204, loss:0.4453356035847111\n",
      "W1:5.4715456853619235, W2:1.24927104751621, bias:-2.9445281868572772, loss:0.4453002185915734\n",
      "W1:5.473296612942683, W2:1.2492385119135416, bias:-2.94521182239712, loss:0.4452648583817003\n",
      "W1:5.475046924224388, W2:1.2492060033937338, bias:-2.945895227373067, loss:0.44522952293471607\n",
      "W1:5.476796619491361, W2:1.2491735219657245, bias:-2.946578401911293, loss:0.445194212230265\n",
      "W1:5.478545699027787, W2:1.2491410676383237, bias:-2.9472613461378145, loss:0.4451589262480097\n",
      "W1:5.480294163117726, W2:1.249108640420213, bias:-2.947944060178491, loss:0.4451236649676318\n",
      "W1:5.482042012045105, W2:1.2490762403199471, bias:-2.948626544159025, loss:0.4450884283688318\n",
      "W1:5.48378924609372, W2:1.2490438673459539, bias:-2.9493087982049606, loss:0.44505321643132906\n",
      "W1:5.485535865547239, W2:1.2490115215065347, bias:-2.9499908224416864, loss:0.44501802913486144\n",
      "W1:5.487281870689195, W2:1.2489792028098656, bias:-2.950672616994434, loss:0.4449828664591863\n",
      "W1:5.489027261802993, W2:1.2489469112639973, bias:-2.95135418198828, loss:0.44494772838407903\n",
      "W1:5.490772039171906, W2:1.2489146468768553, bias:-2.9520355175481443, loss:0.44491261488933453\n",
      "W1:5.492516203079078, W2:1.2488824096562416, bias:-2.9527166237987936, loss:0.44487752595476554\n",
      "W1:5.494259753807521, W2:1.2488501996098347, bias:-2.9533975008648388, loss:0.4448424615602045\n",
      "W1:5.496002691640115, W2:1.2488180167451892, bias:-2.954078148870736, loss:0.4448074216855018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.49774501685961, 1.2487858610697378, -2.954758567940789)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(x_train.age, x_train.affordibility, y_train, 2000, 0.4448)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow model= W1:[5.506266], W2:[1.213402], bias:[-2.9263778]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So the calculated weights and bias are as same as we have got from Tensorflow model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self):\n",
    "        self.w1=1 \n",
    "        self.w2=1 \n",
    "        self.bias=0\n",
    "    \n",
    "    def fit(self,x_train,y_train,epochs,loss_threshold):\n",
    "        self.w1, self.w2, self.bias= self.gradient_descent(x_train['age'],x_train['affordibility'],y_train,epochs,loss_threshold)\n",
    "        print(f\"Final weights and bias: w1: {self.w1}, w2: {self.w2}, bias: {self.bias}\")\n",
    "        \n",
    "    def predict(self,x_test):\n",
    "        weighted_sum= self.w1*x_test['age'] + self.w2*x_test['affordibility'] + self.bias\n",
    "        return sigmoid_np(weighted_sum)\n",
    "    \n",
    "    def gradient_descent(self,x1,x2,y_true,epochs,loss_threshold):\n",
    "\n",
    "        w1=w2=1\n",
    "        bias=0\n",
    "        learning_rate= 0.1\n",
    "        n= len(x1)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            weighted_sum= w1*x1 + w2*x2 + bias \n",
    "            y_predicted= sigmoid_np(weighted_sum)\n",
    "            loss= bce(y_true, y_predicted)\n",
    "\n",
    "            w1d= (1/n)*(np.dot(np.transpose(x1),(y_predicted-y_true)))\n",
    "            w2d= (1/n)*(np.dot(np.transpose(x2),(y_predicted-y_true)))\n",
    "            bias_d= np.mean(y_predicted-y_true)\n",
    "\n",
    "            w1= w1-learning_rate*w1d\n",
    "            w2= w2-learning_rate*w2d\n",
    "            bias= bias-learning_rate*bias_d\n",
    "            \n",
    "            if i%50 == 0:\n",
    "                print(f'W1:{w1}, W2:{w2}, bias:{bias}, loss:{loss}')\n",
    "                \n",
    "            if loss <= loss_threshold: # as need to compare weights and bias with tensorflow model.\n",
    "                break\n",
    "                \n",
    "        return w1, w2, bias    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:0.9955324184480593, W2:0.9896158041440797, bias:-0.022453413341607935, loss:0.7067871004847728\n",
      "W1:0.9708672870973896, W2:0.7981883015810478, bias:-0.6366458135088361, loss:0.602694555804264\n",
      "W1:1.100741604360807, W2:0.8564166656074311, bias:-0.8499245195363202, loss:0.5888997018450574\n",
      "W1:1.2539198429843401, W2:0.937337328883083, bias:-0.9894965460488974, loss:0.5789591309909329\n",
      "W1:1.4094757519678573, W2:1.0076233252928215, bias:-1.1082266349423924, loss:0.5702902768111181\n",
      "W1:1.5642090982796752, W2:1.0652009011674495, bias:-1.2152354697597914, loss:0.5625327058939591\n",
      "W1:1.7174485674268813, W2:1.1118622922549926, bias:-1.3132207424789202, loss:0.5554686354459272\n",
      "W1:1.868924432229138, W2:1.1495354204091275, bias:-1.4037127174888344, loss:0.548948811805506\n",
      "W1:2.018462447217224, W2:1.1798467673178745, bias:-1.4878634021735015, loss:0.5428691129228838\n",
      "W1:2.165933161889934, W2:1.2041253464309152, bias:-1.5666020015852051, loss:0.5371557121475228\n",
      "W1:2.3112389680026806, W2:1.2234546790050531, bias:-1.6406909111829688, loss:0.5317553062947031\n",
      "W1:2.454307748774709, W2:1.2387204476706712, bias:-1.7107599916632028, loss:0.5266286223103275\n",
      "W1:2.595088359831033, W2:1.250648888376513, bias:-1.7773320814332516, loss:0.5217460675558564\n",
      "W1:2.7335470385080534, W2:1.2598371941507056, bias:-1.8408429592899607, loss:0.5170847933354951\n",
      "W1:2.86966446165291, W2:1.266777635382763, bias:-1.9016572108800252, loss:0.5126266979719754\n",
      "W1:3.0034333042347345, W2:1.271876798403088, bias:-1.960080945888553, loss:0.508357059547566\n",
      "W1:3.134856198018285, W2:1.2754710215863563, bias:-2.0163720576574384, loss:0.5042635937904462\n",
      "W1:3.2639440154981436, W2:1.277838851914722, bias:-2.0707485470017013, loss:0.5003358009635153\n",
      "W1:3.390714421631798, W2:1.2792111509655986, bias:-2.123395309207336, loss:0.49656451039800786\n",
      "W1:3.515190648274005, W2:1.2797793337615149, bias:-2.174469692295512, loss:0.49294156091498215\n",
      "W1:3.6374004552982218, W2:1.2797021146269596, bias:-2.224106066548313, loss:0.4894595751118047\n",
      "W1:3.757375249209264, W2:1.2791110517548034, bias:-2.272419593806116, loss:0.4861117987526811\n",
      "W1:3.8751493352693442, W2:1.278115119618129, bias:-2.31950934576488, loss:0.4828919854797983\n",
      "W1:3.990759283227088, W2:1.2768044905453526, bias:-2.3654608902630527, loss:0.4797943131784754\n",
      "W1:4.104243389965207, W2:1.275253669952041, bias:-2.4103484410668248, loss:0.4768133225212851\n",
      "W1:4.215641224982768, W2:1.2735241011405831, bias:-2.4542366482754354, loss:0.47394387110258757\n",
      "W1:4.324993246754258, W2:1.2716663332185634, bias:-2.497182091954877, loss:0.4711810985710338\n",
      "W1:4.432340479768032, W2:1.269721828058436, bias:-2.539234530067057, loss:0.4685203995528411\n",
      "W1:4.5377242435200005, W2:1.2677244682205415, bias:-2.5804379425207964, loss:0.46595740212268544\n",
      "W1:4.641185925981453, W2:1.2657018165648466, bias:-2.620831405726264, loss:0.4634879502515338\n",
      "W1:4.742766795116371, W2:1.263676169264925, bias:-2.660449826003022, loss:0.4611080891305568\n",
      "W1:4.84250784292567, W2:1.2616654366419895, bias:-2.699324555281509, loss:0.4588140525989626\n",
      "W1:4.9404496572691174, W2:1.2596838802994925, bias:-2.7374839085225675, loss:0.45660225213363703\n",
      "W1:5.036632317380505, W2:1.2577427301844895, bias:-2.7749535989841845, loss:0.45446926701957574\n",
      "W1:5.131095309563946, W2:1.2558507012169136, bias:-2.8117571047509102, loss:0.4524118354328324\n",
      "W1:5.223877460052479, W2:1.254014425844807, bias:-2.8479159777006107, loss:0.45042684624659307\n",
      "W1:5.31501688243562, W2:1.2522388161704392, bias:-2.883450104228328, loss:0.44851133142608446\n",
      "W1:5.404550937429452, W2:1.2505273670441956, bias:-2.9183779255085516, loss:0.4466624589164825\n",
      "W1:5.492516203079078, W2:1.2488824096562416, bias:-2.9527166237987936, loss:0.44487752595476554\n",
      "Final weights and bias: w1: 5.49774501685961, w2: 1.2487858610697378, bias: -2.954758567940789\n"
     ]
    }
   ],
   "source": [
    "custom_NN= NeuralNet()\n",
    "custom_NN.fit(x_train,y_train,epochs=2000,loss_threshold= 0.4448)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow model= W1:[5.506266], W2:[1.213402], bias:[-2.9263778]\n",
    "##### This shows that in the end we were able to come up with somewhat same value of w1,w2 and bias using a plain python implementation of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>affordibility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  affordibility\n",
       "24  0.50              1\n",
       "17  0.58              1\n",
       "19  0.18              1\n",
       "20  0.21              1\n",
       "14  0.49              1\n",
       "3   0.52              0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction using custom NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24    0.739409\n",
       "17    0.814983\n",
       "19    0.328191\n",
       "20    0.365529\n",
       "14    0.728677\n",
       "3     0.476036\n",
       "dtype: float64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_NN.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction using Tensorflow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7083558 ],\n",
       "       [0.7678779 ],\n",
       "       [0.41376856],\n",
       "       [0.44212312],\n",
       "       [0.7003138 ],\n",
       "       [0.4667115 ]], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24    1\n",
       "17    1\n",
       "19    0\n",
       "20    0\n",
       "14    1\n",
       "3     0\n",
       "Name: bought_insurance, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
